<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>How to install an open source tool for creating machine learning pipelines</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/05/how-install-open-source-tool-creating-machine-learning-pipelines" /><author><name>JooHo Lee</name></author><id>2cba1b69-5e0b-44a5-b252-1d61acd02f1c</id><updated>2022-05-05T07:00:00Z</updated><published>2022-05-05T07:00:00Z</published><summary type="html">&lt;p&gt;Pachyderm is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; tool for creating and running machine learning (&lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;AI/ML&lt;/a&gt;) pipelines. It runs on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and provides modern developer benefits such as versioning and autoscaling. Pachyderm also integrates with the &lt;a href="https://jupyter.org/hub"&gt;JupyterHub&lt;/a&gt; information sharing platform. On March 8, 2022, &lt;a href="https://cloud.redhat.com/blog/pachyderm-is-joining-the-open-data-hub-family"&gt;Red Hat announced&lt;/a&gt; that Pachyderm has been added to the &lt;a href="https://opendatahub.io/"&gt;Open Data Hub&lt;/a&gt; (ODH), which is a blueprint for building an AI-as-a-service platform on the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. In this article, you'll learn how to install Pachyderm using Open Data Hub.&lt;/p&gt; &lt;p&gt;To walk through the steps in this article, you'll need a Red Hat OpenShift cluster with a default &lt;code&gt;StorageClass&lt;/code&gt;. The procedure in this article has been tested in the following environments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://cloud.redhat.com/products/dedicated/"&gt;OpenShift Dedicated&lt;/a&gt; 4.9 on AWS with a &lt;code&gt;gp2&lt;/code&gt; &lt;code&gt;StorageClass&lt;/code&gt;&lt;/li&gt; &lt;li&gt;An OpenShift cluster using &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat OpenShift Local&lt;/a&gt; (formerly Red Hat CodeReady Containers) with an &lt;code&gt;nfs&lt;/code&gt; &lt;code&gt;StorageClass&lt;/code&gt; set up by the &lt;a href="https://developers.redhat.com/articles/2022/04/20/create-and-manage-local-persistent-volumes-codeready-containers"&gt;NFS Provisioner Operator&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The first option is used in this article. OpenShift Dedicated provides a default &lt;code&gt;gp2&lt;/code&gt; &lt;code&gt;StorageClass&lt;/code&gt;, but it is not cost-free. As an alternative, with the second option you can set up a cost-free environment as follows:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use OpenShift Local to install an &lt;a href="https://cloud.redhat.com/blog/openshift-all-in-one-aio-for-labs-and-fun"&gt;OpenShift All-in-One&lt;/a&gt; cluster on your laptop.&lt;/li&gt; &lt;li&gt;Add an &lt;code&gt;nfs&lt;/code&gt; &lt;code&gt;StorageClass&lt;/code&gt; using the NFS Provisioner Operator, available from &lt;a href="https://operatorhub.io/operator/nfs-provisioner-operator"&gt;OperatorHub&lt;/a&gt; or &lt;a href="https://github.com/jooho/nfs-provisioner-operator"&gt;Github&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Once you've followed those steps, you'll have essentially the same environment as OpenShift Dedicated.&lt;/p&gt; &lt;p&gt;This article also contains an embedded video illustrating the steps.&lt;/p&gt; &lt;p&gt;If you want to experiment with the Red Hat OpenShift Local test environment, please refer to the following articles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/06/configure-codeready-containers-aiml-development"&gt;Configure CodeReady Containers for AI/ML development&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/20/create-and-manage-local-persistent-volumes-codeready-containers"&gt;Create and manage local persistent volumes with CodeReady Containers&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Deploy the Open Data Hub Operator&lt;/h2&gt; &lt;p&gt;Installing an Operator is the easiest step in this procedure. Go to the OperatorHub menu option in the OpenShift console, search for the Open Data Hub Operator, and click its link (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/operator_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/operator_0.png?itok=osVrLwqS" width="864" height="632" alt="From the OperatorHub menu option, search for the Open Data Hub Operator and click its link." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. From the OperatorHub menu option, search for the Open Data Hub Operator and click its link. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Find the Open Data Hub Operator in the Openshift console.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;You'll be taken to the Operator page for the Open Data Hub Operator (Figure 2). Click &lt;strong&gt;Install.&lt;/strong&gt;&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/install1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/install1.png?itok=sms38NdO" width="630" height="468" alt="From the Open Data Hub Operator page, click Install." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. From the Open Data Hub Operator page, click Install. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Begin the Operator installation process by clicking here.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Next, you'll see the Install Operator page (Figure 3). Keep all the defaults and click &lt;strong&gt;Install&lt;/strong&gt; again.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/install2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/install2.png?itok=NdhDEDQ0" width="407" height="633" alt="From the Install Operator page, click Install." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. From the Install Operator page, click Install. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Click Install on the Install Operator page.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;When installation is complete, you'll see a message saying "saying "Installed operator — ready for use," as in Figure 4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ready_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ready_0.png?itok=5evgo1Wx" width="611" height="231" alt="When you finish installation, a page comes up saying "Installed operator — ready for use."" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. When you finish installation, a page comes up saying "Installed operator — ready for use." &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The Operator is now installed.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;After you install Open Data Hub Operator, you need to create a new project that we'll call &lt;code&gt;opendatahub&lt;/code&gt;, where all required components—Jupyterhub, &lt;a href="https://github.com/ceph/cn"&gt;Ceph Nano&lt;/a&gt;, and Pachyderm—will be deployed&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project opendatahub&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create a KfDef to deploy Pachyderm, JuypterHub, and Ceph Nano&lt;/h2&gt; &lt;p&gt;Pachyderm supports any storage option compatible with AWS S3 object storage. Open Data Hub provides two of these storage options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full automation:&lt;/strong&gt; Deploy Ceph Nano on Open Data Hub, which creates a secret for Pachyderm.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Partial automation:&lt;/strong&gt; Manually create a secret for the credentials to access S3 or another S3-compatible object storage, such as &lt;a href="https://min.io"&gt;MinIO&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Full automation using Ceph Nano&lt;/h3&gt; &lt;p&gt;Open Data Hub provides a full automation YAML configuration using a Kubernetes Job named &lt;a href="https://github.com/opendatahub-io/odh-manifests/blob/master/odhpachyderm/deployer/base/job.yaml"&gt;pachyderm-deployer&lt;/a&gt;. Here's an excerpt of the configuration:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;# Ceph Nano - kustomizeConfig: repoRef: name: manifests path: ceph/object-storage/scc name: ceph-nano-scc - kustomizeConfig: repoRef: name: manifests path: ceph/object-storage/nano name: ceph-nano # Pachyderm operator - kustomizeConfig: parameters: - name: namespace value: openshift-operators repoRef: name: manifests path: odhpachyderm/operator name: odhpachyderm-operator # Pachyderm deployer - kustomizeConfig: repoRef: name: manifests path: odhpachyderm/deployer name: odhpachyderm-deployer&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The configuration contains a script that makes sure Ceph Nano is in a ready state, and then creates an S3 bucket in Ceph Nano. After that, the script creates a secret for the S3 bucket credentials, which Pachyderm will use to gain access to the S3 bucket.&lt;/p&gt; &lt;p&gt;To use full automation on Kubernetes, you need a KfDef custom resource (CR). A manifest for this KfDef can be found &lt;a href="https://gist.github.com/Jooho/d4cd41263a1f2d875334c6a9cdb3673b"&gt;in my GitHub repository&lt;/a&gt;. Create the KfDef on OpenShift through the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f https://bit.ly/3wHwt59&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Partial automation using S3 or other compatible storage (MinIO)&lt;/h3&gt; &lt;p&gt;If you want to go the partial automation route, the only difference from using Ceph Nano is that you need to create a secret before creating the KfDef, then pass that information to &lt;code&gt;pachyderm-deployer&lt;/code&gt; in the KfDef. The relevant line can be found in context in the &lt;a href="https://gist.github.com/Jooho/81c883f05bf024fdb803f93a65942135#file-kfdef-for-pachyderm-using-other-storage-L44"&gt;YAML file&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;An &lt;code&gt;oc&lt;/code&gt; command that creates a secret for AWS S3 looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic pachyderm-aws-secret \ --from-literal=access-id=XXX \ --from-literal=access-secret=XXX \ --from-literal=region=us-east-2 \ --from-literal=bucket=pachyderm &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;An &lt;code&gt;oc&lt;/code&gt; command that creates a secret for MinIO looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic pachyderm-minio-secret \ --from-literal=access-id=XXX \ --from-literal=access-secret=XXX \ --from-literal=custom-endpoint=${minio_ip} --from-literal=region=us-east-2 \ --from-literal=bucket=pachyderm &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following excerpt from a KfDef manifest shows how to use the secret with S3. The example uses &lt;code&gt;pachyderm-aws-secret&lt;/code&gt; for the secret:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;# Pachyderm Operator - kustomizeConfig: parameters: - name: namespace value: openshift-operators repoRef: name: manifests path: odhpachyderm/operator name: odhpachyderm-operator # Pachyderm Deployer - kustomizeConfig: parameters: - name: storage_secret #&lt;=== Must set this value: pachyderm-aws-secret #&lt;=== Use your Secret Name repoRef: name: manifests path: odhpachyderm/deployer name: odhpachyderm-deployer&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once you've created the secret, you can create the KfDef through the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create -f https://bit.ly/3NkV31I&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After you create the KfDef, OpenShift creates several pods in the &lt;code&gt;opendatahub&lt;/code&gt; project. Four pods are created for Pachyderm:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get pod etcd-0 1/1 Running 0 12m postgres-0 1/1 Running 0 12m pachd-874f5958c-7w98p 1/1 Running 0 11m pg-bouncer-7587d49769-gwn8f 1/1 Running 0 11m&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Even more pods might be devoted to Pachyderm if you are using Red Hat OpenShift Local. If resources on your cluster are tight, it could take some time to create the pods.&lt;/p&gt; &lt;p&gt;Now you can try Pachyderm on your cluster.&lt;/p&gt; &lt;h2&gt;Video demo&lt;/h2&gt; &lt;p&gt;The following video illustrates the steps outlined so far.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Troubleshooting&lt;/h2&gt; &lt;p&gt;If you are running this example on your laptop, you might see some errors with the JupyterHub pods, &lt;code&gt;jupyterhub&lt;/code&gt; and &lt;code&gt;jupyterhub-db&lt;/code&gt;, due to a lack of resources (Figure 5).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/errors.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/errors.png?itok=wtaAhr77" width="456" height="124" alt="Sometimes, jupyterhub and jupyterhub-db show errors at startup." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. Sometimes, jupyterhub and jupyterhub-db show errors at startup. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Sometimes, jupyterhub and jupyterhub-db show errors at startup.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;&lt;code&gt;traefix-proxy&lt;/code&gt; pods might show some errors, but you can ignore them. When &lt;code&gt;jupyterhub&lt;/code&gt; and &lt;code&gt;jupyterhub-db&lt;/code&gt; are recovered, &lt;code&gt;traefix-proxy&lt;/code&gt; will be automatically healed.&lt;/p&gt; &lt;p&gt;If you see these errors, start a rollout of the DeploymentConfigs for &lt;code&gt;jupyterhub&lt;/code&gt; and &lt;code&gt;jupyterhub-db&lt;/code&gt; as shown in Figures 6 and 7.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jup.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jup.png?itok=037R_NOQ" width="510" height="564" alt="Pull up the DeploymentConfigs page to get access to pages for jupyterhub and jupyterhub-dc." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6. Pull up the DeploymentConfigs page to get access to pages for jupyterhub and jupyterhub-dc. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Pull up the DeploymentConfigs page to get access to pages for jupyterhub and jupyterhub-dc.&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/roll_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/roll_0.png?itok=qsrsxZzE" width="858" height="384" alt="On the jupyterhub page, choose "Start rollout."" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7. On the jupyterhub page, choose "Start rollout." &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: On the jupyterhub page, choose Start rollout.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Start a rollout for &lt;code&gt;jupyterhub-db&lt;/code&gt; in the same way.&lt;/p&gt; &lt;p&gt;If these steps don't solve the problem, roll out &lt;code&gt;jupyterhub-db&lt;/code&gt; first, wait until it is ready, then roll out &lt;code&gt;jupyterhub&lt;/code&gt;. Then enjoy experimenting with what Pachyderm has to offer!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/05/how-install-open-source-tool-creating-machine-learning-pipelines" title="How to install an open source tool for creating machine learning pipelines"&gt;How to install an open source tool for creating machine learning pipelines&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>JooHo Lee</dc:creator><dc:date>2022-05-05T07:00:00Z</dc:date></entry><entry><title>Schedule tests the GitOps way with Testing Farm as GitHub Action</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/04/schedule-tests-gitops-way-testing-farm-github-action" /><author><name>Petr Hracek, Zuzana Miklankova</name></author><id>91cbf943-114f-4c2f-abc8-5f0cb2a1c719</id><updated>2022-05-04T07:00:00Z</updated><published>2022-05-04T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://docs.testing-farm.io/general/0.1/index.html"&gt;Testing Farm&lt;/a&gt; is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; test system that makes it easier to run unit tests on your projects. A previous article, &lt;a href="https://developers.redhat.com/articles/2022/03/09/test-github-projects-github-actions-and-testing-farm"&gt;Test GitHub projects with GitHub Actions and Testing Farm&lt;/a&gt;, showed you how to use the framework. This article introduces a new GitHub Action named &lt;a href="https://github.com/sclorg/testing-farm-as-github-action"&gt;Testing Farm as GitHub Action&lt;/a&gt;, which is &lt;a href="https://github.com/marketplace/actions/schedule-tests-on-testing-farm"&gt;available on GitHub Marketplace&lt;/a&gt; and integrates your tests into a &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; environment so that you can ensure that tests are run at key points during development.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href="https://docs.github.com/en/get-started/customizing-your-github-workflow/exploring-integrations/about-github-marketplace"&gt;GitHub Marketplace&lt;/a&gt; is a central site where developers can find GitHub Actions along with other resources. Anyone with a GitHub account can publish an Action on GitHub Marketplace. Publishing the Action in the Marketplace brings additional visibility.&lt;/p&gt; &lt;p&gt;Testing Farm is a service maintained by Red Hat, and is currently available to &lt;a href="https://docs.testing-farm.io/general/0.1/index.html"&gt;internal services, Red Hat Hybrid Cloud services, and open source projects related to Red Hat products.&lt;/a&gt; Any kind of test described with a Test Management Tool (TMT) plan can be executed. Your testing environment can be &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;, Fedora, CentOS, or CentOS Stream.&lt;/p&gt; &lt;p&gt;We recommend that you read the previous article before this one to learn how Testing Farm works and how it fits into the development cycle.&lt;/p&gt; &lt;h2&gt;About Testing Farm as GitHub Action&lt;/h2&gt; &lt;p&gt;Testing Farm provides a common interface to many types of tests and makes it easy to run them in a uniform manner, but it's still the developer's job to schedule the tests. With &lt;a href="https://github.com/sclorg/testing-farm-as-github-action"&gt;Testing Farm as GitHub Action&lt;/a&gt;, you can decide at what points in your development cycle (creating, checking or merging a pull request (PR), etc.) you want to run tests, and have them run automatically by GitHub.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: In order to avoid running malicious code on the Testing Farm infrastructure, it is important to have the tests reviewed by an authorized person.&lt;/p&gt; &lt;p&gt;The Action can be used by developers, maintainers, or anyone else who wants to test a repository located on GitHub. We recommend that, to make sure unauthorized code doesn't run in a testing environment, you allow only members and owners of a repository to use the Action.&lt;/p&gt; &lt;p&gt;Testing Farm as GitHub Action is a composite Action, intended to be used from other GitHub Actions. You can find more details about how composite actions work in the &lt;a href="https://docs.github.com/en/actions/creating-actions/creating-a-composite-action"&gt;GitHub documentation&lt;/a&gt;. Testing Farm as GitHub Action schedules tests so that &lt;a href="https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows"&gt;triggers in a GitHub repository&lt;/a&gt; run the tests on the Testing Farm infrastructure and, optionally, display their results.&lt;/p&gt; &lt;h2&gt;Action inputs&lt;/h2&gt; &lt;p&gt;Testing Farm as GitHub Action is highly configurable, but all the variables have defaults except the following, which must be configured by the user:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;api_key&lt;/code&gt;: API key used to get access to Testing Farm&lt;/li&gt; &lt;li&gt;&lt;code&gt;git_url&lt;/code&gt;: URL of a GitHub repository with one or more TMT plans&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To obtain your API key, send an email to &lt;a href="mailto:tft@redhat.com"&gt;tft@redhat.com&lt;/a&gt; as described in &lt;a href="https://docs.testing-farm.io/general/0.1/onboarding.html"&gt;Testing Farm's onboarding documentation.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a minimal example of what it would look like to use Testing Farm as GitHub Action on a repository that you have checked out:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;- name: Schedule tests Testing Farm uses: sclorg/testing-farm-as-github-action@v1 with: api_key: ${{ secrets.API_KEY }} git_url: &lt;URL to a TMT plan&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;All other input values are optional. They fall into the following groups:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TMT metadata&lt;/strong&gt;: Options for configuring the TMT specification, such as the URL for the Git repository with the TMT plan, or a regular expression that selects plans.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Testing Farm:&lt;/strong&gt; Options for configuring Testing Farm itself. You can configure the API key, the URL to Testing Farm's API, and the scope (public or private) of the Testing Farm in use.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Test environment:&lt;/strong&gt; Options for configuring the operating system and architecture where the test will be run. The supported Linux distributions are Fedora, CentOS (including CentOS Stream), and Red Hat Enterprise Linux 7 and 8. You can also specify the secrets and environment variables needed during test execution.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Test artifacts:&lt;/strong&gt; Additional artifacts to install in the test environment. For more information, see &lt;a href="https://testing-farm.gitlab.io/api/"&gt;Testing Farm's REST API documentation.&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Miscellaneous:&lt;/strong&gt; Settings for various options, such as whether the PR should be updated with test results after finishing the job and what should be written in the PR status.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;More information about the inputs can be found at the &lt;a href="https://github.com/sclorg/testing-farm-as-github-action"&gt;Action's GitHub repository.&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Action outputs&lt;/h2&gt; &lt;p&gt;Each run of Testing Farm as GitHub Action returns two outputs: a &lt;code&gt;req_id&lt;/code&gt; and a &lt;code&gt;results_url&lt;/code&gt;. You can combine these values to obtain a URL pointing to a log. Test logs and test results from the Testing Farm are collected here in text form.&lt;/p&gt; &lt;p&gt;Optionally, if the event that triggers Testing Farm as GitHub Action is related to a PR, the user can enable a PR status update. This update places a summary of test results in a graphical form directly in the PR. By default, test results are displayed as a status using the &lt;a href="https://docs.github.com/en/rest/reference/repos#statuses"&gt;GitHub statuses API&lt;/a&gt;. Figure 1 shows sample output.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/checks.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/checks.png?itok=2CFy_R0-" width="754" height="203" alt="Output from Testing Farm shows the results of each test, with links to details." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Output from Testing Farm shows the results of each test, with links to details. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Output from Testing Farm shows the results of each test, with links to details.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;A Testing Farm as GitHub Action example&lt;/h2&gt; &lt;p&gt;The following configuration runs tests when a PR is created on the repository. (You can find the full example YAML file in the &lt;a href="https://github.com/sclorg/nginx-container/blob/master/.github/workflows/container-tests.yml"&gt;software collections GitHub repository&lt;/a&gt;.) This configuration follows the recommendation that the person running the tests must be an owner or member of the repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;name: upstream tests at Testing Farm on: issue_comment: types: - created jobs: build: name: A job run on explicit user request runs-on: ubuntu-20.04 if: | github.event.issue.pull_request &amp;&amp; contains(github.event.comment.body, '[test]') &amp;&amp; contains(fromJson('["OWNER", "MEMBER"]'), github.event.comment.author_association)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Action checks out the repository and switches to the branch of the PR branch:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;steps: - name: Checkout repo uses: actions/checkout@v2 with: ref: "refs/pull/${{ github.event.issue.number }}/head" &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now you can schedule tests on Testing Farm using the GitHub Action:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt; - name: Schedule tests on external Testing Farm by Testing-Farm-as-github-action id: github_action uses: sclorg/testing-farm-as-github-action@v1 with: api_key: ${{ secrets.TF_API_KEY }} git_url: "https://github.com/sclorg/sclorg-testing-farm" variables: "REPO_URL=$GITHUB_SERVER_URL/$GITHUB_REPOSITORY;REPO_NAME=$GITHUB_REPOSITORY;PR_NUMBER=${{ github.event.issue.number }};OS=centos7;TEST_NAME=test" compose: "CentOS-7" &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Testing Farm as GitHub Action lets you avoid the tedious work of setting up a testing infrastructure, writing a lot of GitHub Action workflows, and handling PR statuses. With just a TMT test plan and an API key, you can get access to a whole testing infrastructure that will meet your needs. Various processor architectures and Linux distributions are available as testing environments.&lt;/p&gt; &lt;p&gt;To get started, all you need is to request an &lt;code&gt;api_key&lt;/code&gt; from the Testing Farm team, write a simple GitHub workflow, and use the Action.&lt;/p&gt; &lt;p&gt;Your tests are triggered by an event you specify in the configuration file. Logs and results from test execution are collected, reported, and stored in text form and optionally also transparently displayed in the PR status. Testing Farm as GitHub Action makes it easy to test project changes as soon as possible before your project goes to real customers.&lt;/p&gt; &lt;p&gt;If you want to learn more about how GitOps can transform your workflow, check out &lt;a href="https://developers.redhat.com/courses/gitops/getting-started-argocd-and-openshift-gitops-operator"&gt;Getting Started with ArgoCD and OpenShift GitOps Operator&lt;/a&gt;, a hands-on lab from Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/04/schedule-tests-gitops-way-testing-farm-github-action" title="Schedule tests the GitOps way with Testing Farm as GitHub Action"&gt;Schedule tests the GitOps way with Testing Farm as GitHub Action&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Petr Hracek, Zuzana Miklankova</dc:creator><dc:date>2022-05-04T07:00:00Z</dc:date></entry><entry><title type="html">Testing Kogito Serverless Workflow with YAKS</title><link rel="alternate" href="https://blog.kie.org/2022/05/testing-kogito-serverless-workflow-with-yaks.html" /><author><name>Christoph Deppisch</name></author><id>https://blog.kie.org/2022/05/testing-kogito-serverless-workflow-with-yaks.html</id><updated>2022-05-03T20:21:04Z</updated><content type="html">The specification describes a declarative domain-specific language to define service orchestration in the serverless technology domain. is an Open Source testing framework that brings Behavior Driven Development (BDD) concepts to Kubernetes. At the time of this writing, YAKS supports many technologies leveraged by Serverless Workflow such as Cloud events, RESTful services, OpenAPI, Kafka, and Knative eventing. So YAKS as a test framework is lined up to be a perfect fit for verifying Serverless workflows with automated end-to-end integration testing. The following blog post describes how to write fully automated tests for a Serverless Workflow implementation that can be run in a CI/CD environment. UNDERSTANDING THE KOGITO SERVERLESS WORKFLOW EXAMPLE The automated test in this post is about to verify a simple Serverless Workflow example that orchestrates several RESTful services via OpenAPI and connects to the Knative eventing message broker. The sample workflow playtowin.sw.json allows users to participate in a simple game to win a prize. The workflow gets triggered with a Knative event that holds the username of the participant. As a next step, the workflow calls a foreign scoring service to check if the user has won a prize.  In the following, the workflow evaluates the hasWon condition to find out which user actually has won a prize. According to the condition evaluation result, another foreign service is called to retrieve the user details (e.g. shipping address information).  The actual prize information gets injected into the final Knative event that gets published as a workflow outcome. The workflow definition is implemented with Kogito and runs as an arbitrary container image in a Kubernetes Pod. The Kogito workflow implementation is packaged as a normal container image available on Quay (). You can use the container image in a Knative service in order to run the workflow as a system under test in your user namespace. --- apiVersion: serving.knative.dev/v1 kind: Service metadata: labels: app.kubernetes.io/version: "1.0" app.kubernetes.io/name: kogito-serverless-workflow-demo name: kogito-serverless-workflow-demo spec: template: metadata: labels: app.kubernetes.io/version: "1.0" app.kubernetes.io/name: kogito-serverless-workflow-demo spec: containers: - image: quay.io/citrusframework/kogito-serverless-workflow-demo:1.0 name: kogito-serverless-workflow-demo ports: - containerPort: 8080 name: http1 protocol: TCP Once the workflow service is up and running we can start to write an automated YAKS test. WRITING THE YAKS TEST From a testing perspective the two foreign services for retrieving the winner scoring and the user details need to be simulated during the test. In addition to that the test needs to trigger the initial participant event to start the workflow. Last not least the resulting prize event needs to be verified on the Knative broker. YAKS as a testing framework defines a BDD feature that is capable of describing all of these steps in one or more Gherkin Given-When-Then scenarios. The YAKS test is a normal BDD feature file that starts with a feature description and a background section to set the scene with some timeout configuration for the supporting services. Feature: Kogito Serverless workflow - Play To Win Background: Given HTTP server timeout is 15000 ms Given Knative event consumer timeout is 20000 ms The test now needs to provide several supporting services such as the Knative broker and the two foreign RESTful services for retrieving the winner scoring and the user details.  The Knative broker is automatically created in the user namespace with the following steps in the YAKS feature: # Verify Knative broker Given create Knative broker default Given Knative broker default is running YAKS is also able to create RESTful Http server components and expose these as Kubernetes services so the system under test can connect to these services. # Create HTTP server Given HTTP server "kogito-test" Given create Kubernetes service kogito-test The steps create a new HTTP server and Kubernetes service called kogito-test. The Kogito Serverless Workflow implementation should connect to this service during the test. So we need to set the Http base URI accordingly in the Kogito application.properties. #org.kogito.openapi.client.xxx.base_path=http://url_here org.kogito.openapi.client.score.base_path=http://kogito-test org.kogito.openapi.client.employee.base_path=http://kogito-test The base URI makes sure that the Kogito workflow connects to the YAKS service. The YAKS test is able to receive the RESTful Http request calls in order to verify the request and provide a proper response. The YAKS test also needs to consume the final Knative event when the user has actually won a prize. Therefore the test creates another supporting service and a Knative trigger to consume the event later in the test. # Create prize event consumer service Given Knative service port 8081 Given create Knative event consumer service prize-service Given create Knative trigger prize-service-trigger on service prize-service with filter on attributes | type | prizes | The Knative event consumer service uses a custom port 8081 . This is because the default port 8080 is already bound to the kogito-test supporting Http service that the test has created before that. The Knative trigger uses a filter on the event type prizes to consume the events on the Knative broker. Now that all supporting services are prepared the test moves on to describing the actual workflow use case: Scenario: User wins a prize Given variable name is "krisv" # Create new participant event Given Knative event data: {"username": "${name}"} Then send Knative event | type | participants | | source | /test/participant | | subject | New participant | | id | citrus:randomUUID() | The first step creates a new participant event that should trigger the workflow. The test uses a variable name as username. Once the variable is declared it will be used throughout the whole test. Then the Cloud event is sent to the Knative broker. This should trigger the workflow so we expect a call on the winner scoring service as a next step in the test. # Verify score service called Given HTTP server "kogito-test" When receive POST /scores Then HTTP response body: {"result": true} And HTTP response header: Content-Type="application/json" And send HTTP 200 OK This step receives the Http request and verifies the request content such as resource paths and the Http method. As a result the test defines a simulated Http response that holds the winner scoring result as Json payload. The test specifies that the user actually has won a prize (result=true).  This is a very important concept when writing automated tests in general. The test should always be in control of the use case so the next steps following are deterministic from a tester’s perspective. In this specific example the test always returns that the user wins a prize because this is the use case we want to verify.  Of course you can imagine writing another test where the user does not win a prize. With YAKS the tester is in full control of the returned Http response data so you can even verify proper error handling by choosing to respond with a 404 NOT FOUND or 500 INTERNAL SERVER ERROR response instead of a 200 OK.  Now that the user has won a prize the next step expects the workflow to call the user details service for retrieving some shipping information. # Verify get employee details service called When receive GET /employee/${name} Then HTTP response body: {"firstName": "Kris", "lastName":"Verlaenen", "address":"Castle 12, Belgium"} And HTTP response header: Content-Type="application/json" And send HTTP 200 OK Once again the test verifies a RESTful Http request. This time the request should be a Http GET request on the resource path /employee/${name}. The expected resource path is able to reference the name variable that the test has defined at the very beginning. As a response the test simulates some user details in the Http message body that get sent back to the workflow. As a last step in this use case the test expects the final Knative prize event with the user details and the injected prize that the user has won: # Verify prize won event Given Knative service "prize-service" Then expect Knative event data """ { "username": "${name}", "result": true, "firstName": "Kris", "lastName":"Verlaenen", "address":"Castle 12, Belgium", "prize": "Lego Mindstorms" } """ And verify Knative event | id | @ignore@ | | type | prizes | | source | /process/PlayToWin_ServerlessWorkflow | The test consumes the final event and verifies the Cloud event data. YAKS uses the very powerful Json data validation capabilities of here. All Json properties and values are compared with the expected Json template given in the test. Message header and body get verified so the test makes sure that the workflow has published the event to Knative as expected. This completes the test and when all steps are verified as expected we make sure that the workflow has orchestrated all RESTful services and Knative events for this use case under test. You can review the complete feature file and the whole workflow implementation in the code repository for this example on GitHub: USE OPENAPI SPECIFICATIONS The two supporting services for winner scoring and user details define an OpenAPI specification. The Serverless workflow actually makes use of this specification to implement the RESTful Http requests. The YAKS test is able to load these OpenAPI specifications, too. This way the test is able to leverage the rules in the OpenAPI specification to verify the incoming request. The OpenAPI specification for the scoring service looks as follows: --- openapi: 3.0.3 info: title: Score Service version: 1.0.0 paths: /scores: get: operationId: countWinners responses: "200": description: OK content: text/plain: schema: format: int64 type: integer post: operationId: isWinner responses: "200": description: OK content: application/json: schema: $ref: '#/components/schemas/ScoreResult' components: schemas: ScoreResult: type: object properties: result: type: boolean Instead of using pure HTTP level steps in the YAKS test as seen before YAKS is able to load the score.yaml OpenAPI specification and use the specification rules for verification purpose. # Verify score service called Given HTTP server "kogito-test" Given OpenAPI service "kogito-test" Given OpenAPI specification: score.yaml Given OpenAPI outbound dictionary | $.result | true | When verify operation: isWinner Then send operation response: 200 The test just uses the operation identifier isWinner and the response identifier 200. Now the test loads the OpenAPI specification and automatically generates the request verification and the simulated Http response. In the same way the test is able to also verify the user details service call by loading the employee.yaml OpenAPI specification. # Verify get employee details service called Given OpenAPI specification: employee.yaml When verify operation: getEmployeeDetails Then send operation response: 200 The great advantage of this approach is that the test does not care about pure Http level details such as the used Http request method and resource paths. Instead the test loads this information from the OpenAPI specification. The test even generates the proper Json response content from the information given in the OpenAPI specification. This makes sure that the test is up to date with the specification and changes in the OpenAPI specification are directly seen in the test. RUNNING THE TEST IN CI/CD You can run the YAKS test in a CI/CD environment in order to verify the Serverless Workflow implementation for instance with each commit or pull request on GitHub.  The code repository for the example used in this post uses a GitHub actions workflow that runs the test on a Kind cluster: The GitHub actions workflow installs Knative eventing as well as the YAKS operator to set the scene. Then the CI/CD workflow automatically deploys the system under test and runs the YAKS test via CLI tooling. This completes this post on how to verify Serverless Workflow with automated YAKS tests. The example has shown how to interact with Knative eventing and OpenAPI to verify RESTful service orchestration. YAKS is a quite new project and the group of maintainers is happy to receive any kind of feedback. Contributions are also very welcome! Please feel free to join the discussions or even express your appreciation with a star on GitHub: DEMO Watch the demo recording and see the YAKS framework in action: The post appeared first on .</content><dc:creator>Christoph Deppisch</dc:creator></entry><entry><title>Fine-tune Kafka performance with the Kafka optimization theorem</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/03/fine-tune-kafka-performance-kafka-optimization-theorem" /><author><name>Bilgin Ibryam</name></author><id>ae0dbd68-3a12-478e-b133-cafb88444aab</id><updated>2022-05-03T05:00:00Z</updated><published>2022-05-03T05:00:00Z</published><summary type="html">&lt;p&gt;The performance of your &lt;a href="https://kafka.apache.org"&gt;Apache Kafka&lt;/a&gt; environment will be affected by many factors, including choices, such as the number of partitions, number of replicas, producer acknowledgments, and message batch sizes you provision. But different applications have different requirements. Some prioritize latency over throughput, and some do the opposite. Similarly, some applications put a premium on durability, whereas others care more about availability. This article introduces a way of thinking about the tradeoffs and how to design your message flows using a model I call the &lt;em&gt;Kafka optimization theorem.&lt;/em&gt;&lt;/p&gt; &lt;h2&gt;Optimization goals for Kafka&lt;/h2&gt; &lt;p&gt;Like other systems that handle large amounts of data, Kafka balances the opposing goals of &lt;em&gt;latency&lt;/em&gt; and &lt;em&gt;throughput&lt;/em&gt;. Latency, in this article, refers to how long it takes to process a single message, whereas throughput refers to how many messages can be processed during a typical time period. Architectural choices that improve one goal tend to degrade the other, so they can be seen as opposite ends of a spectrum.&lt;/p&gt; &lt;p&gt;Two other opposed goals are &lt;em&gt;durability&lt;/em&gt; and &lt;em&gt;availability&lt;/em&gt;. Durability (which in our context also includes consistency) determines how robust the entire system is in the face of failure, whereas availability determines how likely it is to be running. There are tradeoffs between these two goals as well.&lt;/p&gt; &lt;p&gt;Thus, the main performance considerations for Kafka can be represented as in Figure 1. The diagram consists of two axes, each with one of the goals at one of its ends. For each axis, performance lies somewhere between the two ends.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_3.png?itok=GiAK_cyz" width="600" height="551" alt="Diagram showing that Kafka performance involves two orthogonal axes: Availability versus durability and latency versus throughput." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Kafka performance involves two orthogonal axes: Availability versus durability and latency versus throughput. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Kafka priorities and the CAP theorem&lt;/h2&gt; &lt;p&gt;The CAP theorem, which balances &lt;em&gt;consistency&lt;/em&gt; (C), &lt;em&gt;availability&lt;/em&gt; (A), and &lt;em&gt;partition tolerance&lt;/em&gt; (P), is one of the foundational theorems in distributed systems.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The word &lt;em&gt;partition&lt;/em&gt; in CAP is totally different from its use in Kafka streaming. In CAP, a partition is a rupture in the network that causes two communicating peers to be temporarily unable to communicate. In Kafka, a partition is simply a division of data to permit parallelism.&lt;/p&gt; &lt;p&gt;Partition tolerance in the CAP sense is always required (because one can't guarantee that a partition won't happen), so the only real choice in CAP is whether to prioritize CP or AP.&lt;/p&gt; &lt;p&gt;But CAP is sometimes extended with other considerations. In the absence of a partition, one can ask what &lt;em&gt;else&lt;/em&gt; (E) happens and choose between optimizing for &lt;em&gt;latency&lt;/em&gt; (L) or &lt;em&gt;consistency&lt;/em&gt; (C). The complete list of considerations is called &lt;a href="https://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf"&gt;PACELC&lt;/a&gt; (Figure 2). PACELC explains how some systems such as Amazon's Dynamo favor availability and lower latency over consistency, whereas ACID-compliant systems favor consistency over availability or lower latency.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_2.png?itok=EfEuTuEB" width="600" height="373" alt="Diagram showing that, besides the familiar CAP principle, one can choose between latency and consistency when there is no partitioning." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Besides the familiar CAP principle, one can choose between latency and consistency when there is no partitioning. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Kafka primitives&lt;/h2&gt; &lt;p&gt;Every time you configure a Kafka cluster, create a topic, or configure a producer or a consumer, you are making a choice of latency versus throughput and of availability versus durability. To explain why, we have to look into the main primitives in Kafka and the role that client applications play in an event flow.&lt;/p&gt; &lt;h3&gt;Partitions&lt;/h3&gt; &lt;p&gt;Topic partitions represent the unit of parallelism in Kafka, and are shown on the horizontal axis in Figure 1. In the broker and the clients, reads and writes to different partitions can be done fully in parallel. While many factors can limit throughput, a higher number of partitions for a topic generally enables higher throughput, and a smaller number leads to lower throughput.&lt;/p&gt; &lt;p&gt;However, a very large number of partitions causes the creation of more metadata that needs to be passed and processed across all brokers and clients. This increased metadata can degrade end-to-end latency unless more resources are added to the brokers. This is a somewhat simplified description of how partitions work on Kafka, but it will serve to demonstrate the main tradeoff that the number of partitions leads to in our context.&lt;/p&gt; &lt;h3&gt;Replicas&lt;/h3&gt; &lt;p&gt;Replicas determine the number of copies (including the leader) for each topic partition in a cluster, and are shown on the vertical axis in Figure 1. Data durability is ensured by placing replicas of a partition in different brokers. A higher number of replicas ensures that the data is copied to more brokers and offers better durability of data in the case of broker failure. On the other hand, a lower number of replicas reduces data durability, but in certain circumstances can increase availability to the producers or consumers by tolerating more broker failures.&lt;/p&gt; &lt;p&gt;Availability for a consumer is determined by the availability of in-sync replicas, whereas availability for a producer is determined by the minimum number of in-sync replicas (&lt;code&gt;min.isr&lt;/code&gt;). With a low number of replicas, the availability of overall data flow depends on which brokers fail (whether it's the one with the partition's leader of interest) and whether the other brokers are in sync. For our purposes, we can assume that fewer replicas lead to higher application availability and lower data durability.&lt;/p&gt; &lt;h3&gt;Producers and consumers&lt;/h3&gt; &lt;p&gt;Partitions and replicas represent the broker side of the picture, but are not the only primitives that influence the tradeoffs between throughput and latency and between durability and availability. Other participants that shape the main Kafka optimization tradeoffs are found in the client applications.&lt;/p&gt; &lt;p&gt;Topics are used by producers that send messages and consumers that read these messages. Producers and consumers also state their preference between throughput versus latency and durability versus availability through various configuration options. It is the combination of topic and client application configurations (and other cluster-level configurations, such as leader election) that defines what your application is optimized for.&lt;/p&gt; &lt;p&gt;Consider the flow of events consisting of a producer, a consumer, and a topic. Optimizing such an event flow for average latency on the client side requires tuning the producer and consumer to exchange smaller batches of messages. The same flow can be tuned for average throughput by configuring the producer and consumer for larger batches of messages.&lt;/p&gt; &lt;p&gt;Producers and consumers involved in a message flow also state their preference for durability or availability. A producer that favors durability over availability can demand a higher number of acknowledgments by specifying &lt;code&gt;acks=all&lt;/code&gt;. A lower number of acknowledgments (lower than the default &lt;code&gt;min.isr&lt;/code&gt;, which means 0 or 1) could lead to higher availability from the producer point of view by tolerating a higher number of broker failures. However, this lower number reduces data durability in the case of catastrophic events affecting the brokers.&lt;/p&gt; &lt;p&gt;The consumer configurations influencing the vertical dimension are not as straightforward as the producer configurations, but depend on the consumer application logic. A consumer can favor higher consistency by committing message consumption offsets more often or even individually. This does not affect the durability of the records in the broker, but does affect how consumed records are tracked, and can prevent duplicate message processing in the cosumer. Alternatively, the consumer can favor availability by increasing various timeouts and tolerating broker failures for longer periods of time.&lt;/p&gt; &lt;h2&gt;The Kafka optimization theorem&lt;/h2&gt; &lt;p&gt;We have defined the main actors involved in an event flow as a producer, a consumer, and a topic. We've also defined the opposing goals we have to optimize for: throughput versus latency and durability versus availability. Given these parameters, the Kafka optimization theorem states that any Kafka data flow makes the tradeoffs shown in Figure 3 in throughput versus latency and durability versus availability.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kafka_optimization_theorem_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kafka_optimization_theorem_0.png?itok=4PFOFR11" width="1440" height="1522" alt="Choices for each parameter determines where a Kafka application stands on the axes defined in Figure 1." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Choices for each parameter determines where a Kafka application stands on the axes defined in Figure 1. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Choices for each parameter determines where a Kafka application stands on these two axes&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;The Kafka optimization theorem and the primary configuration options&lt;/h3&gt; &lt;p&gt;For simplicity, we put producer and consumer preferences on the same axes as topic replicas and partitions in Figure 3. Optimizing for a particular goal is easier when these primitives are aligned. For example, optimizing an end-to-end data flow for low latency is best achieved with smaller producer and consumer message batches combined with a small number of partitions.&lt;/p&gt; &lt;p&gt;A data flow optimized for higher throughput would have larger producer and consumer message batches and a higher number of partitions for parallel processing. A data flow optimized for durability would have a higher number of replicas and require a higher number of producer acknowledgments and granular consumer commits. A data flow that is optimized for availability would prefer a smaller number of replicas and a smaller number of producer acknowledgments with larger timeouts.&lt;/p&gt; &lt;p&gt;In practice, there is no correlation in the configuration of partitions, replicas, producers, and consumers. It is possible to have a large number of replicas for a topic (&lt;code&gt;min.isr&lt;/code&gt;), but have a producer that requires a 0 or 1 for acknowledgments. Or you could configure a higher number of partitions because your application logic requires it, and small producer and consumer message batches. These are valid scenarios in Kafka, and one of its strengths is that it's a highly configurable and flexible eventing system that satisfies many use cases.&lt;/p&gt; &lt;p&gt;Nevertheless, the framework proposed here can serve as a mental model for the main Kafka primitives and how they relate to the optimization dimensions. If you understand the foundational forces, you can tune them in ways specific to your application and understand the effects.&lt;/p&gt; &lt;h3&gt;The golden ratio&lt;/h3&gt; &lt;p&gt;The proposed Kafka optimization theorem deliberately avoids stating numbers, instead just laying out the relationships between the main primitives and the direction of change. The theorem is not intended to serve as a concrete optimization configuration but rather as a guide that shows how reducing or increasing a primitive configuration influences the direction of the optimization. Yet sharing a few of the most common configuration options accepted as the industry best practices could be useful as a starting point and demonstration of the theorem in practice.&lt;/p&gt; &lt;p&gt;Kafka clusters today, whether run on-premises with something like &lt;a href="https://strimzi.io/"&gt;Strimzi&lt;/a&gt;, or as a fully managed service offering such as &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;OpenShift Streams for Apache Kafka&lt;/a&gt;, are almost always deployed within a single region. A production-grade Kafka cluster is typically spread into three availability zones with a replication factor of 3 (&lt;code&gt;RF=3&lt;/code&gt;) and minimum in-sync replicas of 2 (&lt;code&gt;min.isr=2&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;These values ensure a good level of data durability during happy times, and good availability for client applications during temporary disruptions. This configuration represents a solid middle ground, because specifying &lt;code&gt;min.isr=3&lt;/code&gt; would prevent producers from producing even when a single broker is affected, and specifying &lt;code&gt;min.isr=1&lt;/code&gt; would affect both producer and consumer when a leader is down. Typically, this replication configuration is accompanied by &lt;code&gt;acks=all&lt;/code&gt; on the producer side and by default offset commit configurations for the consumers.&lt;/p&gt; &lt;p&gt;While there are commonalities in consistency and availability tradeoffs among different applications, throughput and latency requirements vary. The number of partitions for a topic is influenced by the shape of the data, the data processing logic, and its ordering requirements. At the same time, the number of partitions dictates the maximum parallelism and message throughput you can achieve. As a consequence, there is no good default number or range for the partition count.&lt;/p&gt; &lt;p&gt;By default, Kafka clients are optimized for low latency. We can observe that from the default producer values (&lt;code&gt;batch.size=16384&lt;/code&gt;, &lt;code&gt;linger.ms=0&lt;/code&gt;, &lt;code&gt;compression.type=none&lt;/code&gt;) and the default consumer values (&lt;code&gt;fetch.min.bytes=1&lt;/code&gt;, &lt;code&gt;fetch.max.wait.ms=500&lt;/code&gt;). The producer prior to Kafka 3 had &lt;code&gt;acks=1&lt;/code&gt;, which recently changed to &lt;code&gt;acks=all&lt;/code&gt; to create a preference for durability rather than availability or low latency. Optimizing the client applications for throughput would require increasing wait times and batch sizes five to tenfold and examining the results. Knowing the default values and what they are optimized for is a good starting point for your service optimization goals.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;CAP is a great theorem for failure scenarios. While failures are a given and partitioning will always happen, we have to optimize applications for happy paths too. This article introduces a simplified model explaining how Kafka primitives can be used for performance optimization.&lt;/p&gt; &lt;p&gt;The article deliberately focuses on the main primitives and selective configuration options to demonstrate its principles. In real life, more factors influence your application performance metrics. Encryption, compression, and memory allocation affect latency and throughput. Transactions, exactly-one semantics, retries, message flushes, and leader election affect consistency and availability. You might have broker primitives (replicas and partitions) and client primitives (batch sizes and acknowledgments) optimized for competing tradeoffs.&lt;/p&gt; &lt;p&gt;Finding the ideal Kafka configuration for your application will require experimentation, and the Kafka optimization theorem will guide you in the journey.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/03/fine-tune-kafka-performance-kafka-optimization-theorem" title="Fine-tune Kafka performance with the Kafka optimization theorem"&gt;Fine-tune Kafka performance with the Kafka optimization theorem&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bilgin Ibryam</dc:creator><dc:date>2022-05-03T05:00:00Z</dc:date></entry><entry><title type="html">How to use Camel extensions in Quarkus</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/how-to-use-camel-extensions-in-quarkus-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/how-to-use-camel-extensions-in-quarkus-applications/</id><updated>2022-05-02T16:37:24Z</updated><content type="html">Camel is the open source swiss knife framework for integration, with over 300 components allowing the integration between external systems. In this article we will learn how to use Camel extensions for Quarkus to build powerful Java Integrations. Using Camel with Quarkus Firstly, let’s see which extensions you need to develop Camel applications with Quarkus. ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Comparing Distributed Transaction Patterns for Microservices</title><link rel="alternate" href="http://www.ofbizian.com/2022/05/comparing-distributed-transaction.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/05/comparing-distributed-transaction.html</id><updated>2022-05-02T11:16:00Z</updated><content type="html">As a consulting architect at Red Hat, I've had the privilege of working on legions of customer projects. Every customer brings their own challenges but I've found some commonalities. One thing most customers want to know is how to coordinate writes to more than one system of record. Answering this question typically involves a long explanation of dual writes, distributed transactions, modern alternatives, and the possible failure scenarios and drawbacks of each approach. Typically, this is the moment when a customer realizes that is a long and complicated journey, and usually requires tradeoffs. Rather than go down the rabbit hole of discussing transactions in-depth, this article summarizes the main approaches and patterns for coordinating writes to multiple resources. I’m aware that you might have good or bad past experiences with one or more of these approaches. But in practice, in the right context and with the right constraints, all of these methods work fine. Tech leads are responsible for choosing the best approach for their context. Note: If you are interested in dual writes, watch my Red Hat Summit 2021 session, where I covered in depth. You can also from my presentation. Currently, I am involved with Red Hat OpenShift Streams for Apache Kafka, a . It takes less than a minute to start and is completely free during the trial period. and help us shape it with your early feedback. If you have questions or comments about this article, hit me on Twitter and let’s get started. THE DUAL WRITE PROBLEM The single indicator that you may have a dual write problem is the need to write to more than one system of record predictably. This requirement might not be obvious and it can express itself in different ways in the distributed systems design process. For example: * You have chosen the best tools for each job and now you have to update a NoSQL database, a search index, and a cache as part of a single business transaction. * The service you have designed has to update its database and also send a notification to another service about the change. * You have business transactions that span multiple services boundaries. * You may have to implement service operations as idempotent because consumers have to retry failed invocations. For this article, we'll use a single example scenario to evaluate the various approaches to handling dual writes in distributed transactions. Our scenario is a client application that invokes a microservice on a mutating operation. Service A has to update its database, but it also has to call Service B on a write operation, as illustrated in Figure 1. The actual type of the database, the protocol of the service-to-service interactions, is irrelevant for our discussion as the problem remains the same. Figure 1: The dual write problem in microservices. A small but critical clarification explains why there are no simple solutions to this problem. If Service A writes to its database and then sends a notification to a queue for Service B (let’s call it a local-commit-then-publish approach), there is still a chance the application won't work reliably. While Service A writes to its database and then sends the message to a queue, there is a small probability of the application crashing after the commit to the database and before the second operation, which would leave the system in an inconsistent state. If the message is sent before writing to the database (let’s call this approach publish-then-local-commit), there is a possibility of database write failing or timing issues where Service B receives the event before Service A has committed the change to its database. In either case, this scenario involves dual writes to a database and a queue, which is the core problem we are going to explore. In the next sections, I will discuss the various implementation approaches available today for this always-present challenge. THE MODULAR MONOLITH Developing your application as a modular monolith might seem like a hack or going backward in architectural evolution, but I have seen it work fine in practice. It is not a microservices pattern but an exception to the microservices rule that can be combined cautiously with microservices. When strong write consistency is the driving requirement, more important even than the ability to deploy and scale microservices independently, then you could go with the modular monolith architecture. Having a monolithic architecture does not imply that the system is poorly designed or bad. It does not say anything about quality. As the name suggests, it is a system designed in a modular way with exactly one deployment unit. Note that this is a purposefully designed and implemented , which is different from an accidentally created monolith that grows over time. In a purposeful modular monolith architecture, every module follows the microservices principles. Each module encapsulates all the access to its data, but the operations are exposed and consumed as in-memory method calls. THE ARCHITECTURE OF A MODULAR MONOLITH With this approach, you have to convert both microservices (Service A and Service B) into library modules that can be deployed into a shared runtime. You then make both microservices share the same database instance. Because the services are written and deployed as libraries in a common runtime, they can participate in the same transactions. Because the modules share a database instance, you can use a local transaction to commit or rollback all changes at once. There are also differences around the deployment method because we want the modules to be deployed as libraries within a bigger deployment, and to participate in existing transactions. Even in a monolithic architecture, there are ways to isolate the code and data. For example, you can segregate the modules into separate packages, build modules, and source code repositories, which can be owned by different teams. You can do partial data isolation by grouping tables by naming convention, schemas, database instances, or even by database servers. The diagram in Figure 2, inspired by Axel Fontaine's talk on , illustrates the different code- and data-isolation levels in applications.  Figure 2: Levels of code and data isolation for applications. The last piece of the puzzle is to use a runtime and a wrapper service capable of consuming other modules and including them in the context of an existing transaction. All of these constraints make the modules more tightly coupled than typical microservices, but the benefit is that the wrapper service can start a transaction, invoke the library modules to update their databases, and commit or roll back the transaction as one operation, without concerns about partial failure or eventual consistency. In our example, illustrated in Figure 3, we have converted Service A and Service B into libraries and deployed them into a shared runtime, or one of the services could act as the shared runtime. The tables from the databases also share a single database instance, but it is separated as a group of tables managed by the respective library services.  Figure 3: Modular monolith with a shared database. BENEFITS AND DRAWBACKS OF THE MODULAR MONOLITH In some industries, it turns out the benefits of this architecture are far more important than the faster delivery and pace of change that are so highly valued at other places. Table 1 summarizes the benefits and drawbacks of the modular monolith architecture. Table 1: Benefits and drawbacks of the modular monolith architecture. Benefits Simple transaction semantics with local transactions ensuring data consistency, read-your-writes, rollbacks, and so on. Drawbacks * A shared runtime prevents us from independently deploying and scaling modules, and prevents failure isolation. * The logical separation of tables in a single database is not strong. With time, it can turn into a shared integration layer. * Module coupling and sharing transaction context requires coordination during the development stage and increases the coupling between services. Examples * Runtimes such as Apache Karaf and WildFly that allow modular and dynamic deployment of services. * Apache Camel’s direct and direct-vm components allow exposing operations for in-memory invocations and preserve transaction contexts within a JVM process. * Apache Isis is one of the best examples of the modular monolith architecture. It enables domain-driven application development by automatically generating a UI and REST APIs for your Spring Boot applications. * Apache OFBiz is another example of a modular monolith and service-oriented architecture (SOA). It is a comprehensive enterprise resource planning system with hundreds of tables and services that can automate enterprise business processes. Despite its size, its modular architecture allows developers to quickly understand and customize it. Distributed transactions are typically the last resort, used in a variety of instances: * When writes to disparate resources cannot be eventually consistent. * When we have to write to heterogeneous data sources. * When exactly-once message processing is required and we cannot refactor a system and make its operations idempotent. * When integrating with third-party black-box systems or legacy systems that implement the two-phase commit specification. In all of these situations, when scalability is not a concern, we might consider distributed transactions an option. IMPLEMENTING THE TWO-PHASE COMMIT ARCHITECTURE The technical requirements for two-phase commit are that you need a distributed transaction manager such as and a reliable storage layer for the transaction logs. You also need -compatible data sources with associated XA drivers that are capable of participating in distributed transactions, such as RDBMS, message brokers, and caches. If you are lucky to have the right data sources but run in a dynamic environment, such as Kubernetes, you also need an operator-like mechanism to ensure there is only a single instance of the distributed transaction manager. The transaction manager must be highly available and must always have access to the transaction log. For implementation, you could explore a that uses the for singleton purposes and persistent volumes to store transaction logs. In this category, I also include specifications such as (WS-AtomicTransaction) for SOAP web services. What all of these technologies have in common is that they implement the XA specification and have a central transaction coordinator. In our example, shown in Figure 4, Service A is using distributed transactions to commit all changes to its database and a message to a queue without leaving any chance for duplicates or lost messages. Similarly, Service B can use distributed transactions to consume the messages and commit to Database B in a single transaction without any duplicates. Or, Service B can choose not to use distributed transactions, but use local transactions and implement the idempotent consumer pattern. For the record, a more appropriate example for this section would be using WS-AtomicTransaction to coordinate the writes to Database A and Database A in a single transaction and avoid eventual consistency altogether. But that approach is even less common, these days, than what I've described. Figure 4: Two-phase commit spanning between a database and a message broker. BENEFITS AND DRAWBACKS OF THE TWO-PHASE COMMIT ARCHITECTURE The two-phase commit protocol offers similar guarantees to local transactions in the modular monolith approach, but with a few exceptions. Because there are two or more separate data sources involved in an atomic update, they may fail in a different manner and block the transaction. But thanks to its central coordinator, it is still easy to discover the state of the distributed system compared to the other approaches I will discuss. Table 2 summarizes the benefits and drawbacks of this approach. Table 2: Benefits and drawbacks of two-phase commit. Benefits * Standard-based approach with out-of-the-box transaction managers and supporting data sources. * Strong data consistency for the happy scenarios. Drawbacks * Scalability constraints. * Possible recovery failures when the transaction manager fails. * Limited data source support. * Storage and singleton requirements in dynamic environments. Examples * The (formerly Java Transaction API) * WS-AtomicTransaction * JTS/IIOP * eBay’s * Atomikos * Narayana * Message brokers such as Apache ActiveMQ * Relational data sources that implement the XA spec, in-memory data stores such as Infinispan ORCHESTRATION With a modular monolith, we use local transactions and we always know the state of the system. With distributed transactions based on the two-phase commit protocol, we also guarantee a consistent state. The only exception would be an unrecoverable failure that involved the transaction coordinator. But what if we wanted to ease the consistency requirements while still knowing the state of the overall distributed system and coordinating from a single place? In this case, we might consider an orchestration approach, where one of the services acts as the coordinator and orchestrator of the overall distributed state change. The orchestrator service has the responsibility to call other services until they reach the desired state or take corrective actions if they fail. The orchestrator uses its local database to keep track of state changes, and it is responsible for recovering any failures related to state changes. IMPLEMENTING AN ORCHESTRATION ARCHITECTURE The most popular implementations of the orchestration technique are BPMN specification implementations such as the and projects. The need for such systems doesn’t disappear with overly distributed architectures such as microservices or serverless; on the contrary, it increases. For proof, we can look to newer stateful orchestration engines that do not follow a specification but provide similar stateful behavior, such as Netflix’s , Uber’s , and Apache's . Serverless stateful functions such as Amazon StepFunctions, Azure Durable Functions, and Azure Logic Apps are in this category, as well. There are also open source libraries that allow you to implement stateful coordination and rollback behavior such as Apache Camel’s pattern implementation and the NServiceBus capability. The many homegrown systems implementing the Saga pattern are also in this category.   Figure 5: Orchestrating distributed transactions between two services. In our example diagram, shown in Figure 5, we have Service A acting as the stateful orchestrator responsible to call Service B and recover from failures through a compensating operation if needed. The crucial characteristic of this approach is that Service A and Service B have local transaction boundaries, but Service A has the knowledge and the responsibility to orchestrate the overall interaction flow. That is why its transaction boundary touches Service B endpoints. In terms of implementation, we could set this up with synchronous interactions, as shown in the diagram, or using a message queue in between the services (in which case you could use a two-phase commit, too). BENEFITS AND DRAWBACKS OF ORCHESTRATION Orchestration is an eventually consistent approach that may involve retries and rollbacks to get the distribution into a consistent state. While it avoids the need for distributed transactions, orchestration requires the participating services to offer idempotent operations in case the coordinator has to retry an operation. Participating services also must offer recovery endpoints in case the coordinator decides to roll back and fix the global state. The big advantage of this approach is the ability to drive heterogeneous services that might not support distributed transactions into a consistent state by using only local transactions. The coordinator and the participating services need only local transactions, and it is always possible to discover the state of the system by asking the coordinator, even if it is in a partially consistent state. Doing that is not possible with the other approaches I will describe. Table 3: Benefits and drawbacks of orchestration. Benefits * Coordinates state among heterogeneous distributed components. * No need for XA transactions. * Known distributed state at the coordinator level. Drawbacks * Complex distributed programming model. * May require idempotency and compensating operations from the participating services. * Eventual consistency. * Possibly unrecoverable failures during compensations. Examples * jBPM * Camunda * MicroProfile * Conductor * Cadence * Step Functions * Durable Functions * Apache Camel Saga pattern implementation * NServiceBus Saga pattern implementation * The CNCF specification * Homegrown implementations CHOREOGRAPHY As you've seen in the discussion so far, a single business operation can result in multiple calls among services, and it can take an indeterminate amount of time before a business transaction is processed end-to-end. To manage this, the orchestration pattern uses a centralized controller service that tells the participants what to do. An alternative to orchestration is choreography, which is a style of service coordination where participants exchange events without a centralized point of control. With this pattern, each service performs a local transaction and publishes events that trigger local transactions in other services. Each component of the system participates in decision-making about a business transaction's workflow, instead of relying on a central point of control. Historically, the most common implementation for the choreography approach was using an asynchronous messaging layer for the service interactions. Figure 6 illustrates the basic architecture of the choreography pattern.   Figure 6: Service choreography through a messaging layer. CHOREOGRAPHY WITH A DUAL WRITE For message-based choreography to work, we need each participating service to execute a local transaction and trigger the next service by publishing a command or event to a messaging infrastructure. Similarly, other participating services have to consume a message and perform a local transaction. That in itself is a dual-write problem within a higher-level dual-write problem. When we develop a messaging layer with a dual write to implement the choreography approach, we could design it as a two-phase commit that spans a local database and a message broker. I covered that approach earlier. Alternatively, we might use a publish-then-local-commit or local-commit-then-publish pattern: * Publish-then-local-commit: We could try to publish a message first and then commit a local transaction. While this option might sound fine, it has practical challenges. For example, very often you need to publish an ID that is generated from the local transaction commit, which won’t be available to publish. Also, the local transaction might fail, but we cannot rollback the published message. This approach lacks read-your-write semantics and it is an impractical solution for most use cases. * Local-commit-then-publish: A slightly better approach would be to commit the local transaction first and then publish the message. This has a small probability of failure occurring after a local transaction has been committed and before publishing the message. But even in that case, you could design your services to be idempotent and retry the operation. That would mean committing the local transaction again and then publishing the message. This approach can work if you control the downstream consumers and can make them idempotent, too. It's also a pretty good implementation option overall. CHOREOGRAPHY WITHOUT A DUAL WRITE The various ways of implementing a choreography architecture constrain every service to write only to a single data source with a local transaction, and nowhere else. Let’s see how that could work without a dual write. Let’s say Service A receives a request and writes it to Database A, and nowhere else. Service B periodically polls Service A and detects new changes. When it reads the change, Service B updates its own database with the change and also the index or timestamp up to which it picked up the changes. The critical part here is the fact that both services only write to their own database and commit with a local transaction. This approach, illustrated in Figure 7, can be described as service choreography, or we could describe it using the good old data pipeline terminology. The possible implementation options are more interesting. Figure 7: Service choreography through polling. The simplest scenario is for Service B to connect to the Service A database and read the tables owned by Service A. The industry tries to avoid that level of coupling with shared tables, however, and for a good reason: Any change in Service A's implementation and data model could break Service B. We can make a few gradual improvements to this scenario, for example by using the and giving Service A a table that acts as a public interface. This table could only contain the data Service B requires, and it could be designed to be easy to query and track for changes. If that is not good enough, a further improvement would be for Service B to ask Service A for any changes through an API management layer rather than connecting directly to Database A. Fundamentally, all of these variations suffer from the same drawback: Service B has to poll Service A continuously. Doing this can lead to unnecessary continuous load on the system or unnecessary delay in picking up the changes. Polling a microservice for changes is a hard sell, so let’s see what we can do to further improve this architecture. CHOREOGRAPHY WITH DEBEZIUM One way to improve a choreography architecture and make it more attractive is to introduce a tool like , which we can use to perform change data capture (CDC) using Database A’s transaction log. Figure 8 illustrates this approach.  Figure 8: Service choreography with change data capture. Debezium can monitor a database's transaction log, perform any necessary filtering and transformation, and deliver relevant changes into an Apache Kafka topic. This way, Service B can listen to generic events in a topic rather than polling Service A's database or APIs. Swapping database polling for streaming changes and introducing a queue between the services makes the distributed system more reliable, scalable, and opens up the possibility of introducing other consumers for new use cases. Using Debezium offers an elegant way to implement the for orchestration- or choreography-based . A side-effect of this approach is that it introduces the possibility of Service B receiving duplicate messages. This can be addressed by implementing the service as idempotent, either at the business logic level or with a technical deduplicator (with something like Apache ActiveMQ Artemis’s or Apache Camel's idempotent consumer pattern). CHOREOGRAPHY WITH EVENT SOURCING Event sourcing is another implementation of the service choreography approach. With this pattern, the state of an entity is stored as a sequence of state-changing events. When there is a new update, rather than updating the entity's state, a new event is appended to the list of events. Appending new events to an event store is an atomic operation done in a local transaction. The beauty of this approach, shown in Figure 9, is that the event store also behaves like a message queue for other services to consume updates. Figure 9: Service choreography through event sourcing. Our example, when converted to use event sourcing, would store client requests in an append-only event store. Service A can reconstruct its current state by replaying the events. The event store also needs to allow Service B to subscribe to the same update events. With this mechanism, Service A uses its storage layer also as the communication layer with other services. While this mechanism is very neat and solves the problem of reliably publishing events whenever the state change occurs, it introduces a new programming style unfamiliar to many developers and additional complexity around state reconstruction and message compaction, which require specialized data stores. BENEFITS AND DRAWBACKS OF CHOREOGRAPHY Regardless of the mechanism used to retrieve data changes, the choreography approach decouples writes, allows independent service scalability, and improves overall system resiliency. The downside of this approach is that the flow of decision-making is decentralized and it is hard to discover the globally distributed state. Discovering the state of a request requires querying multiple data sources which can be challenging with a large number of services. Table 4 summarizes the benefits and drawbacks of this approach. Table 4: Benefits and drawbacks of choreography. Benefits * Decouples implementation and interaction. * No central transaction coordinator. * Improved scalability and resilience characteristics. * Near real-time interactions. * Less overhead on the system with Debezium and similar tools. Drawbacks * The global system state and coordination logic is scattered across all participants. * Eventual consistency. Examples * Homegrown database or API polling implementations. * The outbox pattern * Choreography based on the Saga pattern * * * * * * * * PARALLEL PIPELINES With the choreography pattern, there is no central place to query the state of the system, but there is a sequence of services that propagates the state through the distributed system. Choreography creates a sequential pipeline of processing services, so we know that when a message reaches a certain step of the overall process, it has passed all the previous steps. What if we could loosen this constraint and process all the steps independently? In this scenario, Service B could process a request regardless of whether Service A had processed it or not. With parallel pipelines, we add a router service that accepts requests and forwards them to Service A and Service B through a message broker in a single local transaction. From this step onward, as shown in Figure 10, both services can process the requests independently and in parallel. Figure 10: Processing through parallel pipelines. While this pattern is very simple to implement, it is only applicable to situations where there is no temporal binding between the services. For example, Service B should be able to process the request regardless of whether Service A has processed the same request. Also, this approach requires an additional router service or the client being aware of both Service A and B for targeting the messages. LISTEN TO YOURSELF There is a lighter alternative to this approach, known as the pattern, where one of the services also acts as the router. With this alternative approach, when Service A receives a request, it would not write to its database but would instead publish the request into the messaging system, where it is targeted to Service B, and to itself. Figure 11 illustrates this pattern.  Figure 11: The Listen to yourself pattern. The reason for not writing to the database is to avoid dual writes. Once a message is in the messaging system, the message goes to Service B, and also it goes to back Service A in a completely separate transaction context. With that twist of the processing flow, Service A, and Service B can independently process the request and write to their respective databases. BENEFITS AND DRAWBACKS OF PARALLEL PIPELINES Table 5 summarizes the benefits and drawbacks of using parallel pipelines. Table 5: Benefits and drawbacks of parallel pipelines. Benefits Simple, scalable architecture for parallel processing. Drawbacks Requires temporal dismantling; hard to reason about the global system state. Examples Apache Camel’s multicast and splitter with parallel processing. HOW TO CHOOSE A DISTRIBUTED TRANSACTIONS STRATEGY As you might have already guessed from this article, there is no right or wrong pattern for handling distributed transactions in a microservices architecture. Every pattern has its pros and cons. Each pattern solves some problems while generating others in turn. The chart in Figure 12 offers a short summary of the main characteristics of the dual write patterns I've discussed.  Figure 12: Characteristics of dual write patterns. Whatever approach you choose, you will need to explain and document the motivation behind the decision and the long-lasting architectural consequences of your choice. You will also need to get support from the teams that will implement and maintain the system in the long term. I like to organize and evaluate the approaches described in this article based on their data consistency and scalability attributes, as shown in Figure 13.    Figure 13: Relative data consistency and scalability characteristics of dual write patterns. As a good starting point, we could evaluate the various approaches from the most scalable and highly available to the least scalable and available ones. HIGH: PARALLEL PIPELINES AND CHOREOGRAPHY If your steps are temporarily decoupled, then it could make sense to run them in a parallel pipelines method. The chances are you can apply this pattern for certain parts of the system, but not for all of them. Next, assuming there is a temporal coupling between the processing steps, and certain operations and services have to happen before others, you might consider the choreography approach. Using service choreography, it is possible to create a scalable, event-driven architecture where messages flow from service to service through a decentralized orchestration process. In this case, Outbox pattern implementations with Debezium and Apache Kafka (such as ) are particularly interesting and gaining traction. MEDIUM: ORCHESTRATION AND TWO-PHASE COMMIT If choreography is not a good fit, and you need a central point that is responsible for coordination and decision making, then you would consider orchestration. This is a popular architecture, with standard-based and custom open source implementations available. While a standard-based implementation may force you to use certain transaction semantics, a custom orchestration implementation allows you to make a trade-off between the desired data consistency and scalability. LOW: MODULAR MONOLITH If you are going further left in the spectrum, most likely you have a very strong need for data consistency and you are ready to pay for it with significant tradeoffs. In this case, distributed transactions through two-phase commits will work with certain data sources, but they are difficult to implement reliably on dynamic cloud environments designed for scalability and high availability. In that case, you can go all the way to the good old modular monolith approach, accompanied by practices learned from the microservices movement. This approach ensures the highest data consistency but at the price of runtime and data source coupling. CONCLUSION In a sizable distributed system with tens of services, there won’t be a single approach that works for all, but a few of these combined and applied for different contexts. You might have a few services deployed on a shared runtime for exceptional requirements around data consistency. You might choose a two-phase commit for integration with a legacy system that supports JTA. You might orchestrate a complex business process, and also use choreography and parallel processing for the rest of the services. In the end, it doesn't matter what strategy you pick; what matters is choosing a strategy deliberately for the right reasons, and executing it. This post was originally published on Red Hat Developers. To read the original post, check .</content><dc:creator>Unknown</dc:creator></entry><entry><title>No-code and low-code integrations with Camel and Kaoto</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/02/no-code-and-low-code-integrations-camel-and-kaoto" /><author><name>Maria Arias de Reyna Dominguez</name></author><id>cf2eaab9-30af-482e-9f53-8e838b439a91</id><updated>2022-05-02T07:00:00Z</updated><published>2022-05-02T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://camel.apache.org"&gt;Apache Camel&lt;/a&gt; and the &lt;a href="https://kaoto.io"&gt;Kaoto&lt;/a&gt; graphical editor can work together to provide a no-code or low-code environment to simplify the &lt;a href="https://delawen.com/2020/10/what-is-integration/"&gt;integration of services&lt;/a&gt; into &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; applications. Integrations with &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; can &lt;a href="https://medium.com/trueengineering/a-review-of-microservice-orchestration-frameworks-d22797b34ea5"&gt;be complex&lt;/a&gt;. This article introduces criteria for a no-code environment and shows how Apache Camel and Kaoto achieve this goal for microservices and containers.&lt;/p&gt; &lt;h2&gt;The right director for your symphony&lt;/h2&gt; &lt;p&gt;Several criteria have to be considered when choosing an integration framework:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Functionality:&lt;/strong&gt; The most obvious criterion is a framework that's as complete as possible. Our framework should feature not only &lt;a href="https://www.enterpriseintegrationpatterns.com/"&gt;Enterprise Integration Patterns&lt;/a&gt;, but also a wide range of connectors to different protocols and formats. We want to be able to extract, transform, and load data in a painless way. We would prefer not to write code or custom scripts to complement the framework.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connector extensibility:&lt;/strong&gt; If we need to connect to some protocol or use some data format not currently supported by the framework, will we be able to extend it to cover our use case? What if we need to connect to services with noninteroperable authentication systems? Can we add custom building blocks to our orchestration workflow?&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Decoupling:&lt;/strong&gt; A good integration framework must keep the interactions between components to a minimum so that the technical debt doesn't grow exponentially. We need to be able to upgrade the framework without breaking our services. And we want to be able to modify, extend, upgrade, and improve our services without worrying whether the integration will work.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Usability:&lt;/strong&gt; Ease of use, especially if we have complex use cases to maintain, helps keep our software architecture clean. But usability should never be detrimental to the functionality or extensibility of the framework. Sacrificing features in favor of user experience will make you hit the ceiling of your application's potential faster.&lt;/p&gt; &lt;p&gt;If you want good usability, you can't tolerate bad documentation. Users want to have some reference to fall back on when something doesn't work as expected.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Size:&lt;/strong&gt; We don't want to drag a heavyweight dependency into our project, especially if the integration makes our application slower or imposes a bigger footprint on our hardware.&lt;/p&gt; &lt;p&gt;Having a lightweight dependency usually also means less source code, and less source code means fewer opportunities to bring bugs into your software ecosystem.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multiple languages:&lt;/strong&gt; We don't want to be tied to a specific language like &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; or &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;. Our library should be able to interact with a varied range of components and services.&lt;/p&gt; &lt;p&gt;This flexibility is especially important if our citizen integrators or users come from different backgrounds, or are not very tech-savvy. We want them to feel comfortable in whatever language they need to orchestrate.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Technical support&lt;/strong&gt;: What if we have a problem? Is there someone we can pay for support—or maybe multiple someones? Having a wide range of companies offering technical support will improve our productivity in the long run.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;strong&gt;License:&lt;/strong&gt; Whether or not there is a strong company behind the framework, only free and open source software (FOSS) protects us from the whims or misfortunes of any single company.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Fortunately for us, there is a framework that checks off all these criteria with excellent marks: &lt;a href="https://camel.apache.org"&gt;Apache Camel&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Apache Camel&lt;/h2&gt; &lt;p&gt;Apache Camel is an integration framework that automates the integration of databases, communication protocols, and other frameworks into your application. Camel offers &lt;a href="https://developers.redhat.com/articles/2022/03/14/choose-best-camel-your-integration-ride-part-1"&gt;many ways to achieve integrations&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Camel has its own domain specific languages (DSLs) that are extensions of, or are based on, existing programming languages and interoperable formats. Java, &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt;, XML, YAML ... all of these languages can translate a simple Camel route into an integration that can be deployed (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/camel.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/camel.png?itok=gZ5Xa7xf" width="783" height="99" alt="Routes written in popular languages are processed by Camel to produceJava integrations." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Routes written in popular languages are processed by Camel to produceJava integrations. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Routes written in popular languages are processed by Camel to produce Java integrations.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Camel Quarkus&lt;/h3&gt; &lt;p&gt;The Java code produced by Camel for an integration sometimes uses a bigger memory footprint than necessary. This waste is avoided through the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; build for Apache Camel. Camel Quarkus looks exactly the same as creating traditional integrations with Apache Camel from the user or developer perspective, but brings all the &lt;a href="https://quarkus.io/"&gt;developer joy&lt;/a&gt; of native builds and deployments to the integration world by providing &lt;a href="https://camel.apache.org/camel-quarkus"&gt;Quarkus extensions for Apache Camel&lt;/a&gt; (Figure 2).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/quarkus.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/quarkus.png?itok=9Ug78c35" width="783" height="198" alt="Apache Camel Quarkus works next to Camel to create integrations." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Apache Camel Quarkus works next to Camel to create integrations. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Apache Camel Quarkus works next to Camel to create integrations.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;Camel K and Kamelets&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/camel-k"&gt;Camel K&lt;/a&gt; provides Kubernetes-native integration for Camel orchestrations. With Camel Quarkus built in, Camel K deploys Camel integrations to your Kubernetes cluster and monitors the connections during runs.&lt;/p&gt; &lt;p&gt;But Camel K offers much more than just Kubernetes integrations. You can use &lt;a href="https://developers.redhat.com/blog/2021/04/02/design-event-driven-integrations-with-kamelets-and-camel-k"&gt;Kamelets&lt;/a&gt; to simplify your integration definitions (Figure 3). Kamelets are snippets of Camel routes that act as metasteps in an integration, hiding complexities in otherwise simple orchestrations.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kamelet.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kamelet.png?itok=MGFZIG4k" width="840" height="343" alt="Kamelets are input to Kamel K to produce integrations for Kubernetes." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. Kamelets are input to Kamel K to produce integrations for Kubernetes. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Kamelets are input to Kamel K to produce integrations for Kubernetes.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;No-code integrations with Kaoto&lt;/h2&gt; &lt;p&gt;The obvious missing step in the previous diagrams was a visual editor that can transform the graphical representation of the workflow into source code, allowing people to integrate new services and connections without writing a single line of code. The desire to make integration frameworks accessible and easy to use is not new. There have been different approaches to a graphical solution to this problem; these always come back to the question of how to visualize source code without exceeding the &lt;a href="https://en.wikipedia.org/wiki/Deutsch_limit"&gt;Deutsch limit&lt;/a&gt;, which states that you shouldn't have more than 50 visual primitives on your screen at the same time.&lt;/p&gt; &lt;p&gt;At Red Hat, we have been searching for a free and open source tool for no-code or low-code integration that uses Apache Camel. Not finding a suitable tool, we decided to create &lt;a href="https://kaoto.io"&gt;Kaoto&lt;/a&gt;. Figure 4 shows how it sets up the whole workflow. The primitives or building blocks of Kaoto are steps in an Apache Camel integration DSL: Kamelets, Camel components, or Enterprise Integration Patterns.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kaolo.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kaolo.png?itok=wX5bhc8a" width="1118" height="253" alt="Kaoto produces the Kamelets, bindings, and routes as input to Camel K." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Kaoto produces the Kamelets, bindings, and routes as input to Camel K. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Kaoto produces the Kamelets, bindings, and routes as input to Camel K.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;We wanted Kaoto to have a simple user interface that provides building blocks for integrations with drag-and-drop manipulation. At the same time, we wanted our users to be able to manipulate the source code. With this flexibility, Kaoto can be transparent about what the users are building and users can safely deploy integrations without blindly trusting the integration editor.&lt;/p&gt; &lt;h3&gt;What does a no-code integration look like?&lt;/h3&gt; &lt;p&gt;In a no-code solution, there is no need to interact with or even see the source code at any point. The user can focus on bringing integration capabilities to their architecture without worrying about implementation details. Figure 5 is an animation showing Kaoto adding a logging service to an application.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/no.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/no.gif" width="1012" height="794" alt="Kaoto lets users create an integration through searches and clicks." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. Kaoto lets users create an integration through searches and clicks. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Kaoto lets users create an integration through searches and clicks.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h3&gt;What does a low-code integration look like?&lt;/h3&gt; &lt;p&gt;A low-code solution allows the user to view and interact with the source code to deploy an integration. The user can focus on the features being implemented without knowing how to write the source code. The source code appears as an adjacent add-on or guide to help new users get familiar with concepts. Figure 6 is an animation showing Kaoto adding a Kafka broker to an application with source code displayed.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/low.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/low.gif" width="1012" height="794" alt="Kaoto lets users interact with code through searches and clicks." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6. Kaoto lets users interact with code through searches and clicks. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Kaoto lets users create an integration through searches and clicks.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;You can find more &lt;a href="https://kaoto.io/portfolio/"&gt;use cases and examples on the Kaoto website&lt;/a&gt;. Kaoto is the next big step in the ongoing quest to simplify the integration of multiple capabilities into an application.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/02/no-code-and-low-code-integrations-camel-and-kaoto" title="No-code and low-code integrations with Camel and Kaoto"&gt;No-code and low-code integrations with Camel and Kaoto&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Maria Arias de Reyna Dominguez</dc:creator><dc:date>2022-05-02T07:00:00Z</dc:date></entry><entry><title type="html">Portfolio Architecture Examples - Data Engineering Collection</title><link rel="alternate" href="http://www.schabell.org/2022/05/portfolio-architecture-examples-data-engineering-collection.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/05/portfolio-architecture-examples-data-engineering-collection.html</id><updated>2022-05-02T05:00:00Z</updated><content type="html">Figure 1: The portfolio architecture process For a few years now we've been working on a project we have named . These are based on selecting a specific use case we are seeing used in the real world by customers and then finding implementations of that case using three or more products from the Red Hat portfolio. This basic premise is used as the foundation, but many aspects of open source are included in both the process and the final product we have defined. There is a community, where we share the initial project kickoff with a group of architects and use their initial feedback from the start. We also present the architecture product we've created right at the end before we publish to ensure usability by architects in the field. The final publish product includes some internal only content around the customer projects researched, but most of the content is  through various open source channels.  This article is sharing an overview of the product we've developed, what's available to you , and concludes by sharing a collection of architectures we've published. INTRODUCTION The basis of a portfolio architecture is a use case, two to three actual implementations that can be researched, and includes the use of a minimum of three products. This is the ideal foundation for a project to start, but we encountered a problem with use cases containing emerging technologies or emerging domains in the market. To account for these we've chosen to note the fact that these are opinionated architectures based on internal reference architectures.  The product has been defined as complete for publishing when it contains the following content: * Short use case definition * Diagrams - logical, schematic (physical), and detail diagrams * Public slide deck containing the use case story and architecture diagrams * Internal slide deck containing both the public deck content and the confidential customer research * Video (short) explanation of the architecture * Either a technical brief document or one or more articles covering the solution architecture Note that the items in italics are all available to anyone  in the Red Hat Portfolio Architecture Center or in the Portfolio Architecture Examples repository. FIGURE 2: LOGICAL DIAGRAM DESIGN TEMPLATE TOOLING AND WORKSHOPS The progress towards our products required a good idea of how we wanted to diagram our architectures. We chose to keep them very generic and simple in style to facilitate all levels of conversation around a particular use case without getting bogged down in notational discussions.  A simple three level design for our architectures was captured by using logical, schematic, and detail diagrams. All of these have been integrated in  with pre-defined templates and icons for easily getting started. Furthermore, we've developed a tooling workshop to quickly ramp up on the design methods and tooling we've made available. It's called , has been featured in several. DATA ENGINEERING COLLECTION The collection featured today is centered around architectures leveraging data engineering concepts and tooling. There are currently eight architectures in this collection and we'll provide a short overview of each, leaving the in depth exploration as an exercise for the reader. Figure 3: Data engineering architecture collection In each of these architecture overviews you'll find a table of contents outlining the technologies used, several example schematic diagrams with descriptions, and a link in the last section to open the diagrams directly into the online tooling in your browser. This architecture covers the use case of providing a consistent infrastructure experience from cloud to edge and enabling modern containerized applications at edge. (Note: this project is a new architecture and currently in progress, so sharing one of the schematic architecture diagrams and you can monitor this project for updates as it progresses to completion.) This use case is bringing cloud like capabilities to the edge locations. This architecture covers the use case around data center to edge where the energy (utility) infrastructure companies operate across vast geographical area that connects the upstream drilling operations with downstream fuel processing and delivery to customers. These companies need to monitor the condition of pipeline and other infrastructure for operational safety and optimization. The use case is bringing computing closer to the edge by monitoring for potential issues with gas pipelines (edge). The manufacturing industry has consistently used technology to fuel innovation, production optimization and operations. Now, with the combination of edge computing and AI/ML, manufacturers can benefit from bringing processing power closer to data. This helps actions be taken faster on things like errors and predictive maintenance. The use case is for boosting manufacturing efficiency and product quality with artificial intelligence and machine learning out to the edge. This architecture covers edge medical diagnosis in the healthcare industry. It Accelerates medical diagnosis using condition detection in medical imagery with AI/ML at medical facilities. The use case is accelerating medical diagnosis using condition detection in medical imagery with AI/ML at medical facilities. Intelligent DaaS (Data as a Service) is about building and delivery of systems and platforms in a secure and scalable manner while driving data needs for moving towards consumerisation in healthcare. Feel free to explore this portfolio architecture by clicking on the diagram below. The use case is Intelligent Data as a Service (iDaaS) is about building and delivery of systems and platforms in a secure and scalable manner while driving data needs for moving towards consumerisation in healthcare. An offering of (near) real-time payments lets businesses, consumers, and even governments send and accept funds that provide both availability to the recipient and instant confirmation to the sender. Enabling real-time - or at least faster - payments that improve the speed of online payment experiences to customers has the potential to give banks a greater opportunity to win, serve, and retain their customers. By building solutions that capture real-time payment business, banks also can drive higher payment volumes, ideally at lower costs as well as engage new customer segments. The use case examines financial institutions enabling customers with fast, easy to use, and safe payment services available anytime, anywhere. Retail is the process of selling consumer goods or services to customers through multiple channels of distribution to earn a profit. A data framework refers to the process of managing enterprise retail data. The framework or system sets the guidelines and rules of engagement for business and management activities, especially those that deal with or result in the creation and manipulation of data. The use case is creating a framework for access to retail data from customers, stock, stores, and staff across multiple internal teams. This Portfolio Architecture targets energy providers in North America that need to be compliant with NERC regulations and in order to achieve this they decide to modernise the interfaces between their business applications and their SCADA systems also for a better consumption of information that can be used in combination with AI/ML and decision management tools to better address customer needs. The use case is providing interfaces with SCADA systems that are compliant with NERC regulations, creating different layers of API gateways to protect business service depending on the network zones. If you are interested in more architecture solutions like these, feel free to explore the . More architecture collections include: * Application development * * * * * * * *</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Support any JSON, metrics and CSV as dataset in Dashbuilder</title><link rel="alternate" href="https://blog.kie.org/2022/04/support-any-json-metrics-and-csv-as-dataset-in-dashbuilder.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/04/support-any-json-metrics-and-csv-as-dataset-in-dashbuilder.html</id><updated>2022-04-29T20:14:56Z</updated><content type="html">A few weeks ago we released . At that time a limited type of JSON was supported, now external datasets are more powerful than ever supporting any JSON format, CSV and metrics.  External datasets are the source of data that are outside Dashbuilder, hence they can be loaded on client side, removing the requirement of a backend to run dashbuilder. In this post we will share all the new features added to external datasets. By the way, you can try the new features in a quick-try . EXTERNAL DATASETS IMPROVEMENTS External datasets now support the following new features: * Headers: It is possible to declare headers. This is important to provide bearer tokens or any custom header * Columns: As stated in the announcement post, column information can be provided on the JSON itself otherwise dashbuilder “guess” the type for the columns and give it a generic name. Now it is possible to declare the columns to override the values coming from the JSON. * Transformation: Most of the cases the JSON you want to use will not be in the formats supported by Dashbuilder. To overcome this we now support transform expressions. Our selected tool is , but there are discussions to support in future versions. The transform expression can be provided using the field expression. The result expression will be applied on the input JSON and the result must be a JSON two dimension array. To find the right expression for your JSON you can use the .  More than simply transforming the input to an array, the support to expressions is also important to cover edge cases of a source and apply complex changes to the original source. Check for more information. * Inline JSON: You may not have access to the dataset during development. In this case you can use the new content field to declare an inline JSON two dimensional array that will serve as the dataset. NEW SUPPORTED FORMATS JSON is already supported by external datasets, and to extend the power of external datasets we introduced two new formats that are supported on the client (without the requirement of a backend): * CSV: Dashbuilder now has experimental support for CSV as the external data set. The only format it supports is the specified in with the following limitations: * The first line, supposedly the header, is always skipped * Fields with line breaks are not supported * Metrics: Nowadays Prometheus is the standard for keeping track of metrics of an application. It is possible to grab data directly from Prometheus REST API Using JSON and transformation. There are cases where the service is not connected to Prometheus and users may want to read directly from the metrics source, for this reason we transform content from URLs ending with “metrics” as a dataset, which makes it possible to read metrics without the need of a Prometheus installation. Bear in mind that it only reads a snapshot of the metrics, to keep the history of metrics you will still need a Prometheus. All these new external dataset formats are internally handled as a JSON two dimensional array, which means that you can still transform it using JSONata. CONCLUSION In this post we shared the new features available in the latest Dashbuilder release. Soon we will share real data visualizations and dashboards using popular APIs. Stay tuned! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title type="html">Serverless Workflow Expressions</title><link rel="alternate" href="https://blog.kie.org/2022/04/serverless-workflow-expressions.html" /><author><name>Francisco Javier Tirado Sarti</name></author><id>https://blog.kie.org/2022/04/serverless-workflow-expressions.html</id><updated>2022-04-28T19:27:15Z</updated><content type="html">Expressions are an essential feature of . They are everywhere and they are powerful. As you should already be aware if you have ever watched a superhero movie, with . In the universe, when discussing expressions, this famous sentence means there is a risk you will overuse them. Or, employing culinary terms, expressions are like the salt in a stew, you need to find out the right amount for your recipe to excel.   But what is exactly an expression? In a nutshell, a string that adheres to certain conventions established by a language specification that allows you to interact with the flow model. There are two terms in the previous sentence that deserve explanation: workflow data model and language specification. Every workflow instance is associated with a data model. This model, regardless if your flow file is written in or , consists of a JSON object.  The initial content of that object is set by the creator of the flow. As you already know, the flow can be started through a or a POST invocation. No matter which approach you use to start the flow, both the event or the request body will be a JSON object, which is expected to have a workflowdata property. Its value, typically another JSON object, will be used as the initial model. That model will be accessed/updated as part of the flow execution. Expressions are the mechanism defined by the for the states to interact with the model. Kogito supports two expression languages: and . The default one is jq, but you can change it to jsonpath by using the expressionLang property. Why these two languages? As you already guessed, since the flow model is a JSON object, it makes sense that the expression languages intended to interact with it are JSON-oriented ones. Why is jq the default? Because it is more powerful. In fact, jsonpath is not suitable for all use cases supported by the specification, as you will soon find out in this post.   Given this quick introduction, let’s discuss in this post some use cases for expressions: switch state conditions, action function args and state filtering.  SWITCH CONDITIONS Unlike a human, a flow does not have free will, its destiny is decided by the contents of the model and the flow designer. Conditions inside a switch state allow the flow designer to choose which path the flow should follow depending on the model content. A condition is an expression that returns a boolean when evaluated against the model. If the condition associated with a state transition returns true, that is the place for the flow to go.  For example, in repository, we are selecting which message should be displayed to the user depending on his language of choice: English or Spanish. In computational terms, if the value of the property language is english, the constant literal to be injected  on property message will be Hello from… , else if the value of the same property is spanish, then the injected message will be Saludos desde…. Using jq as expression language and JSON as workflow definition language, the switch state contains.  "dataConditions": [         {           "condition": "${ .language == \"English\" }",           "transition": "GreetInEnglish"         },         {           "condition": "${ .language == \"Spanish\" }",           "transition": "GreetInSpanish"         }       ] Using jsonpath as expression language and YAML as workflow definition language, the switch would have look like  dataConditions:       - condition: "${$.[?(@.language  == 'English')]}"         transition: GreetInEnglish       - condition: "${$.[?(@.language  == 'Spanish')]}"         transition: GreetInSpanish Note that, as required by the specification, in these examples, all expressions are embedded within ${… }.  But Kogito is smart enough to infer that the string within condition is an expression, so you can skip it and just write "dataConditions": [         {           "condition": ".language == \"English\"",           "transition": "GreetInEnglish"         },         {           "condition": ".language == \"Spanish\"",           "transition": "GreetInSpanish"         }] Which will behave the same and is a bit shorter.  FUNCTION ARGUMENTS One of the coolest things about Serverless Workflow Specification is the capability to define that can be invoked several times by the states of the flow. Every different function call might contain different arguments, which are specified using function arguments. A function definition example can be found in the repository. This flow performs two consecutive REST invocations to convert Fahrenheit to Celsius (a subtraction and a multiplication). Lets focus on the first function definition, the subtraction.  "functions": [ {   "name": "subtraction",   "operation": "specs/subtraction.yaml#doOperation" }] In the snippet above, we are defining a function called subtraction that performs an call. Kogito knows that the operation property defines an OpenAPI invocation because REST is the default operation type (adding “type”: “rest” will also work, but it is redundant). The OpenAPI specification URI is the sub-string before the # character in the operation property. The operationId is the sub-string after the #. In Kogito Serverless Workflow implementation, when an URI does not have a scheme, it is assumed to be located in the classpath of the Maven project. paths: /: post: operationId: doOperation requestBody: content: application/json: schema: $ref: '#/components/schemas/SubtractionOperation' responses: "200": description: OK components: schemas: SubtractionOperation: type: object properties: leftElement: format: float type: number rightElement: format: float type: number As you can see in the snippet above, subtraction.yaml specification file referenced in this example defines an operationId called doOperation, which expects two parameters: leftelement and rightelement. In order to invoke a function, we use a construct. It is composed by the refName (which should match the function definition name) and the arguments to be used in the function call.  Function arguments are expressed as a JSON object whose property values might be either a string containing an expression or any (string, number, boolean…). Note that in this example, the expression is not embedded within ${}. Kogito will infer it is a valid jq expression because of the . prefix, but you can embed it if you prefer to do so.  "functionRef": { "refName": "subtraction", "arguments": { "leftElement": ".fahrenheit", "rightElement": ".subtractValue" } } In the snippet above, we are specifying that the left number of the subtraction is equal to fahrenheit property (which is an input number provided by the user invoking the flow) and that the right element is equal to substractvalue property (which is a constant number injected into the flow model by SetConstants state). After resolving expression evaluation, the JSON object is used as a request body. "functionRef": { "refName": "subtraction", "arguments": "{leftElement: .fahrenheit, rightElement : .subtractValue}" } You can also write function arguments as a string containing an expression that returns a JSON object. You should be aware that this capability might not be supported by the expression language (jsonpath does not, hence one of the reasons why jq is the default expression language in the specification). Also, it is only suitable when the OpenAPI operation does not define any path, query or header parameter. The snippet above returns the same JSON object as in the previous one, but using a jq expression string rather than a JSON object. SETTING OPENAPI PARAMETERS In the previous example, the function arguments are used as the POST/PUT request body. But what happens if you want to set path, header or query parameters? In this case, you must use the JSON object approach, where the proper query, path or header parameters are extracted from arguments matching the name of the parameter. The remaining arguments in the JSON object, if any, will be used as  the request body.  Let’s illustrate it with an example, consider the following OpenAPI definition, which adds a header named pepe to multiplication operation id. paths: /: post: operationId: doOperation parameters: - in: header name: pepe schema: type: string required: false requestBody: content: application/json: schema: $ref: '#/components/schemas/MultiplicationOperation' responses: "200": description: OK components: schemas: MultiplicationOperation: type: object properties: leftElement: format: float type: number rightElement: format: float type: number You can set the value for header pepe by including a property named pepe into the function arguments using the JSON object approach, as in the snippet below, which sets the value of the header to pepa. As explained before, the resulting POST request body will just contain leftElement and rightElement. "functionRef": { "refName": "multiplication", "arguments": { "pepe":"pepa", "leftElement": ".subtraction.difference", "rightElement": ".multiplyValue" } } STATE FILTERING Let’s conclude our review of expression usage by considering this . The input model is an array of complex numbers (where x is the real coordinate and y the imaginary one) and the output model is the maximum value of the real coordinate within the input array. The flow consists of an action expression (defined inside squareState) that calculates the maximum x and the minimum y; an (defined inside squareState) that selects the maximum x as the action output that should be merged into the model; and a (defined inside finish state) that sets that max value as the whole model that will be returned to the caller. Let’s examine the three of them in detail.  "functions": [ { "name": "max", "type": "expression", "operation": "{max: .numbers | max_by(.x), min: .numbers | min_by(.y)}" } ] In the snippet above we define a max function of type expression. The operation property is a string containing a jq expression. This expression returns a JSON object, where max property is the maximum value of x coordinate in the input array and min property is the minimum value of y coordinate in the same array.  Now let’s go to the place where this function is invoked. "actions": [ { "name": "maxAction", "functionRef": { "refName": "max" }, "actionDataFilter": { "results" : ".max.x", "toStateData" : ".number" } } ] Since we are only interested in the maximum x, besides invoking the function using functionRef, we add an action data filter. If we do not add this filter, the whole JSON Object returned by the function call will be merged into the flow model. The filter has two properties: results, which selects the attribute and toStateData, which indicates the name of the target property inside the flow model (in case this property does not exist, it will be added). So, after executing the action, the flow model will consist of a number property storing the maximum value and the original numbers array. Then the flow moves to the next state: finish. "name": "finish", "type": "operation", "stateDataFilter": { "input": "{result: .number}" } Since we do not want to return the user input as a result of the flow execution, the final stage consists of a state data filter that sets the contents of the output model. Hence, we set the model to be a JSON object containing a property named result, whose value is the maximum number calculated by the previous state, stored in the number property. We do this  using the input property of the stateDataFilter construct, meaning that the model is changed before the state gets executed. So the final model content returned to the user contains a result property whose value is the maximum x. There are some aspects I would like to remark before jumping to the conclusions:  * The significant difference between action and state data filters. While the former just selects the portion of the action result that will be merged into the model, overriding only those properties in the flow model that share the name with the selected action result, the latter sets the whole flow model to the JSON object returned by the expression, discarding any existing property.  * Since stateDataFilter is expected to return a whole JSON object, jq is the only real valid option if you are using this construct (remember jsonpath expressions are not able to build new JSON objects) * We use expression strings that are not embedded within ${..}. As explained before, this is just syntactic sugar. You can use the approach you prefer. CONCLUSION This post illustrates usage of expressions in three main areas of Serverless Workflow specification: branching using Switch State, invoking OpenAPI services with arguments that are extracted from the flow model, and manipulating the flow model using jq expression language. Now is your turn to combine the three of them to perform complex and unrestricted service orchestration. The only limit is your imagination. The post appeared first on .</content><dc:creator>Francisco Javier Tirado Sarti</dc:creator></entry></feed>
