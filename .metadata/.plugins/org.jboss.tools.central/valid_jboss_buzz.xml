<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">What’s new in Jakarta EE 10</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jakarta-ee/whats-new-in-jakarta-ee-10/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jakarta-ee/whats-new-in-jakarta-ee-10/</id><updated>2022-04-08T15:56:30Z</updated><content type="html">Jakarta EE 10 is the first major release of Jakarta EE since the “jakarta” namespace update. Many of the component specifications are introducing Major or Minor version updates that are going to reflect in the implementation APIs. Let’s learn in this article what we can expect from the upcoming new major release. Project status Firstly, ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>3 ways to install a database with Helm charts</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/07/3-ways-install-database-helm-charts" /><author><name>Wanja Pernath</name></author><id>f6ca6909-f9d0-4fc8-873e-9f17b651b575</id><updated>2022-04-07T07:00:00Z</updated><published>2022-04-07T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/helm/all"&gt;Helm&lt;/a&gt; is a package manager for &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Helm uses a packaging format called &lt;em&gt;charts,&lt;/em&gt; which include all of the Kubernetes resources that are required to deploy an application, such as deployments, services, ingress, and so on. Helm charts are very useful for installing applications and performing upgrades on a Kubernetes cluster.&lt;/p&gt; &lt;p&gt;In chapter 3 of my e-book &lt;a href="https://developers.redhat.com/e-books/getting-gitops-practical-platform-openshift-argo-cd-and-tekton"&gt;&lt;em&gt;Getting GitOps: A Practical Platform with OpenShift, Argo CD and Tekton&lt;/em&gt;&lt;/a&gt;, I discuss the basics of creating and using Helm charts. I also dig into the the use case of creating a post-install and post-upgrade job.&lt;/p&gt; &lt;p&gt;However, that chapter provided a very basic example that focused only on what's necessary to create and deploy a Helm chart. This article will demonstrate some more advanced techniques to create a chart that could be installed more than once in the same namespace. It also shows how you could easily install a dependent database with your chart.&lt;/p&gt; &lt;p&gt;The source code for this example can be found in the &lt;a href="https://github.com/wpernath/book-example/tree/main/better-helm"&gt;GitHub repository that accompanies my book&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;The use case: How to install a dependent database with a Helm chart&lt;/h2&gt; &lt;p&gt;Chapter 1 of my book outlines the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;-based &lt;code&gt;person-service&lt;/code&gt;, a simple REST API service that reads and writes personal data from and into a PostgreSQL database. A Helm chart packages this service, and needs to provide all the dependencies necessary to successfully install it. As discussed in that chapter, you have three options to achieve that goal:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use the corresponding OpenShift template to install the necessary PostgreSQL database&lt;/li&gt; &lt;li&gt;Use the &lt;a href="https://github.com/CrunchyData/postgres-operator/"&gt;CrunchyData Postgres Operator&lt;/a&gt; (or any other Operator-defined PostgreSQL database extension) for the database&lt;/li&gt; &lt;li&gt;Install a dependent Helm chart, such as the &lt;a href="https://artifacthub.io/packages/helm/bitnami/postgresql"&gt;PostgreSQL chart by Bitnami&lt;/a&gt;, with your chart&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;No matter which route you take, however, you also need to ensure that your chart can be installed multiple times on each namespace. So let's tackle that task first.&lt;/p&gt; &lt;h2&gt;Make the chart installable multiple times in the same namespace&lt;/h2&gt; &lt;p&gt;The most crucial step for making your chart installable multiple times in the same namespace is to use generated names for all the manifest files. Therefore, you need an object called &lt;code&gt;Release&lt;/code&gt; with the following properties:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;Name&lt;/code&gt;: The name of the release&lt;/li&gt; &lt;li&gt;&lt;code&gt;Namespace&lt;/code&gt;: Where you are going to install the chart&lt;/li&gt; &lt;li&gt;&lt;code&gt;Revision&lt;/code&gt;: The revision number of this release (starts at 1 on install, and each update increments it by one)&lt;/li&gt; &lt;li&gt;&lt;code&gt;IsInstall&lt;/code&gt;: &lt;code&gt;true&lt;/code&gt; if it's an installation process&lt;/li&gt; &lt;li&gt;&lt;code&gt;IsUpgrade&lt;/code&gt;: &lt;code&gt;true&lt;/code&gt; if it's an upgrade process&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If you want to make sure that your chart installation won't conflict with any other installations in the same namespace, do the following:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: v1 kind: ConfigMap metadata: name: {{ .Release.Name }}-config labels: app.kubernetes.io/part-of: {{ .Release.Name }}-chart data: APP_GREETING: |- {{ .Values.config.greeting | default "Yeah, it's openshift time" }} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This creates a &lt;code&gt;ConfigMap&lt;/code&gt; with the name of the release, followed by a dash, followed by &lt;code&gt;config&lt;/code&gt;. Of course, you now need to make sure that the &lt;code&gt;ConfigMap&lt;/code&gt; is being read by the deployment accordingly:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;- image: "{{ .Values.deployment.image }}:{{ .Values.deployment.version }}" envFrom: - configMapRef: name: {{ .Release.Name }}-config [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are updating all the other manifest files in your Helm's &lt;code&gt;templates&lt;/code&gt; folder, you can install your chart multiple times.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm install person-service1 &lt;path to chart&gt; $ helm install person-service2 &lt;path to chart&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With that task out of the way, we can now consider each of the three potential approaches outlined above in turn.&lt;/p&gt; &lt;h2&gt;Install the database via an existing OpenShift template&lt;/h2&gt; &lt;p&gt;By far easiest way to install a PostgreSQL database in an OpenShift namespace is by using an OpenShift template. We did it several times in my book. The call is simple:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc new-app postgresql-persistent \ -p POSTGRESQL_USER=wanja \ -p POSTGRESQL_PASSWORD=wanja \ -p POSTGRESQL_DATABASE=wanjadb \ -p DATABASE_SERVICE_NAME=wanjaserver &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But how could you automate this process? There is no way to execute this call from within a Helm chart installation. (Well, you could do it by using a pre-install hook, but that would be quite ugly.)&lt;/p&gt; &lt;p&gt;Fortunately, the OpenShift client has a function called &lt;code&gt;process&lt;/code&gt; that processes a template. The result of this call is a list of YAML objects that can then be installed into OpenShift.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc process postgresql-persistent -n openshift -o yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you're piping the result into a new file, you would get something like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: v1 kind: List items: - apiVersion: v1 kind: Secret metadata: labels: template: postgresql-persistent-template name: postgresql stringData: database-name: sampledb database-password: KSurRUMyFI2fiVpx database-user: user0U4 - apiVersion: v1 kind: Service [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you're not happy with the default parameters for username, password, and database name, call the process function with the &lt;code&gt;-p PARAM=VALUE&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ oc process postgresql-persistent -n openshift -o yaml \ -p POSTGRESQL_USER=wanja \ -p POSTGRESQL_PASSWORD=wanja \ -p POSTGRESQL_DATABASE=wanjadb \ -p DATABASE_SERVICE_NAME=wanjaserver &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Place the resulting file into your chart's &lt;code&gt;templates&lt;/code&gt; folder, and it will be used to install the database. If you have a closer look at the file, you can see that it's using &lt;code&gt;DATABASE_SERVICE_NAME&lt;/code&gt; as manifest names for its &lt;code&gt;Service&lt;/code&gt;, &lt;code&gt;Secret&lt;/code&gt;, and &lt;code&gt;DeploymentConfig&lt;/code&gt; objects, which would make it impossible to install your resulting chart more than once into any namespace.&lt;/p&gt; &lt;p&gt;If you're providing the string &lt;code&gt;-p DATABASE_SERVICE_NAME='pg-{{ .Release.Name }}'&lt;/code&gt; instead of the fixed string &lt;code&gt;wanjaserver&lt;/code&gt;, then this will be used as the object name for these manifest files. However, if you try to install your Helm chart now, you'll get some verification error messages. This is because &lt;code&gt;oc process&lt;/code&gt; generates some top-level status fields that the Helm parser does not understand, so you need to remove them.&lt;/p&gt; &lt;p&gt;The only thing you now need to do is to connect your &lt;code&gt;person-service&lt;/code&gt; deployment with the corresponding database instance. Simply add the following entries to the &lt;code&gt;env&lt;/code&gt; section of your &lt;code&gt;Deployment.yaml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] env: - name: DB_host value: pg-{{ .Release.Name }}.{{ .Release.Namespace }}.svc - name: DB_dbname valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-name - name: DB_user valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-user - name: DB_password valueFrom: secretKeyRef: name: pg-{{ .Release.Name }} key: database-password [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your Helm chart is now ready to be packaged and installed:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package better-helm/with-templ $ helm upgrade --install ps1 person-service-templ-0.0.10.tgz &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Unfortunately, one of the resulting manifest files is a &lt;code&gt;DeploymentConfig&lt;/code&gt;, which would only work on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. As a result, this chart can't be installed on any other Kubernetes distribution. So let's discuss other options.&lt;/p&gt; &lt;h2&gt;Install a Kubernetes Operator with your chart&lt;/h2&gt; &lt;p&gt;Another way to install a dependent database with your Helm chart is to look for a Kubernetes Operator on &lt;a href="https://operatorhub.io/"&gt;OperatorHub&lt;/a&gt;. If your cluster already has an Operator Lifecycle Manager (OLM) installed (as all OpenShift clusters do), then the only thing you need to do is create a &lt;code&gt;Subscription&lt;/code&gt; that describes your desire to install an Operator.&lt;/p&gt; &lt;p&gt;For example, to install the community operator by CrunchyData into OpenShift, you would need to create the following file:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: postgresql-operator namespace: openshift-operators spec: channel: v5 name: postgresql source: community-operators sourceNamespace: openshift-marketplace &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you put this file into the &lt;code&gt;crds&lt;/code&gt; folder of your Helm chart, Helm takes care of installing the Operator before it processes the template files of the chart. Please note, however, that Helm will &lt;em&gt;never&lt;/em&gt; uninstall the custom resource definitions, so the Operator will stay on the Kubernetes cluster.&lt;/p&gt; &lt;p&gt;If you place the following file into the &lt;code&gt;templates&lt;/code&gt; folder of the chart, your PostgreSQL database instance will be ready to use:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: postgres-operator.crunchydata.com/v1beta1 kind: PostgresCluster metadata: name: {{ .Release.Name }}-db labels: app.kubernetes.io/part-of: {{ .Release.Name }}-chart spec: image: registry.developers.crunchydata.com/crunchydata/crunchy-postgres:centos8-13.5-0 postgresVersion: 13 instances: - name: instance1 dataVolumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi backups: pgbackrest: image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:centos8-2.36-0 repos: - name: repo1 volume: volumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Of course, you now need to make sure that your &lt;code&gt;person-service&lt;/code&gt; is able to connect to this PostgreSQL instance. Simply add a &lt;code&gt;secretRef&lt;/code&gt; to the deployment file with the following content:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] envFrom: - secretRef: name: {{ .Release.Name }}-db-pguser-{{ .Release.Name }}-db prefix: DB_ [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will map all values of the &lt;code&gt;PostgresCluster&lt;/code&gt; secret to your deployment with a prefix of &lt;code&gt;DB_&lt;/code&gt;, which is exactly what you need.&lt;/p&gt; &lt;p&gt;Now your chart is ready to be packaged and can be installed in any OpenShift namespace:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package with-crds $ helm install ps1 person-service-crd-0.0.10.tgz $ helm uninstall ps1 &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install the database by adding a subchart dependency&lt;/h2&gt; &lt;p&gt;The last option is to use a subchart within your chart. For this scenario, Helm has a &lt;a href="https://helm.sh/docs/topics/charts/#chart-dependencies"&gt;dependency management system&lt;/a&gt; that makes it easier for you as a chart developer to use third-party charts. The example that follows makes use of the &lt;a href="https://artifacthub.io/packages/helm/bitnami/postgresql"&gt;Bitnami PostgreSQL chart&lt;/a&gt;, which you can find on &lt;a href="https://artifacthub.io"&gt;ArtifactHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To start, you have to change the &lt;code&gt;Chart.yaml&lt;/code&gt; file to add the external dependency. With the following lines, you can add the dependency to the Bitnami PostgreSQL database with the version &lt;code&gt;11.1.3&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;dependencies: - name: postgresql repository: https://charts.bitnami.com/bitnami version: 11.1.3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you want to define properties from within your &lt;code&gt;values.yaml&lt;/code&gt; file, you simply need to use the name of the chart as the first parameter in the tree; in this case, it is &lt;code&gt;postgresql&lt;/code&gt;. You can then add all necessary parameters below that key:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;postgresql: auth: username: wanja [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, you need to have a look at the documentation of the Bitnami chart to understand how to use it in your target environment (OpenShift, in this case). Unfortunately, as of the time of this writing, the current documentation is a bit outdated, so you would not be able to install your chart without digging further into the &lt;code&gt;values.yaml&lt;/code&gt; file Bitnami supplies to see which security settings you have to set in order to use it with OpenShift's strong enterprise security.&lt;/p&gt; &lt;p&gt;To save you the trouble, I've put together this minimum list of settings you would need to use:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;postgresql: auth: username: wanja password: wanja database: wanjadb primary: podSecurityContext: enabled: false fsGroup: "" containerSecurityContext: enabled: false runAsUser: "auto" readReplicas: podSecurityContext: enabled: false fsGroup: "" containerSecurityContext: enabled: false runAsUser: "auto" volumePermissions: enabled: false securityContext: runAsUser: "auto" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The final step is to make sure that your deployment is able to connect to this database.&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;[...] env: - name: DB_user value: wanja - name: DB_password valueFrom: secretKeyRef: name: {{ .Release.Name }}-postgresql key: password - name: DB_dbname value: wanjadb - name: DB_host value: {{ .Release.Name }}-postgresql.{{ .Release.Namespace }}.svc [...] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to package your chart. Because you're depending on a third-party chart, you need to use the &lt;code&gt;-u&lt;/code&gt; option, which downloads the dependencies into the &lt;code&gt;charts&lt;/code&gt; folder of your Helm chart.&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;$ helm package -u better-helm/with-subchart $ helm install ps1 person-service-sub.0.0.11.tgz &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Using Helm charts for your own projects is quite easy, even if you need to make sure certain dependencies are being installed as well. Thanks to Helm's dependency management, you can easily use subcharts with your charts. And thanks to the flexibility of Helm, you can also either use a (processed) template or quickly install a Kubernetes Operator before proceeding.&lt;/p&gt; &lt;p&gt;Check out these articles to learn more about Helm:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/05/24/deploy-helm-charts-jenkins-cicd-red-hat-openshift-4"&gt;Deploy Helm charts with Jenkins CI/CD in Red Hat OpenShift 4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/19/deploy-java-application-using-helm-part-1"&gt;Deploy a Java application using Helm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;And for a more in-depth look at the example explored here, check out my e-book, &lt;a href="https://developers.redhat.com/e-books/getting-gitops-practical-platform-openshift-argo-cd-and-tekton"&gt;&lt;em&gt;Getting GitOps: A Practical Platform with OpenShift, Argo CD and Tekton&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/07/3-ways-install-database-helm-charts" title="3 ways to install a database with Helm charts"&gt;3 ways to install a database with Helm charts&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Wanja Pernath</dc:creator><dc:date>2022-04-07T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 07 April 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-04-07.html" /><category term="quarkus" /><category term="kubernetes" /><category term="java" /><category term="infinispan" /><category term="wildfly" /><category term=".net core" /><category term="hot rod" /><category term="cloud-native" /><category term="openshift" /><category term="quarkus camel" /><category term="apache camel" /><category term="atlasmap" /><category term="microservice" /><category term="events" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-04-07.html</id><updated>2022-04-07T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kubernetes, java, infinispan, wildfly, .net core, hot rod, cloud-native, openshift, quarkus camel, apache camel, atlasmap, microservice, events"&gt; &lt;h1&gt;This Week in JBoss - 07 April 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hi everyone! It’s great to be back and bringing you another edition of the JBoss Editorial. As always there’s a lot of exciting news and updates from JBoss communities so let’s dive in.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/vscode-quarkus-1.10.0/"&gt;Quarkus Tools for Visual Studio Code 1.10.0&lt;/a&gt; - Quarkus Tools for Visual Studio Code 1.10.0 is available on the &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.vscode-quarkus"&gt;VS Code Marketplace&lt;/a&gt; and &lt;a href="https://open-vsx.org/extension/redhat/vscode-quarkus"&gt;Open VSX&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/03/24/WildFly26-1-Beta-Released/"&gt;WildFly 26.1 Beta1&lt;/a&gt; - WildFly and WildFly Preview 26.1.0.Beta1 releases are available for download.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/03/29/WildFly-s2i-26-1-Beta1-Released/"&gt;WildFly S2I builder and runtime images&lt;/a&gt; - S2I images for WildFly 26.1 Beta are on &lt;a href="quay.io/wildfly"&gt;quay.io/wildfly&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/04/kogito-1-19-0-released.html"&gt;Kogito 1.19.0&lt;/a&gt; - Kogito 1.19.0 is now available.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_net_core_client_for_infinispan"&gt;.Net core client for Infinispan&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://infinispan.org/blog/2022/03/22/dotnetcore-0-0-3-beta"&gt;Infinispan .Net Core Client&lt;/a&gt;, by Vittorio Rigamonti&lt;/p&gt; &lt;p&gt;Vittorio Rigamonti has made a beta release of his .Net core client that allows access to remote caches on Infinispan clusters. The client provides .Net core applications with the performance benefits of in-memory data storage. Combined with the Protobuf data encoding Vittorio’s client also gives .Net core applications data interoperability with Java applications that can access the same Infinispan caches. Using Protobuf with the .Net core client lets you perform remote queries as an added bonus!&lt;/p&gt; &lt;p&gt;Go check out Vittorio’s post and use his &lt;a href="https://github.com/infinispan/Infinispan.Hotrod.Core/tree/main/Infinispan.Hotrod.Application"&gt;sample application&lt;/a&gt; to get started coding. While you’re at it, you might find Vittorio’s other blog post on &lt;a href="https://infinispan.org/blog/2022/01/21/dotnet-core-query"&gt;performing remote queries&lt;/a&gt; useful.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_inside_out_event_streaming_with_microservice_architectures"&gt;Inside out event streaming with microservice architectures&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.ofbizian.com/2022/04/turning-microservices-inside-out.html"&gt;Turning Microservices Inside-Out&lt;/a&gt;, by Bilgin Ibryam&lt;/p&gt; &lt;p&gt;This excellent read from Bilgin starts with recent ideas about the design of relational databases and their place in the event-driven world of microservice architectures. Bilgin makes his point that, rather than replacing relational databases with event streams, the inside out design principle is better applied to the service level. Using traditional databases with an event-driven approach allows you to combine the best of both technologies but requires a deliberate focus on APIs. From there Bilgin explains the flow of events through outbound and inbound APIs and how a "connecting tissue" such as Debezium can help you effectively bring relational databases into event-driven microservices.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_camel_quarkus_the_swiss_knife_of_integration"&gt;Camel Quarkus: the Swiss knife of integration&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/camel-quarkus-effortless-apis/"&gt;Riding Camel Quarkus: effortless APIs&lt;/a&gt;, by Bruno Meseguer&lt;/p&gt; &lt;p&gt;Bruno showcases Camel Quarkus in an extensive post that will leave anyone working on service integration how they could live without it.&lt;/p&gt; &lt;p&gt;Using the OpenAPI specification as an example Bruno explains how to use Camel Quarkus to connect to an HTTP service and transform data in a few effortless steps. There’s a lot to discover in his post and Bruno even walks you through a second iteration of his application in which he reduces the path to integration even further.&lt;/p&gt; &lt;p&gt;I think by the end Bruno proves his point, that Camel Quarkus really is a Swiss knife for service integration. He offers a compelling look at how reducing integration overhead lowers maintenance costs and accelerates development cycles for rapid functionality growth.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quickly_debug_and_profile_mysql_databases"&gt;Quickly debug and profile MySQL databases&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jbossas/jboss-datasource/how-to-trace-jdbc-statements-performance-in-mysql/"&gt;How to trace JDBC Statements performance in MySQL&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;Francesco shows us how to quickly debug and profile SQL statements with Java applications using the MySQL JDBC driver. Using the WildFly CLI Francesco adds a JDBC connection to MySQL database in a container in a few simple steps. After creating the datasource he demonstrates the &lt;code&gt;hibernate.show_sql&lt;/code&gt; option in the application’s &lt;code&gt;persistence.xml&lt;/code&gt;. Once that’s done you can find all sorts of useful information about SQL statements in your logs. You’re all set to debug and tune!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_youtube_videos"&gt;YouTube videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;From unmissable demos to brilliant chat about the latest Java trends, the JBoss community has some great video content for you:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/Y2En5miRKjY"&gt;Build &amp;#38; Deploy WildFly Quickstarts on OpenShift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/2gQO4_7Z5CI"&gt;Securing a WildFly application with OpenID Connect on OpenShift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/22wg8oO9xXM"&gt;Quarkus Insights #86: Vaadin &amp;#38; Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/saTxdw-SPGA"&gt;Quarkus Insights #85: What’s new in Quarkus Kafka, REX with Loïc Mathieu&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/W2QPxfEU_bw"&gt;Build your first Java Serverless Function using Quarkus Quick start&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_see_you_next_time"&gt;See you next time&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;em&gt;Hope you enjoyed this edition. Please join us again in two weeks for our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Don Naro</dc:creator></entry><entry><title type="html">How to trace JDBC Statements performance in MySQL</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-datasource/how-to-trace-jdbc-statements-performance-in-mysql/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-datasource/how-to-trace-jdbc-statements-performance-in-mysql/</id><updated>2022-04-06T14:30:07Z</updated><content type="html">In this quick article we will learn an handy option which is available in MySQL JDBC Driver to debug and profile SQL statements run by Java applications. There are several strategies to trace SQL Statements that your application is running. For example, JPA applications typically use this option to show the actual SQL Statement that ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>An introduction to Linux bridging commands and features</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/06/introduction-linux-bridging-commands-and-features" /><author><name>Hangbin Liu</name></author><id>a0865fd8-d51d-482c-9d6a-131ded31de60</id><updated>2022-04-06T07:00:00Z</updated><published>2022-04-06T07:00:00Z</published><summary type="html">&lt;p&gt;A &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; bridge is a kernel module that behaves like a network switch, forwarding packets between interfaces that are connected to it. It's usually used for forwarding packets on routers, on gateways, or between VMs and network namespaces on a host.&lt;/p&gt; &lt;p&gt;The Linux bridge has included basic support for the &lt;a href="https://www.inap.com/blog/spanning-tree-protocol-explained/"&gt;Spanning Tree Protocol&lt;/a&gt; (STP), multicast, and &lt;a href="https://www.netfilter.org"&gt;Netfilter&lt;/a&gt; since the 2.4 and 2.6 kernel series. Features that have been added in more recent releases include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Configuration via &lt;a href="https://man7.org/linux/man-pages/man7/netlink.7.html"&gt;Netlink&lt;/a&gt;&lt;/li&gt; &lt;li&gt;VLAN filter&lt;/li&gt; &lt;li&gt;VxLAN tunnel mapping&lt;/li&gt; &lt;li&gt;Internet Group Management Protocol version 3 (IGMPv3) and Multicast Listener Discovery version 2 (MLDv2)&lt;/li&gt; &lt;li&gt;Switchdev&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In this article, you'll get an introduction to these features and some useful commands to enable and control them. You'll also briefly examine &lt;a href="https://www.openvswitch.org"&gt;Open vSwitch&lt;/a&gt; as an alternative to Linux bridging.&lt;/p&gt; &lt;h2&gt;Basic bridge commands&lt;/h2&gt; &lt;p&gt;All the commands used in this article are part of the &lt;code&gt;iproute2&lt;/code&gt; module, which invokes Netlink messages to configure the bridge. There are two &lt;code&gt;iproute2&lt;/code&gt; commands for setting and configuring bridges: &lt;code&gt;ip link&lt;/code&gt; and &lt;code&gt;bridge&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;ip link&lt;/code&gt; can add and remove bridges and set their options. &lt;code&gt;bridge&lt;/code&gt; displays and manipulates bridges on final distribution boards (FDBs), main distribution boards (MDBs), and virtual local area networks (VLANs).&lt;/p&gt; &lt;p&gt;The listings that follow demonstrate some basic uses for the two commands. Both require administrator privileges, and therefore the listings are shown with the &lt;code&gt;#&lt;/code&gt; root prompt instead of a regular user prompt.&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Show help information about the &lt;code&gt;bridge&lt;/code&gt; object:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link help bridge # bridge -h&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Create a bridge named &lt;code&gt;br0&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link add br0 type bridge&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Show bridge details:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip -d link show br0&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Show bridge details in a pretty JSON format (which is a good way to get bridge key-value pairs):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip -j -p -d link show br0&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add interfaces to a bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set veth0 master br0 # ip link set tap0 master br0&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Spanning Tree Protocol&lt;/h2&gt; &lt;p&gt;The purpose of STP is to prevent a networking loop, which can lead to a traffic storm in the network. Figure 1 shows such a loop.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_1.png?itok=J-oXObCl" width="473" height="229" alt="Without STP, a network can be configured in a loop." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Without STP, a network can be configured in a loop. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;With STP enabled, the bridges will send each other Bridge Protocol Data Units (BPDUs) so they can elect a root bridge and block an interface, making the network topology loop-free (Figure 2).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_2.png?itok=ZJbkJjVE" width="471" height="229" alt="STP can choose a link and block it." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: STP can choose a link and block it. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Linux bridging has supported STP since the 2.4 and 2.6 kernel series. To enable STP on a bridge, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set br0 type bridge stp_state 1&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; The Linux bridge does not support the Rapid Spanning Tree Protocol (RSTP).&lt;/p&gt; &lt;p&gt;Now you can show the STP blocking state on the bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip -j -p -d link show br0 | grep root_port "root_port": 1, # ip -j -p -d link show br1 | grep root_port "root_port": 0, # bridge link show 7: veth0@veth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master br0 state forwarding priority 32 cost 2 8: veth1@veth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master br1 state forwarding priority 32 cost 2 9: veth2@veth3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master br0 state blocking priority 32 cost 2 10: veth3@veth2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 master br1 state forwarding priority 32 cost 2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The line labeled 9 in the output shows that the &lt;code&gt;veth2&lt;/code&gt; interface is in a blocking state, as illustrated in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_3.png?itok=RcE8tX77" width="470" height="257" alt="The link from br0 to veth2 is blocked." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The link from br0 to veth2 is blocked. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To change the STP hello time, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set br0 type bridge hello_time 300 # ip -j -p -d link show br0 | grep \"hello_time\" "hello_time": 300,&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can use the same basic approach to change other STP parameters, such as maximum age, forward delay, ageing time, and so on.&lt;/p&gt; &lt;h2&gt;VLAN filter&lt;/h2&gt; &lt;p&gt;The VLAN filter was introduced in Linux kernel 3.8. Previously, to separate VLAN traffic on the bridge, the administrator needed to create multiple bridge/VLAN interfaces. As illustrated in Figure 4, three bridges—&lt;code&gt;br0&lt;/code&gt;, &lt;code&gt;br2&lt;/code&gt;, and &lt;code&gt;br3&lt;/code&gt;—would be needed to support three VLANs to make sure that VLAN traffic went to the corresponding VLANs.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_4.png?itok=gQOf45H2" width="874" height="328" alt="Without VLAN filter, three VLANs required three bridges and network configurations." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Without VLAN filter, three VLANs required three bridges and network configurations. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Without the VLAN filter, three VLANs required three bridges and network configurations.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But with the VLAN filter, just one bridge device is enough to set all the VLAN configurations, as illustrated in Figure 5.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_5.png?itok=W0KYDfZM" width="558" height="301" alt="With VLAN filter, a single bridge can serve multiple VLANs." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: With VLAN filter, a single bridge can serve multiple VLANs. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: With the VLAN filter, a single bridge can serve multiple VLANs.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The following commands enable the VLAN filter and configure three VLANs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set br0 type bridge vlan_filtering 1 # ip link set eth1 master br0 # ip link set eth1 up # ip link set br0 up # bridge vlan add dev veth1 vid 2 # bridge vlan add dev veth2 vid 2 pvid untagged # bridge vlan add dev veth3 vid 3 pvid untagged master # bridge vlan add dev eth1 vid 2-3 # bridge vlan show port vlan-id eth1 1 PVID Egress Untagged 2 3 br0 1 PVID Egress Untagged veth1 1 Egress Untagged 2 veth2 1 Egress Untagged 2 PVID Egress Untagged veth3 1 Egress Untagged 3 PVID Egress Untagged&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then the following command enables a VLAN filter on the &lt;code&gt;br0&lt;/code&gt; bridge:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge vlan_filtering 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This next command makes the &lt;code&gt;veth1&lt;/code&gt; bridge port transmit only VLAN 2 data:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev veth1 vid 2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following command, similar to the previous one, makes the &lt;code&gt;veth2&lt;/code&gt; bridge port transmit VLAN 2 data. The &lt;code&gt;pvid&lt;/code&gt; parameter causes untagged frames to be assigned to this VLAN at ingress (&lt;code&gt;veth2&lt;/code&gt; to bridge), and the &lt;code&gt;untagged&lt;/code&gt; parameter causes the packet to be untagged on egress (bridge to &lt;code&gt;veth2&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev veth2 vid 2 pvid untagged&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The next command carries out the same operation as the previous one, this time on &lt;code&gt;veth3&lt;/code&gt;. The &lt;code&gt;master&lt;/code&gt; parameter indicates that the link setting is configured on the software bridge. However, because &lt;code&gt;master&lt;/code&gt; is a default option, this command has the same effect as the previous one:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev veth3 vid 3 pvid untagged master&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following command enables VLAN 2 and VLAN 3 traffic on &lt;code&gt;eth1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev eth1 vid 2-3&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To show the VLAN traffic state, enable VLAN statistics (added in kernel 4.7) as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge vlan_stats_enabled 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The previous command enables just global VLAN statistics on the bridge, and is not fine grained enough to show each VLAN's state. To enable per-VLAN statistics when there are &lt;code&gt;no&lt;/code&gt; port VLANs in the bridge, you also need to enable &lt;code&gt;vlan_stats_per_port&lt;/code&gt; (added in kernel 4.20). You can run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge vlan_stats_per_port 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then you can show per-VLAN statistics like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# bridge -s vlan show port vlan-id br0 1 PVID Egress Untagged RX: 248 bytes 3 packets TX: 333 bytes 1 packets eth1 1 PVID Egress Untagged RX: 333 bytes 1 packets TX: 248 bytes 3 packets 2 RX: 0 bytes 0 packets TX: 56 bytes 1 packets 3 RX: 0 bytes 0 packets TX: 224 bytes 7 packets veth1 1 Egress Untagged RX: 0 bytes 0 packets TX: 581 bytes 4 packets 2 PVID Egress Untagged RX: 6356 bytes 77 packets TX: 6412 bytes 78 packets veth2 1 Egress Untagged RX: 0 bytes 0 packets TX: 581 bytes 4 packets 2 PVID Egress Untagged RX: 6412 bytes 78 packets TX: 6356 bytes 77 packets veth3 1 Egress Untagged RX: 0 bytes 0 packets TX: 581 bytes 4 packets 3 PVID Egress Untagged RX: 224 bytes 7 packets TX: 0 bytes 0 packets&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;VLAN tunnel mapping&lt;/h2&gt; &lt;p&gt;VxLAN builds Layer 2 virtual networks on top of a Layer 3 underlay. A VxLAN tunnel endpoint (VTEP) originates and terminates VxLAN tunnels. VxLAN bridging is the function provided by VTEPs to terminate VxLAN tunnels and map the VxLAN network identifier (VNI) to the traditional end host's VLAN.&lt;/p&gt; &lt;p&gt;Previously, to achieve VLAN tunnel mapping, administrators needed to add local ports and VxLAN network devices (netdevs) into a VLAN filtering bridge. The local ports were configured as trunk ports carrying all VLANs. A VxLAN netdev for each VNI would then need to be added to the bridge. VLAN to VNI mapping was achieved by configuring a port VLAN identifier (pvid) for each VLAN as on the corresponding VxLAN netdev, as shown in Figure 6.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_6.png?itok=cbHrDl-b" width="398" height="234" alt="VxLAN used to require multiple netdevs." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; VxLAN used to require multiple netdevs. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6. VxLAN used to require multiple netdevs.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Since 4.11, the kernel has provided a native way to support VxLAN bridging. The topology for this looks like Figure 7. The &lt;code&gt;vxlan0&lt;/code&gt; endpoint in this figure was added with lightweight tunnel (LWT) support to handle multiple VNIs.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_7.png?itok=8fJ6xtLr" width="399" height="236" alt="Now Linux bridging handle multiple VNIs with one VxLAN." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Now Linux bridging handle multiple VNIs with one VxLAN. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Now Linux bridging can handle multiple VNIs with one VxLAN.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To create a tunnel, you must first add related VIDs to the interfaces:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan add dev eth1 vid 100-101 bridge vlan add dev eth1 vid 200 bridge vlan add dev vxlan0 vid 100-101 bridge vlan add dev vxlan0 vid 200&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now enable a VLAN tunnel mapping on a bridge port:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set dev vxlan0 type bridge_slave vlan_tunnel on&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can enable the tunnel with this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# bridge link set dev vxlan0 vlan_tunnel on&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then add VLAN tunnel mapping:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# bridge vlan add dev vxlan0 vid 2000 tunnel_info id 2000 # bridge vlan add dev vxlan0 vid 1000-1001 tunnel_info id 1000-1001 # bridge -j -p vlan tunnelshow [ { "ifname": "vxlan0", "tunnels": [ { "vlan": 100, "vlanEnd": 101, "tunid": 100, "tunidEnd": 101 },{ "vlan": 200, "tunid": 200 } ] } ]&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Multicast&lt;/h2&gt; &lt;p&gt;Linux bridging has included support for IGMPv2 and MLDv1 support since kernel version 2.6. IGMPv3/MLDv2 support was added in kernel 5.10.&lt;/p&gt; &lt;p&gt;To use multicast, enable bridge multicast snooping, querier, and statistics as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link set br0 type bridge mcast_snooping 1 # ip link set br0 type bridge mcast_querier 1 # ip link set br0 type bridge mcast_stats_enabled 1 # tcpdump -i br0 -nn -l 02:47:03.417331 IP 0.0.0.0 &gt; 224.0.0.1: igmp query v2 02:47:03.417340 IP6 fe80::3454:82ff:feb9:d7b4 &gt; ff02::1: HBH ICMP6, multicast listener querymax resp delay: 10000 addr: ::, length 24&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default, when snooping is enabled, the bridge uses IGMPv2/MLDv1. You can change the versions with these commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge mcast_igmp_version 3 ip link set br0 type bridge mcast_mld_version 2&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After a port joins a group, you can show the multicast database (mdb) like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# bridge mdb show dev br0 port br0 grp ff02::fb temp dev br0 port eth1 grp ff02::fb temp dev br0 port eth2 grp ff02::fb temp dev br0 port eth2 grp 224.1.1.1 temp dev br0 port br0 grp ff02::6a temp dev br0 port eth1 grp ff02::6a temp dev br0 port eth2 grp ff02::6a temp dev br0 port br0 grp ff02::1:ffe2:de9f temp dev br0 port eth1 grp ff02::1:ffe2:de9f temp dev br0 port eth2 grp ff02::1:ffe2:de9f temp&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bridging also supports multicast snooping and querier on a single VLAN. Set them as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;bridge vlan set vid 10 dev eth1 mcast_snooping 1 mcast_querier 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can show bridge xstats (multicast RX/TX information) with this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# ip link xstats type bridge br0 IGMP queries: RX: v1 0 v2 1 v3 0 TX: v1 0 v2 131880 v3 0 IGMP reports: RX: v1 0 v2 1 v3 0 TX: v1 0 v2 496 v3 18956 IGMP leaves: RX: 0 TX: 0 IGMP parse errors: 0 MLD queries: RX: v1 1 v2 0 TX: v1 51327 v2 0 MLD reports: RX: v1 66 v2 6 TX: v1 3264 v2 213794 MLD leaves: RX: 0 TX: 0 MLD parse errors: 0&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There are other multicast parameters you can configure, including &lt;code&gt;mcast_router&lt;/code&gt;, &lt;code&gt;mcast_query_interval&lt;/code&gt;, and &lt;code&gt;mcast_hash_max&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Bridge switchdev&lt;/h2&gt; &lt;p&gt;Linux bridging is always used when virtual machines (VMs) connect to physical networks, by using the virtio tap driver. You can also attach a Single Root I/O Virtualization (SR-IOV) virtual function (VF) in a VM guest to get better performance (Figure 8).&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_8.png?itok=s21JwYjI" width="596" height="291" alt="VFs in virtual machines." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: VFs in virtual machines. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But the way Linux used to deal with SR-IOV embedded switches limited their expressiveness and flexibility. And the kernel model for controlling the SR-IOV eSwitch did not allow any forwarding unless it was based on MAC/VLAN.&lt;/p&gt; &lt;p&gt;To make VFs also support dynamic FDB (as in Figure 9) and maintain the benefits of the VLAN filter while still providing optimal performance, Linux bridging added switchdev support in kernel version 4.9. Switchdev allows the offloading of Layer 2 forwarding to a hardware switch such as &lt;a href="https://github.com/Mellanox/mlxsw/wiki"&gt;Mellanox Spectrum devices&lt;/a&gt;, DSA-based switches, and MLX5 CX6 Dx cards.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/br_9.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/br_9.png?itok=_yu2G2C5" width="582" height="285" alt="Switchdev provides widespread support for offloading traffic to hardware." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: Switchdev provides widespread support for offloading traffic to hardware. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In switchdev mode, the bridge is up and its related configuration is enabled, e.g., MLX5_BRIDGE for an MLX5 SRIOV eSwitch. Once in switchdev mode, you can connect the VF's representors to the bridge, and frames that are supposed to be transmitted by the bridge are transmitted by hardware only. Their routing will be done in the switch at the network interface controller (NIC).&lt;/p&gt; &lt;p&gt;Once a frame passes through the VF to its representor, the bridge learns that the source MAC of the VF is behind a particular port. The bridge adds an entry with the MAC address and port to its FDB. Immediately afterward, the bridge sends a message to the mlx5 driver, and the driver adds a relevant rule or line to two tables located in the eSwitch on the NIC. Later, frames with the same destination MAC address that come from the VF don't go through the kernel; instead, they go directly through the NIC to the appropriate port.&lt;/p&gt; &lt;p&gt;Switchdev support for embedded switches in NICs is simple, but for full-featured switches such as Mellanox Spectrum, the offloading capabilities are much richer, with support for link aggregation group (LAG) hashing (team, bonding), tunneling (VxLAN, etc.), routing, and TC offloading. Routing and TC offloading are out of scope for bridging, but LAGs can be attached to the bridge as well as to VxLAN tunnels, with full support for offloading.&lt;/p&gt; &lt;h2&gt;Bridging with Netfilter&lt;/h2&gt; &lt;p&gt;By default, the traffic forwarded by the bridge does not go through an iptables firewall. To let the iptables forward rules filter Layer 2 traffic, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge nf_call_iptables 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The same procedure works for ip6tables and arptables.&lt;/p&gt; &lt;h2&gt;Bridge ageing time&lt;/h2&gt; &lt;p&gt;Ageing determines the number of seconds a MAC address is kept in the FDB after a packet has been received from that address. After this time has passed, entries are cleaned up. To change the timer, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ip link set br0 type bridge ageing_time 20000&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Bridging versus Open vSwitch&lt;/h2&gt; &lt;p&gt;Linux bridging is very useful and has become popular over the past few years. It supplies Layer 2 forwarding, and connects VMs and networks with VLAN/multicast support. Bridging on Linux is stable, reliable, and easy to set up and configure.&lt;/p&gt; &lt;p&gt;On the other hand, Linux bridging also has some limitations. It's missing some types of tunnel support, for instance. If you want to get easier network management, more tunnel support (GRE, VXLAN, etc.), Layer 3 forwarding, and integration with software-defined networking (SDN), you can try &lt;a href="https://www.openvswitch.org"&gt;Open vSwitch&lt;/a&gt; (OVS).&lt;/p&gt; &lt;p&gt;To learn more about Linux network interfaces and other networking topics, check out these articles from Red Hat Developer:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/05/17/an-introduction-to-linux-virtual-interfaces-tunnels"&gt;An introduction to Linux virtual interfaces: Tunnels&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking"&gt;Introduction to Linux interfaces for virtual networking&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/04/01/get-started-with-xdp"&gt;Get started with XDP&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/06/introduction-linux-bridging-commands-and-features" title="An introduction to Linux bridging commands and features"&gt;An introduction to Linux bridging commands and features&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Hangbin Liu</dc:creator><dc:date>2022-04-06T07:00:00Z</dc:date></entry><entry><title type="html">Turning Microservices Inside-Out</title><link rel="alternate" href="http://www.ofbizian.com/2022/04/turning-microservices-inside-out.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/04/turning-microservices-inside-out.html</id><updated>2022-04-05T08:42:00Z</updated><content type="html">There is a fantastic talk by Martin Kleppmann called “”. Once watched, it can change your perspective on databases and event logs irreversibly. While I agree with the outlined limitations of databases and the benefits of event logs, I’m not convinced of the practicality of replacing databases with event logs. I believe the same design principles used for turning databases inside-out, should instead be applied at a higher, service design level to ensure microservices stream changes from inside-out. With that twist, within the services, we can keep using traditional databases for what they are best for - efficiently working with mutable state and also use event logs to reliably propagate changes among services. With the help of frameworks such as Debezium which can act as a connecting tissue between databases and event logs, we can benefit from the time-tested and familiar database technology and modern event logs such as Red Hat’s managed at the same time. This inside-out mindset requires a deliberate focus on offering outbound APIs in microservices to stream all relevant state change and domain events from within the service to the outside world. This merge of microservices movement with the event driven emerging trends is what I call turning the microservices data inside-out. MICROSERVICES API TYPES To build up this idea, I will look into microservices from the point of different API types they provide and consume. A common way to describe microservices is as independently deployed components, built around a business domain, that own their data and are exposed over APIs. That is very similar to how databases are described in the post mentioned above - a black box with a single API that goes in and out. Data flowing from microservices’ inbound to outbound APIs I believe a better way to think about microservices would be one where every microservice is composed of inbound and outbound APIs where the data flows through and a meta API that describes these APIs. While inbound APIs are well known today, outbound APIs are not used as much, and the responsibilities of meta API are spread around various tools and proliferating microservices technologies. To make the inside-out approach work, we need to make outbound and meta APIs first-class microservices constructs and improve the tooling and practices around these areas. INBOUND APIS Inbound APIs are what every microservice has today in the form of service endpoints. These APIs are outside-in, and they allow outside systems to interact with the service directly through commands and queries or indirectly through events. Inbound APIs are the norm in microservices today In terms of implementation, these are typically REST-based APIs that offer mutating or read-only operations for synchronous operations, fronted by a load balancing gateway. These can also be implemented as queues for asynchronous command-based interactions, or topics for event-based interactions. The responsibilities and governance of these APIs are well understood and they form the majority of the microservices API landscape today. OUTBOUND APIS What I refer to as outbound APIs here are the interactions that originate from within the service and go to outside services and systems. The majority of these are queries and commands initiated by the service and targeted to dependent services owned by somebody else. What I also put under this category are the outbound events that originate from within the service. Outbound events are different from the query and commands targeted for a particular endpoint because an outbound event is defined by the service without concrete knowledge of the existing and possible future recipients. Regardless of the indirect nature of the API, there is still the expectation that these events are generated predictably and reliably for any significant change that happens within the service (typically caused by inbound interactions). Today, outbound events are often an afterthought. They are either created for the needs of a specific consumer that depends on them, or they are added later in the service lifecycle, not by the service owners but other teams responsible for data replication. On both occasions, the possible use cases of outbound events remain low and diminish its potential. The challenging part with outbound events is implementing a uniform and reliable notification mechanism for any change that happens within the service. To apply this approach uniformly in every microservice and for any kind of database, the tools here have to be non-intrusive and developer-friendly. Not having good frameworks that support this pattern, not having proven patterns, practices, and standards are impediments preventing the adoption of outbound events as a common top-level microservices construct. Outbound events implemented through change data capture To implement outbound events, you can include the logic of updating a database and publishing an event to a messaging system in your application code but that leads to the well-known . Or you could try to replace the traditional database with an event log, or use specialized event sourcing platforms. But if you consider that your most valuable resources in a project are the people and their proven tools and practices, replacing a fundamental component such as the database with something different will have a significant impact. A better approach would be to keep using the relational databases and all the surrounding tools and practices that have served fine for decades and complement your database with a connecting tissue such as Debezium (disclaimer: I’m the product manager for Debezium at Red Hat and I’m biased about it). I believe the best implementation approach for outbound events is the which uses a single transaction to both perform the normal database update dictated by the service logic and insert a message into a specific outbox table within the same database. Once the transaction is written to the database’s transaction log, Debezium picks up the outbox message from the log and sends it to Apache Kafka. This has nice properties such as "read your own writes" semantics, where a subsequent query to the service returns the newly persisted record and at the same time, we get reliable, asynchronous, propagation of changes via Apache Kafka. Debezium can selectively capture changes from the database transaction logs, transform and publish them into Kafka in a uniform way acting as an outbound eventing interface of the services. Debezium can be embedded into the Java application runtimes as a library, or decoupled as a . It is a plug-and-play component you add to your service regardless of whether it is a legacy service or created from scratch. It is the missing configuration-based outbound eventing API for any service. META APIS Today meta APIs describe the inbound and outbound APIs, enabling their governance, discovery, and consumption. They are implemented in siloed tools around a specific technology. In my definition, an OpenAPI definition for a REST endpoint published to an API portal is an example of meta API. An AsyncAPI definition for a messaging topic that is published to a schema registry is an example of meta API too. The schema change topic that Debezium publishes database schema change events (which are different from the data change events) is an example of meta API. There are various capabilities in other tools that describe the data structures and the APIs serving them that can all be classified as meta APIs. So in my definition, meta APIs are all the artifacts that allow different stakeholders to work with the service and enable other systems to use the inbound and outbound APIs. The evolving responsibilities of Meta APIs One of the fundamental design principles of microservices is to make them independently updatable and deployable. But today there are still significant amounts of coordination required among service owners for upgrades that involve API changes. Service owners need better meta API tools to subscribe for updates from dependent services and prepare to change timely. The meta API tools need to be integrated deeper into development and operational activities to increase agility. Meta API tools today are siloed, passive, and disparate across the technology stack. Instead, meta tools need to reflect the changing nature of service interactions towards an event-driven approach and play a more proactive role in automating some of the routine tasks of the development and operational teams. EMERGING TRENDS THE RISE OF OUTBOUND EVENTS Outbound events are already present as the preferred integration method for most modern platforms. Most cloud services emit events. Many data sources (such as Cockroach changefeeds, MongoDB change streams) and even file systems (for example Ceph ) can emit state change events. Custom-built microservices are not an exception here. Emitting state change or domain events is the most natural way for modern microservices to fit uniformly among the event-driven systems they are connected to in order to benefit from the same tooling and practices. Outbound events are bound to become a top-level microservices design construct for many reasons. Designing services with outbound events can help replicate data during an process. Outbound events are also the enabler for implementing elegant inter-service interactions through the Outbox Patterns and complex business transactions that span multiple services using a non-blocking implementation. Outbound events fit nicely into the architecture where a service is designed with its data consumers in mind. Data mesh claims that for data to fuel innovation, its ownership must be federated among domain data owners who are accountable for providing their data as products… In short, rather than having a centralized data engineering team to replicate data from every microservice through an ETL process, it is better if microservices are owned jointly with developers and data engineers and design the services to make the data available in the first place. What better way to do that than outbound events with real-time data streaming through Debezium, Apache Kafka, and Schema Registry. To sum up, outbound events align microservices with the Unix philosophy where “the output of every program becomes the input of a yet unknown program”. To future proof your services, you have to design them in a way to let the data flow from inbound to outbound APIs. This allows all the services to be developed and operated uniformly using modern event-oriented tools and patterns, and unlocks yet unknown future uses of data exposed through events. CONVERGENCE OF META API TOOLS With the increasing adoption of event-driven architectures and faster pace of service evolution, the responsibilities and the importance of meta APIs are growing too. The scope of meta API tools is no longer limited to but includes asynchronous APIs too. The meta APIs are expanding towards enabling faster development cycles by ensuring safe schema evolution through compatibility checks, notifications for updates, code generation for bindings, test simulations, and so forth. As a consumer of a service, I want to discover existing endpoints and data formats, the API compatibility rules, limits, and SLAs the service complies with in one place. At the same time, I want to get notifications for any changes that are coming, any deprecations, updates to the APIs, or any new APIs the service is going to offer that might be of interest to me. Not only that, developers are challenged to ship code faster and faster, and modern API tools can automate the process of schema and event structure discovery. Once a schema is discovered and added to the registry, a developer can quickly generate code bindings for their language and start developing in an IDE. Then, other tools could use the meta API definitions and generate tests and mocks, and simulate load by emitting dummy events with something like or even . At runtime, the contextual information available in the meta APIs can enable the platforms I’m running the application on to inject the , register it with monitoring tools, and so on. Overall, the role of meta API is evolving towards playing a more active role in the asynchronous interaction ecosystem by automating some of the coordination activities among service owners, increasing developer productivity, and automating operations teams’ tasks. And for that to become a reality, the different tools containing API metadata, code generation, test stimulation, environment management must converge, standardize and integrate better. STANDARDIZATION OF THE EVENT-DRIVEN SPACE While event-driven architecture (EDA) has a long history, recent drivers such as cloud adoption, microservices architecture, and a faster pace of change have amplified the relevance and adoption of EDA. Similar to the consolidation and the standardization that happens with Kubernetes and its ecosystem on the platform space, there is a consolidation and community-driven standardization that is happening in the event-driven space around Apache Kafka. Let see a few concrete examples. Apache Kafka has reached the point of becoming the de facto standard platform for event streaming, the same way AWS S3 is for object store, and Kubernetes is for container orchestration. Kafka has a huge community behind, a large open source ecosystem of tools and services, and possibly the largest adoption as eventing infrastructure by modern digital organizations. There are all kinds of self-hosted Kafka offerings, managed services by boutique companies, cloud providers, and recently by Red Hat too (Red Hat OpenShift Streams for Apache Kafka is a managed I’m involved with and I’d love to hear your feedback). Kafka as an API for log-based messaging is so widespread that even non-Kafka projects such as Pulsar, Red Panda, Azure Event Hubs offer compatibility with it. Kafka today is more than a 3rd party architectural dependency. Kafka influences how services are designed and implemented, it dictates how systems are scaled and made highly available, it drives how the users consume the data in real-time. But Kafka alone is like a bare Kubernetes platform without any pods. Let’s see what else in the Kafka ecosystem is a must-have complement and is becoming a de facto standard too. A Schema Registry is as important for asynchronous APIs as an API manager is for synchronous APIs. In many streaming scenarios, the event payload contains structured data that both the producer and consumer need to understand and validate. A schema registry provides a central repository and a common governance framework for schema documents and enables applications to adhere to these contracts. Today there are registries such as Apicurio by Red Hat, by Aiven, registries by Cloudera, , Confluent, Azure, AWS, and more. While schema repositories are increasing in popularity and consolidating in the capabilities and practices around schema management, at the same time they vary in licensing restrictions. Not only that, schema registries tend to leak into client applications in the form of Kafka Serializer/Deserializer (SerDes), converters, and other client dependencies. So the need for an open and vendor-neutral standard where the implementations can be swapped has been apparent for a while. And the good news is that Schema Registry API exists in CNCF and few registries such and have already started to follow it. Complementing the open source Kafka API with an open source service registry API and common governance practices feels right and I expect the adoption and consolidation in this space to grow to make the whole meta API concept a cornerstone of event-driven architectures. Similar to EDA, the concept of Change Data Capture () is not new. But the recent drivers around event-driven systems and the increasing demand for access to real-time data are building the momentum for transaction-log-driven event streaming tools. Today, there are many closed source, point-and-click tools (such as Striim, HVR, Qlik) that rely on the same transaction log concept to replicate data point-to-point. There are cloud services such as AWS DMS, Oracle GoldenGate Cloud Service and Google Datastream that will stream your data into their services (but never in the opposite direction). There are many databases, key-value stores that stream changes too. The need for an open source and vendor-neutral CDC standard that different vendors can follow and downstream change-event consumers can rely on is growing. To succeed, such a standard has to be managed on a vendor-neutral foundation and be part of a larger related ecosystem. The closest thing that exists today is CNCF which is already home to AsyncAPI, , Schema Registry, and specifications too. Today, by far, the leading open source project in the CDC space is Debezium. Debezium is by major companies, embedded into cloud services from Google, Heroku, Confluent, Aiven, Red Hat, embedded into multiple open source projects, and used by many proprietary solutions that we won’t ever know about. If you are looking for a standard in this domain, the closest de facto standard is Debezium. To clarify, with a CDC standard I don’t mean an API for data sources to emit changes. I mean standard conventions for data sources and connecting tissues such as Debezium to follow when converting database transaction logs into events. That includes data mapping (from database field types into JSON/Avro types), data structures (for example Debezium’s Before/After message structure), , partitioning of tables into topics, and primary keys into topic partitions, transaction indicators, and so forth. If you are going heavy on CDC, using Debezium will ensure consistent semantics for mapping from database transaction log entries into Apache Kafka events that are uniform across datasources.  Specifications and implementation around the Apache Kafka ecosystem There are already a few existing specifications from the event-driven space at CNCF that are gaining traction. * is OpenAPI’s equivalent for event-driven applications that recently joined CNCF. It offers a specification to document your event-driven systems to maintain consistency, and governance across different teams and tools. * CloudEvents (also part of CNCF) aims to eliminate the metadata challenge by specifying mandatory metadata information into what could be called a standard envelope. It also offers libraries for multiple programming languages for multiple protocols, which streamlines interoperability. * (another CNCF sandbox project) standardizes the creation and management of trace information that reveals the end-to-end path of events through multiple applications. * CNCF Serverless Workflow is a vendor-neutral spec for coordination asynchronous stateless and stateful interaction. * The service registry proposal in CNCF we discussed above... Whether we call it standardization, community adoption or something else, we cannot deny the consolidation process around event-driven constructs and the rise of some open source projects as de facto standards. SUMMARY Microservices are focused around the encapsulation of data that belongs to a business domain and exposing it over a minimal API as possible. But that is changing. Data going out of a service is as important as data going into it. Exposing data in microservices can no longer be an afterthought. Siloed and inaccessible data wrapped in a highly decoupled microservice are of limited value. There are new users of data and possible yet unknown users that will demand access to discoverable, understandable, real-time data. To satisfy the needs of these users, microservices have to turn data inside-out and be designed with outbound APIs that can emit data and meta APIs that make the consumption of data a self-service activity. Projects such as Apache Kafka, Debezium and schema registries are a natural enabler of this architecture and with the help of the various open source asynchronous specifications are turning into de facto choice for implementing future-proof event-driven microservices.  This article was originally published on InfoQ .</content><dc:creator>Unknown</dc:creator></entry><entry><title>A developer's guide to using Kafka with Java, Part 1</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/05/developers-guide-using-kafka-java-part-1" /><author><name>Bob Reselman</name></author><id>85766642-6713-4f8c-b7cb-29a0ae88c7b2</id><updated>2022-04-05T07:00:00Z</updated><published>2022-04-05T07:00:00Z</published><summary type="html">&lt;h2&gt;What is Kafka?&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; is a distributed, &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; messaging technology. It's all the rage these days, and with good reason: It's used to accept, record, and publish messages at a very large scale, &lt;a href="https://medium.com/hulu-tech-blog/how-hulu-uses-influxdb-and-kafka-to-scale-to-over-1-million-metrics-a-second-1721476aaff5"&gt;in excess of a million messages per second&lt;/a&gt;. Kafka is fast, it's big, and it's highly reliable. You can think of Kafka as a giant logging mechanism on steroids.&lt;/p&gt; &lt;h2&gt;What is Kafka used for?&lt;/h2&gt; &lt;p&gt;Kafka is used to collect big data, conduct real-time analysis, and process real-time streams of data—and it has the power to do all three at the same time. It can feed events to complex event streaming systems or &lt;a href="https://ifttt.com/"&gt;IFTTT&lt;/a&gt; and IoT systems or be used in accordance with in-memory &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; for added durability. It's distributed, which means it's highly scalable; adding new nodes to a Kafka cluster is all it takes.&lt;/p&gt; &lt;h2&gt;Using Java and other programming languages with Kafka&lt;/h2&gt; &lt;p&gt;Apache Kafka itself is written in &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; and Scala, and, as you'll see later in this article, it runs on JVMs. Kafka's native API was written in Java as well. But you can write application code that interacts with Kafka in a number of other programming languages, such as &lt;a href="https://github.com/Shopify/sarama"&gt;Go&lt;/a&gt;, &lt;a href="https://github.com/confluentinc/confluent-kafka-python"&gt;Python&lt;/a&gt;, or &lt;a href="https://github.com/confluentinc/confluent-kafka-dotnet"&gt;C#&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;One of the nice things about Kafka from a developer's point of view is that getting it up and running and then doing hands-on experimentation is a fairly easy undertaking. Of course, there's a lot more work that goes into implementing Kafka clusters at the enterprise level. For enterprise installations, many companies will use a scalable platform such as &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; or a service provider. Using a Kafka service provider abstracts away the work and maintenance that goes with supporting large-scale Kafka implementations. All that developers need to concern themselves with when using a service provider is producing messages into and consuming messages out of Kafka. The service provider takes care of the rest.&lt;/p&gt; &lt;p&gt;Yet, no matter what, at the most essential level a developer needs to understand how Kafka works in terms of accepting, storing, and emitting messages. In this piece, I'll cover these essentials. In addition, I'll provide instructions about how to get Kafka up and running on a local machine. I'll also demonstrate how to produce and consume messages using the Kafka Command Line Interface (CLI) tool. In subsequent articles, I'll cover some more advanced topics, such as how to write Kafka producers and consumers in a specific programming language.&lt;/p&gt; &lt;p&gt;We'll start with a brief look at the benefits that using the Java client provides. After that, we'll move on to an examination of Kafka's underlying architecture before eventually diving in to the hands-on experimentation.&lt;/p&gt; &lt;h2&gt;What are the benefits of using a Java client?&lt;/h2&gt; &lt;p&gt;As mentioned above, there are a number of language-specific clients available for writing programs that interact with a Kafka broker. One of the more popular is the Java client. Essentially, the Java client makes programming against a Kafka client a lot easier. Developers do not have to write a lot of low-level code to create useful applications that interact with Kafka. The ease of use that the Kafka client provides is the essential value proposition, but there's more, as the following sections describe.&lt;/p&gt; &lt;h3&gt;Real-time data processing&lt;/h3&gt; &lt;p&gt;When developers use the Java client to consume messages from a Kafka broker, they're getting real data in real time. Kafka is designed to emit hundreds of thousands—if not millions—of messages a second. Having access to enormous amounts of data in real time adds a new dimension to data processing. Working with a traditional database just doesn't provide this type of ongoing, real-time data access. Kafka gives you all the data you want all the time.&lt;/p&gt; &lt;h3&gt;Decouple data pipelines&lt;/h3&gt; &lt;p&gt;Flexibility is built into the Java client. For example, it's quite possible to use the Java client to create producers and consumers that send and retrieve data from a number of topics published by a Kafka installation. (As we'll discuss in more detail below, &lt;em&gt;producers&lt;/em&gt; and &lt;em&gt;consumers&lt;/em&gt; are the creators and recipients of messages within the Kafka ecosystem, and a &lt;em&gt;topic&lt;/em&gt; is a mechanism for organizing those messages.) Switching among producers and consumers of message topics is just a matter of implementing a few lines of code. This means that the Kafka client is not dedicated to a particular stream of data. It can access different pipelines according to the need at hand.&lt;/p&gt; &lt;h3&gt;Data integration&lt;/h3&gt; &lt;p&gt;Kafka is by nature an event-driven programming paradigm. Events are represented by messages that are emitted from a Kafka broker. (You'll read more about this in sections to come.) Messages coming from Kafka are structured in an agnostic format. A message can contain a simple string of data, a JSON object, or packets of binary data that can be deserialized into a language-specific object.&lt;/p&gt; &lt;p&gt;This versatility means that any message can be used and integrated for a variety of targets. For example, you could set things up so that the content of a message is transformed into a database query that stores the data in a PostgreSQL database. Or that data could be passed on to a microservice for further processing. The agnostic nature of messages coming out of Kafka makes it possible to integrate that data with any kind of data storage or processing endpoint.&lt;/p&gt; &lt;h3&gt;Scalability and automation&lt;/h3&gt; &lt;p&gt;The Java client is designed with isolation and scalability in mind. Thus, it's quite possible to scale up clients within a Java application by spawning more threads using automation logic that is internal to the application.&lt;/p&gt; &lt;p&gt;However, there are times when running a large number of threads in a single application can be a burden to the host system. As an alternative, developers can scale up Java applications and components that implement Kafka clients using a distributed application framework such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Using Kubernetes allows Java applications and components to be replicated among many physical or virtual machines. Developers can use automation scripts to provision new computers and then use the built-in replication mechanisms of Kubernetes to distribute the Java code in a load-balanced manner. The design of the Java client makes this all possible.&lt;/p&gt; &lt;h2&gt;Kafka architecture&lt;/h2&gt; &lt;p&gt;The Kafka messaging architecture is made up of three components: &lt;em&gt;producers,&lt;/em&gt; the &lt;em&gt;Kafka broker,&lt;/em&gt; and &lt;em&gt;consumers,&lt;/em&gt; as illustrated in Figure 1. We'll discuss these in more detail in the following sections.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_1.png?itok=jeFWYDkB" width="600" height="110" alt="Diagram showing the production and consumption of event messages using Kafka." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Producing and consuming event messages using Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Brokers and clusters&lt;/h3&gt; &lt;p&gt;Kafka runs using the Java Virtual Machine (JVM). An individual Kafka server is known as a &lt;em&gt;broker,&lt;/em&gt; and a broker could be a physical or virtual server. A Kafka &lt;em&gt;cluster&lt;/em&gt; is composed of one or more brokers, each of which is running a JVM. The Kafka cluster is central to the architecture, as Figure 1 illustrates.&lt;/p&gt; &lt;h3&gt;ZooKeeper&lt;/h3&gt; &lt;p&gt;In most Kafka implementations today, keeping all the cluster machines and their metadata in sync is coordinated by &lt;a href="https://zookeeper.apache.org/"&gt;ZooKeeper&lt;/a&gt;. ZooKeeper is another Apache project, and Apache describes it as "a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services."&lt;/p&gt; &lt;p&gt;While Kafka uses ZooKeeper by default to coordinate server activity and store metadata about the cluster, as of &lt;a href="https://kafka.apache.org/downloads"&gt;version 2.8.0&lt;/a&gt; Kafka can run without it by enabling Kafka Raft Metadata (&lt;a href="https://developer.confluent.io/learn/kraft/"&gt;KRaft&lt;/a&gt;) mode. When KRaft is enabled, Kafka uses internal mechanisms to coordinate a cluster's metadata. However, as of this writing, some &lt;a href="https://developer.confluent.io/learn/kraft/"&gt;companies with extensive experience&lt;/a&gt; using Kafka recommend that you avoid KRaft mode in production.&lt;/p&gt; &lt;h3&gt;Producers and consumers&lt;/h3&gt; &lt;p&gt;&lt;em&gt;Producers&lt;/em&gt; create messages that are sent to the Kafka cluster. The cluster accepts and stores the messages, which are then retrieved by a &lt;em&gt;consumer.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;There are two basic ways to produce and consume messages to and from a Kafka cluster. One way is to use the CLI tool, which is appropriate for development and experimental purposes, and that's what we'll use to illustrate Kafka concepts later on in this article.&lt;/p&gt; &lt;p&gt;Remember, though, that Kafka is designed to emit millions of messages in a very short span of time. Consuming messages at this rate goes far behind the capabilities of using the CLI tool in the real world. And in a production situation, once a message is consumed, it most likely will either be processed by the consumer or forwarded onto another target for processing and subsequent storage. Again, this type of computing is well beyond the capabilities of the CLI tool. A fast, robust programming environment is required, and so for production purposes, the preferred technique is to write application code that acts as a producer or a consumer of these messages. This too is illustrated in Figure 1.&lt;/p&gt; &lt;h3&gt;Events&lt;/h3&gt; &lt;p&gt;Typically, messages sent to and from Kafka describe &lt;em&gt;events.&lt;/em&gt; For example, an event can be a TV viewer's selection of a show from a streaming service, which is one of the use cases supported by the video streaming company Hulu. Another type of event could describe the workflow status of content creation at a daily newspaper, &lt;a href="https://open.nytimes.com/publishing-with-apache-kafka-at-the-new-york-times-7f0e3b7d2077"&gt;which is one of the &lt;em&gt;New York Times'&lt;/em&gt;&lt;/a&gt; use cases.&lt;/p&gt; &lt;h3&gt;Schema&lt;/h3&gt; &lt;p&gt;A schema defines the way that data in a Kafka message is structured. You can think of a schema as a contract between a producer and a consumer about how a data entity is described in terms of attributes and the data type associated with each attribute. Using a consistent data schema is essential for message decoupling in Kafka.&lt;/p&gt; &lt;p&gt;For example, at the conceptual level, you can imagine a schema that defines a &lt;code&gt;person&lt;/code&gt; data entity like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt; firstName (string) lastName (string) age (number) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This schema defines the data structure that a producer is to use when emitting a message to a particular topic that we'll call &lt;code&gt;Topic_A&lt;/code&gt;. (Topics will be described in detail in the following section.) The schema also describes what the consumer expects to retrieve from &lt;code&gt;Topic_A&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Now, imagine another producer comes along and emits a message to &lt;code&gt;Topic_A&lt;/code&gt; with this schema:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt; fname (string) lname (string) dob (date) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, the consumer wouldn't know what to do. Code within the consumer would log an error and move on. In order for an event-driven system to work, all parties need to be using the same data schema for a particular topic.&lt;/p&gt; &lt;h3&gt;Topics and partitions&lt;/h3&gt; &lt;p&gt;The organizational unit by which Kafka organizes a stream of messages is called a &lt;em&gt;topic.&lt;/em&gt; You can think of a topic as something like an email inbox folder. A key feature of Kafka is that it stores all messages that are submitted to a topic. When you first set Kafka up, it will save those messages for seven days by default; if you'd like, you can change this retention period by altering settings in the &lt;code&gt;config/server.properties&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;Under Kafka, a message is sent or retrieved according to its topic, and, as you can see in Figure 2, a Kafka cluster can have many topics.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig2_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig2_0.png?itok=rr9aLNE_" width="600" height="196" alt="A diagram showing how Kafka separates message streams according to topics." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Kafka separates message streams according to topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Topics are a useful way to organize messages for production and consumption according to specific types of events. For example, imagine a video streaming company that wants to keep track of when a customer logs into its service. The same company wants to keep track of when a user starts, pauses, and completes movies from its catalog. Instead of sending all those messages to a single consumer, a developer can program the set-top box or smart television application to send login events to one topic and movie start/pause/complete events to another, as shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig3_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig3_0.png?itok=M2q2OhuC" width="600" height="113" alt="Diagram illustrating that using topics wisely can make maintenance easier and improve overall application performance." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Using topics wisely can make maintenance easier and improve overall application performance. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;There are a few benefits to using topics. First, producers and consumers dedicated to a specific topic are easier to maintain, because you can update code in one producer without affecting others. The same is true for consumers. When all events are created by one producer and sent to only a single consumer, even making a subtle change in the consumer or producer means that the entire code base will need to be replaced. This is not a trivial matter.&lt;/p&gt; &lt;p&gt;Secondly, separating events among topics can optimize overall application performance. Remember, Kafka is typically used in applications where logic is distributed among a variety of machines. Thus, you can configure the Kafka cluster as well as producers and consumers to meet the burdens at hand.&lt;/p&gt; &lt;p&gt;For example, a consumer that's bound to a topic that emits hundreds of thousands of messages a second will need a lot more computing power than a consumer bound to a topic that's expected to generate only a few hundred messages in the same timespan. Logic dictates that you put the consumer requiring more computing power on a machine configured to meet that demand. The less taxing consumer can be put on a less powerful machine. You'll save in terms of resource utilization, but also in terms of dollars and cents, particularly if the producers and consumers are running on a third-party cloud.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Partitions&lt;/em&gt; distribute data across Kafka nodes. All topics are divided into partitions, and partitions can be placed on separate brokers. This facilitates the horizontal scaling of single topics across multiple servers in order to deliver superior performance and fault-tolerance far beyond the capabilities of a single server.&lt;/p&gt; &lt;h3&gt;Message streams in Kafka&lt;/h3&gt; &lt;p&gt;Topics provide a lot of versatility and independence for working with messages. While it's possible that a one-to-one relationship between producer, Kafka cluster, and consumer will suffice in many situations, there are times when a producer will need to send messages to more than one topic and a consumer will need to consume messages from more than a single topic. Kafka can accommodate complex one-to-many and many-to-many producer-to-consumer situations with no problem.&lt;/p&gt; &lt;p&gt;Figure 4 illustrates a single consumer retrieving messages from many topics, in which each topic has a dedicated producer.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig4_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig4_1.png?itok=rE0Mzy0K" width="600" height="195" alt="Diagram showing a single consumer processing messages from many topics with each topic getting messages from a dedicated producer." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: A single consumer processing messages from many topics with each topic getting messages from a dedicated producer. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Figure 5 shows a single producer creating messages that are sent to many topics. Notice that each topic has a dedicated consumer that will retrieve its messages.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig5_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig5_1.png?itok=S1Cb5-Vt" width="600" height="196" alt="A diagram showing a single producer sending messages to many topics with each topic having a dedicated consumer." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: A single producer sending messages to many topics with each topic having a dedicated consumer. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Figure 6 shows a situation in which the middle producer in the illustration is sending messages to two topics, and the consumer in the right-middle of the illustration is retrieving messages from all three topics.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig6_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig6_1.png?itok=da-h4QXa" width="600" height="251" alt="A diagram showing a producer sending messages to different topics with a consumer processing messages from many topics." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: A producer sending messages to different topics with a consumer processing messages from many topics. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The thing to remember about mixing and matching producers and consumers in one-to-one, one-to-many, or many-to-many patterns is that the real work at hand is not so much about the Kafka cluster itself, but more about the logic driving the producers and consumers. For better or worse, while there is very complex work being done internally within Kafka, it's pretty dumb in terms of message management. The scope of Kafka's concern is making sure that a message destined for a topic gets to that topic, and that consumers can get messages from a topic of interest. The actual logic that drives a message's destination is programmed in the producer. The same is true for determining topics of interest for a consumer. There is no magic in play. The content of the messages, their target topics, and how they are produced and consumed is work that is done by the programmer.&lt;/p&gt; &lt;h3&gt;Batches&lt;/h3&gt; &lt;p&gt;One of the reasons Kafka is so efficient is because events are written in &lt;em&gt;batches.&lt;/em&gt; A batch is a collection of events produced to the same partition and topic. Batches can be enormous, with streams of events happening at once. The larger the batches, the longer individual events take to propagate.&lt;/p&gt; &lt;h2&gt;What setup do you need to get started?&lt;/h2&gt; &lt;p&gt;Now that you have a basic understanding of what Kafka is and how it uses topics to organize message streams, you're ready to walk through the steps of actually setting up a Kafka cluster. Once you've done that, you'll use the Kafka CLI tool to create a topic and send messages to that topic. Finally, you'll use the CLI tool to retrieve messages from the beginning of the topic's message stream.&lt;/p&gt; &lt;p&gt;The step-by-step guide provided in the sections below assumes that you will be running Kafka under the &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; or macOS operating systems. If you're running Windows, the easiest way to get Kafka up and running is to use &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/about"&gt;Windows Subsystem for Linux&lt;/a&gt; (WSL). Running terminals under WSL is nearly identical to running them on Linux.&lt;/p&gt; &lt;p&gt;To begin, you need to confirm the Java runtime is installed on your system, and install it if it isn't. In a terminal window, execute the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; java -version &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You will see output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; openjdk version "11.0.13" 2021-10-19 OpenJDK Runtime Environment (build 11.0.13+8-Ubuntu-0ubuntu1.20.04) OpenJDK 64-Bit Server VM (build 11.0.13+8-Ubuntu-0ubuntu1.20.04, mixed mode, sharing) &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If not, you'll need to install the Java runtime. This is how you'd do it on Linux:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; sudo apt update -y &amp;&amp; sudo apt install default-jre -y &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If you're using &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;, Fedora, or CentOS, execute the following command to install Java:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; sudo dnf update -y &amp;&amp; sudo dnf install java-11-openjdk-devel -y &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;On macOS, if you have &lt;a href="https://brew.sh/"&gt;Homebrew&lt;/a&gt; installed, you can install Java using these two commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ brew tap caskroom/cask $ brew cask install java &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Next, you need to install Kafka. The following commands will download the Kafka&lt;code&gt;.tgz&lt;/code&gt; file and expand it into a directory within the &lt;code&gt;HOME&lt;/code&gt; directory.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; cd ~/ wget https://dlcdn.apache.org/kafka/3.1.0/kafka_2.13-3.1.0.tgz tar -xzf kafka_2.13-3.1.0.tgz &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now you need to get ZooKeeper up and running. In a &lt;em&gt;separate&lt;/em&gt; terminal window, execute the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; cd ~/kafka_2.13-3.1.0 bin/kafka-server-start.sh config/server.properties &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You'll see Zookeeper start up in the terminal and continuously send log information to stdout.&lt;/p&gt; &lt;p&gt;Finally, you're ready to get Kafka itself up and running. Go back to the &lt;em&gt;first&lt;/em&gt; terminal window (the one where you downloaded Kafka) and execute the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; cd ~/kafka_2.13-3.1.0 bin/kafka-server-start.sh config/server.properties &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You'll see Kafka start up in the terminal. It too will continuously send log information to stdout. That means your Kafka instance is now ready for experimentation!&lt;/p&gt; &lt;h2&gt;Running Kafka in a Linux container&lt;/h2&gt; &lt;p&gt;Kafka can be hosted in a standalone manner directly on a host computer, but it can also be run as a Linux &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt;. The following sections describe how to run Kafka on a host computer that has either Docker or Podman installed.&lt;/p&gt; &lt;h3&gt;How to start Kafka in Docker&lt;/h3&gt; &lt;p&gt;Kafka can be run as a Docker container. Before you can do so, Docker must be installed on the computer you plan to use.&lt;/p&gt; &lt;p&gt;To see if your system has Docker installed, type the following in a terminal window:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;which docker &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If Docker is installed you'll see output that looks something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; /usr/local/bin/docker &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Should this call result in no return value, Docker is not installed, and you should install it. &lt;a href="https://docs.docker.com/engine/install/"&gt;Docker's documentation&lt;/a&gt; describes how to install Docker on your computer.&lt;/p&gt; &lt;p&gt;Once Docker is installed, execute the following command to run Kafka as a Linux container:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;docker run -it --name kafka-zkless -p 9092:9092 -e LOG_DIR=/tmp/logs quay.io/strimzi/kafka:latest-kafka-2.8.1-amd64 /bin/sh -c 'export CLUSTER_ID=$(bin/kafka-storage.sh random-uuid) &amp;&amp; bin/kafka-storage.sh format -t $CLUSTER_ID -c config/kraft/server.properties &amp;&amp; bin/kafka-server-start.sh config/kraft/server.properties' &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;How to start Kafka with Podman&lt;/h3&gt; &lt;p&gt;Podman is a container engine you can use as an alternative to Docker. To see if your system has Podman installed, type the following in a terminal window:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; which podman &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If Podman is installed, you'll see output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; /usr/bin/podman &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Should this call result in no return value, Podman is not installed. &lt;a href="https://podman.io/getting-started/installation.html"&gt;Podman's documentation&lt;/a&gt; walks you through the installation process. Once Podman is installed, execute the following command to run Kafka as a Linux container using Podman:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman run -it --name kafka-zkless -p 9092:9092 -e LOG_DIR=/tmp/logs quay.io/strimzi/kafka:latest-kafka-2.8.1-amd64 /bin/sh -c 'export CLUSTER_ID=$(bin/kafka-storage.sh random-uuid) &amp;&amp; bin/kafka-storage.sh format -t $CLUSTER_ID -c config/kraft/server.properties &amp;&amp; bin/kafka-server-start.sh config/kraft/server.properties'&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Producing Kafka messages&lt;/h2&gt; &lt;p&gt;You now should have Kafka installed in your environment and you're ready to put it through its paces. In the following steps, you'll create a topic named &lt;code&gt;test_topic&lt;/code&gt; and send messages to that topic.&lt;/p&gt; &lt;h3&gt;Step 1: Create a topic&lt;/h3&gt; &lt;p&gt;Open a new terminal window, separate from any of the ones you opened previously to install Kafka, and execute the following command to create a topic named &lt;code&gt;test_topic&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; cd ~/kafka_2.13-3.1.0 bin/kafka-topics.sh --create --topic test_topic --bootstrap-server localhost:9092 &lt;/code&gt; &lt;/pre&gt; &lt;h3&gt;Step 2: Produce some messages&lt;/h3&gt; &lt;p&gt;In the terminal window where you created the topic, execute the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; bin/kafka-console-producer.sh --topic test_topic --bootstrap-server localhost:9092 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;At this point, you should see a prompt symbol (&lt;code&gt;&gt;&lt;/code&gt;). This indicates that you are at the command prompt for the Kafka CLI tool for producing messages. Enter the following message:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; This is my first event message, which is cool! &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Press the Enter key and then enter another message at the same prompt:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; This is my second event message, which is even cooler!! &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;To exit the Kafka CLI tool, press CTRL+C.&lt;/p&gt; &lt;h2&gt;Consuming Kafka messages&lt;/h2&gt; &lt;p&gt;In the open Kafka CLI terminal window in which you've been producing messages, execute the following command to consume the messages from the topic named &lt;code&gt;test_topic&lt;/code&gt; from the beginning of the message stream:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; $ bin/kafka-console-consumer.sh --topic test_topic --from-beginning --bootstrap-server localhost:9092 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You'll have to wait a few seconds for the consumer to bind to the Kafka server. Once it's done, you'll see the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; This is my first event, which is cool! This is my second event, which is even cooler! &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You've consumed all the messages in the topic named &lt;code&gt;test_topic&lt;/code&gt; from the beginning of the message stream.&lt;/p&gt; &lt;h2&gt;Putting it all together, and a look ahead&lt;/h2&gt; &lt;p&gt;This article has covered the very basics of Kafka. You learned about the concepts behind message streams, topics, and producers and consumers. Also, you learned about message retention and how to retrieve past messages sent to a topic. Finally, you got some hands-on experience installing and using Kafka on a computer running Linux.&lt;/p&gt; &lt;p&gt;Having a solid understanding of the fundamentals of Kafka is important. But please be advised that there is a lot more to know, particularly about the mechanisms that Kafka uses to support distributed messaging over a cluster made up of many computers.&lt;/p&gt; &lt;p&gt;Kafka is powerful. Using it to its full potential can become a very complex undertaking. Still, the basics discussed in this article will provide a good starting point for working with the technology in a fundamental way.&lt;/p&gt; &lt;p&gt;The next article in this series will show you how to write code that uses the &lt;code&gt;KafkaProducer&lt;/code&gt;, which is part of the Java Kafka client, to emit messages to a Kafka broker continuously. Then you'll use the &lt;code&gt;KafkaConsumer&lt;/code&gt; to continuously retrieve and process all the messages emitted.&lt;/p&gt; &lt;p&gt;Want to learn more about Kafka in the meantime? Check out the &lt;a href="https://developers.redhat.com/learn/openshift-streams-for-apache-kafka"&gt;Red Hat OpenShift Streams for Apache Kafka learning paths&lt;/a&gt; from Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/05/developers-guide-using-kafka-java-part-1" title="A developer's guide to using Kafka with Java, Part 1"&gt;A developer's guide to using Kafka with Java, Part 1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-04-05T07:00:00Z</dc:date></entry><entry><title type="html">Riding Camel Quarkus: effortless APIs</title><link rel="alternate" href="https://quarkus.io/blog/camel-quarkus-effortless-apis/" /><author><name>Bruno Meseguer</name></author><id>https://quarkus.io/blog/camel-quarkus-effortless-apis/</id><updated>2022-04-05T00:00:00Z</updated><content type="html">Discover Camel, the swiss-knife of integration brought to Quarkus. The example encourages API best practices, with effortless coding effort showcasing AtlasMap for data transformation. Introduction Quarkus offers an extensive collection of extensions to connect to web, data and messaging systems, providing the developer fantastic functionality at his disposal. However, in...</content><dc:creator>Bruno Meseguer</dc:creator></entry><entry><title>Writing Kubernetes Operators in Java with JOSDK, Part 3: Implementing a controller</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/04/writing-kubernetes-operators-java-josdk-part-3-implementing-controller" /><author><name>Christophe Laprun</name></author><id>bfd68675-9e7c-4830-b324-542930685004</id><updated>2022-04-04T18:30:00Z</updated><published>2022-04-04T18:30:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://javaoperatorsdk.io"&gt;Java Operator SDK&lt;/a&gt; (JOSDK) is an open source project that aims to simplify the task of creating &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; Operators using &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;. The project was started by &lt;a href="https://container-solutions.com"&gt;Container Solutions&lt;/a&gt; and Red Hat is now a major contributor.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2022/02/15/write-kubernetes-java-java-operator-sdk"&gt;first article in this series&lt;/a&gt; introduced JOSDK and gave reasons for creating Operators in Java. The &lt;a href="https://developers.redhat.com/articles/2022/03/22/write-kubernetes-java-java-operator-sdk-part-2"&gt;second article&lt;/a&gt; showed how the &lt;a href="https://github.com/quarkiverse/quarkus-operator-sdk"&gt;quarkus-operator-sdk extension&lt;/a&gt; for JOSDK, together with the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; framework, facilitates the development experience by managing the custom resource definition automatically. This article focuses on adding the reconciliation logic.&lt;/p&gt; &lt;h2&gt;Where things stand&lt;/h2&gt; &lt;p&gt;You ended the second article having more or less established the model for your custom resource (CR). You developed it iteratively, thanks to the Quarkus extension for JOSDK. For reference, the CRs look similar to:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: "halkyon.io/v1alpha1" kind: ExposedApp metadata: name: &lt;Name of your application&gt; spec: imageRef: &lt;Docker image reference&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's now examine what's required to implement the second aspect of Operators: Writing a controller to handle your CRs. In JOSDK parlance, a Kubernetes controller is represented by a &lt;code&gt;Reconciler&lt;/code&gt; implementation, and the association of a &lt;code&gt;Reconciler&lt;/code&gt; implementation with its configuration is called a &lt;code&gt;Controller&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Reconciler&lt;/h2&gt; &lt;p&gt;Here's the implementation that the &lt;code&gt;operator-sdk create api&lt;/code&gt; command generated for you when you ran it in the second article:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;package io.halkyon; import io.fabric8.kubernetes.client.KubernetesClient; import io.javaoperatorsdk.operator.api.reconciler.Context; import io.javaoperatorsdk.operator.api.reconciler.Reconciler; import io.javaoperatorsdk.operator.api.reconciler.UpdateControl; public class ExposedAppReconciler implements Reconciler&lt;ExposedApp&gt; { private final KubernetesClient client; public ExposedAppReconciler(KubernetesClient client) { this.client = client; } // TODO Fill in the rest of the reconciler @Override public UpdateControl&lt;ExposedApp&gt; reconcile(ExposedApp resource, Context context) { // TODO: fill in logic return UpdateControl.noUpdate(); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As mentioned previously, your &lt;code&gt;Reconciler&lt;/code&gt; implementation needs to be parameterized by your custom resource class (&lt;code&gt;ExposedApp&lt;/code&gt; in this instance).&lt;/p&gt; &lt;p&gt;By default, you have to implement only one method, called &lt;code&gt;reconcile()&lt;/code&gt;. This method takes two parameters: the resource for which the reconciliation was triggered, and a &lt;code&gt;Context&lt;/code&gt; object that allows your reconciler to retrieve contextual information from the SDK.&lt;/p&gt; &lt;p&gt;If you are used to writing Operators in &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt;, you might be surprised: Where is the watcher or informer implementation, and where is the manager? JOSDK takes care of wiring everything together so that your reconciler implementation is called whenever there is a modification to a resource that the reconciler declares ownership of. The runtime extracts the resource in question and passes it to your &lt;code&gt;reconcile()&lt;/code&gt; method automatically without you having to do anything specific. The Quarkus extension for JOSDK also simplifies things further by taking care of creating an &lt;code&gt;Operator&lt;/code&gt; instance and registering your reconciler with it.&lt;/p&gt; &lt;h3&gt;Controller configuration&lt;/h3&gt; &lt;p&gt;Let's look at how to configure your reconciler before diving into the implementation of your reconciliation logic. As explained above, the &lt;code&gt;Controller&lt;/code&gt; associates a &lt;code&gt;Reconciler&lt;/code&gt; with its configuration. You rarely have to deal with &lt;code&gt;Controller&lt;/code&gt;s directly, though, because a &lt;code&gt;Controller&lt;/code&gt; instance is created automatically when you register your reconciler (or when the Quarkus extension does it automatically for you).&lt;/p&gt; &lt;p&gt;Looking at the logs of your Operator as it starts, you should see:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;INFO [io.jav.ope.Operator] (Quarkus Main Thread) Registered reconciler: 'exposedappreconciler' for resource: 'class io.halkyon.ExposedApp' for namespace(s): [all namespaces] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;exposedappreconciler&lt;/code&gt; is the name that was automatically generated for your reconciler.&lt;/p&gt; &lt;p&gt;Also note that the reconciler is registered automatically to watch all namespaces in your cluster. The reconciler will therefore receive any event associated with your custom resources, wherever it might happen on the cluster. Although this might be convenient when developing your controller, it's not necessarily how you'd like your controller to operate when deployed to production. JOSDK offers several ways to control this particular aspect of association, and the Quarkus extension adds several more options as well.&lt;/p&gt; &lt;p&gt;Let's look at what's probably the most common option: The &lt;code&gt;@ControllerConfiguration&lt;/code&gt; annotation, which allows you to configure which namespaces your reconciler will watch, among other features. You can use the option by setting the &lt;code&gt;namespaces&lt;/code&gt; field of the annotation to a list of comma-separated namespace names. If the field is not set, which is the case by default, reconcilers are configured to watch all namespaces. An interesting option is to make the reconciler solely watch the namespace in which the Operator is deployed. This restriction is imposed by specifying the &lt;code&gt;Constants.WATCH_CURRENT_NAMESPACE&lt;/code&gt; value for the &lt;code&gt;namespaces&lt;/code&gt; field.&lt;/p&gt; &lt;p&gt;We won't go into the details of all the configuration options here, because that's not the topic of this article, but we'll mention that you can also configure your controller programmatically. Alternatively, you can take advantage of the &lt;code&gt;application.properties&lt;/code&gt; file commonly used in Quarkus, as is typical when &lt;a href="https://quarkus.io/guides/config-reference"&gt;configuring Quarkus applications&lt;/a&gt;. The name of your reconciler is used in &lt;code&gt;application.properties&lt;/code&gt; for configuration options that affect your controller specifically. While a name is automatically generated based on your reconciler's class name, it might be useful to provide one that makes more sense to you or that is easier to remember. You can specify a name using the &lt;code&gt;name&lt;/code&gt; field of the &lt;code&gt;@ControllerConfiguration&lt;/code&gt; annotation.&lt;/p&gt; &lt;p&gt;Let's rename your reconciler and configure it to watch only the current namespace:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;&lt;br /&gt;@ControllerConfiguration(namespaces = Constants.WATCH_CURRENT_NAMESPACE, name = "exposedapp") public class ExposedAppReconciler implements Reconciler&lt;ExposedApp&gt; { // rest of the code here } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Since the configuration has changed, the Quarkus extension restarts your Operator and shows that the new configuration has indeed been taken into account:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;INFO [io.jav.ope.Operator] (Quarkus Main Thread) Registered reconciler: 'exposedapp' for resource: 'class io.halkyon.ExposedApp' for namespace(s): [default] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you wanted to watch only the &lt;code&gt;foo&lt;/code&gt;, &lt;code&gt;bar&lt;/code&gt;, and &lt;code&gt;baz&lt;/code&gt; namespaces, you could modify &lt;code&gt;application.properties&lt;/code&gt; to change the &lt;code&gt;namespaces&lt;/code&gt; configuration as follows, using the newly configured name for your reconciler:&lt;/p&gt; &lt;pre&gt;&lt;code class="properties"&gt;quarkus.operator-sdk.controllers.exposedapp.namespaces=foo,bar,baz &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Reconciliation logic&lt;/h3&gt; &lt;p&gt;Now that you have configured your controller, it's time to implement the reconciliation logic. Whenever you create an &lt;code&gt;ExposedApp&lt;/code&gt; CR, your Operator needs to create three dependent resources: a &lt;code&gt;Deployment&lt;/code&gt;, a &lt;code&gt;Service&lt;/code&gt;, and an &lt;code&gt;Ingress&lt;/code&gt;. This concept of dependent resources is central to writing Operators: The desired state that you're targeting, as materialized by your CR, very often requires managing the state of several other resources either within Kubernetes or completely external to the cluster. Managing this state is what the &lt;code&gt;reconcile()&lt;/code&gt; method is all about.&lt;/p&gt; &lt;p&gt;Kubernetes doesn't offer explicit ways to manage such related resources together, so it's up to controllers to identify and manage resources of interest. One common way to specify that resources belong together is to use &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/"&gt;labels&lt;/a&gt;. The use of labels is so common that there is a &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/"&gt;set of recommended labels&lt;/a&gt; to help you manage the resources associated with your applications.&lt;/p&gt; &lt;p&gt;Since the goal of the Operator you're developing here is to expose an application, it makes sense to add some of these labels to all the resources associated with your application, at least to be able to visualize them. Because the goal of this article is not to create a production-ready Operator, here you add only the &lt;code&gt;app.kubernetes.io/name&lt;/code&gt; label and set its value to the name of your CR.&lt;/p&gt; &lt;p&gt;One aspect that Kubernetes can take care of automatically, though, is the lifecycle of dependent resources. More specifically, it often makes sense to remove all dependent resources when the primary resource is removed from the cluster. Removing the dependent resources make sense particularly in this use case: If you remove your &lt;code&gt;ExposedApp&lt;/code&gt; CR from the cluster, you don't want to have to manually delete all the associated resources that your Operator created.&lt;/p&gt; &lt;p&gt;Of course, it's perfectly possible for your Operator to react to a deletion event of your CR by programmatically deleting the associated resources. However, if Kubernetes can do it automatically for you, you should definitely take advantage of this feature. You can ask Kubernetes to take care of this deletion by adding an &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/owners-dependents/"&gt;owner reference&lt;/a&gt; pointing to your CR in all your dependent resources. This lets Kubernetes know that your primary resource (&lt;code&gt;ExposedApp&lt;/code&gt; in this example) owns a set of associated dependent resources, so Kubernetes will automatically clean those up when the owner is deleted.&lt;/p&gt; &lt;p&gt;Both labels and owner references are part of a &lt;code&gt;metadata&lt;/code&gt; resource field.&lt;/p&gt; &lt;p&gt;But enough theory. Let's look at the reconciler skeleton the &lt;code&gt;operator-sdk&lt;/code&gt; tool generated for you:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;public class ExposedAppReconciler implements Reconciler&lt;ExposedApp&gt; { private final KubernetesClient client; public ExposedAppReconciler(KubernetesClient client) { this.client = client; } // TODO Fill in the rest of the reconciler @Override public UpdateControl&lt;ExposedApp&gt; reconcile(ExposedApp exposedApp, Context context) { // TODO: fill in logic return UpdateControl.noUpdate(); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The generated code gives you access to a &lt;code&gt;KubernetesClient&lt;/code&gt; field. It's an automatically injected instance of the &lt;a href="https://github.com/fabric8io/kubernetes-client"&gt;Kubernetes client provided by the Fabric8 project&lt;/a&gt;. You automatically get an instance configured to access the cluster you're connected to. You can, of course, configure the client using the configuration options provided by the &lt;a href="https://quarkus.io/guides/kubernetes-client#configuration-reference"&gt;Quarkus Kubernetes client extension&lt;/a&gt;, if necessary. This client was chosen because it offers an interface that is very natural for Java developers, providing a &lt;a href="https://java-design-patterns.com/patterns/fluentinterface/"&gt;fluent API&lt;/a&gt; to interact with the Kubernetes API. Each Kubernetes API group is represented by a specific interface, guiding the user during their interaction with the cluster.&lt;/p&gt; &lt;p&gt;For example, to operate on Kubernetes &lt;code&gt;Deployment&lt;/code&gt;s, which are defined in the &lt;code&gt;apps&lt;/code&gt; API group, retrieve the interface specific to a &lt;code&gt;Deployment&lt;/code&gt; by calling &lt;code&gt;client.apps().deployments()&lt;/code&gt;, where &lt;code&gt;client&lt;/code&gt; is your Kubernetes client instance. To interact with CRDs in version &lt;code&gt;v1&lt;/code&gt;, defined in the &lt;code&gt;apiextensions.k8s.io&lt;/code&gt; group, call &lt;code&gt;client.apiextensions().v1().customResourceDefinitions()&lt;/code&gt;, etc.&lt;/p&gt; &lt;p&gt;The logic of your Operator is implemented in the following method:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt; public UpdateControl&lt;ExposedApp&gt; reconcile(ExposedApp exposedApp, Context context) { &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This method gets triggered automatically by JOSDK whenever an &lt;code&gt;ExposedApp&lt;/code&gt; is created or modified on the cluster. The resource that triggered the method is provided: That's the &lt;code&gt;exposedApp&lt;/code&gt; parameter in the previous snippet. The &lt;code&gt;context&lt;/code&gt; parameter provides, quite logically, contextual information about the current reconciliation. You'll learn more about this parameter in greater detail later.&lt;/p&gt; &lt;p&gt;The function has to return an &lt;code&gt;UpdateControl&lt;/code&gt; instance. This return value tells JOSDK what needs to be done with your resource after the reconciliation is finished. Typically, you want to change the status of the resource, in which case you return &lt;code&gt;UpdateControl.updateStatus&lt;/code&gt;, passing it your updated resource. If you want to enrich your resource with added metadata, return &lt;code&gt;UpdateControl.updateResource&lt;/code&gt;. Return &lt;code&gt;UpdateControl.noUpdate&lt;/code&gt; if no changes at all are needed on your resource.&lt;/p&gt; &lt;p&gt;While it's technically possible to even change the &lt;code&gt;spec&lt;/code&gt; field of your resource, remember that this field represents the desired state specified by the user, and shouldn't be changed unduly by the Operator.&lt;/p&gt; &lt;p&gt;Now let's see what you need to do to create the &lt;code&gt;Deployment&lt;/code&gt; associated with your &lt;code&gt;ExposedApp&lt;/code&gt; CR:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;final var name=exposedApp.getMetadata().getName(); final var spec=exposedApp.getSpec(); final var imageRef=spec.getImageRef(); var deployment =new DeploymentBuilder() .withMetadata(createMetadata(exposedApp, labels)) .withNewSpec() .withNewSelector().withMatchLabels(labels).endSelector() .withNewTemplate() .withNewMetadata().withLabels(labels).endMetadata() .withNewSpec() .addNewContainer() .withName(name).withImage(imageRef) .addNewPort() .withName("http").withProtocol("TCP").withContainerPort(8080) .endPort() .endContainer() .endSpec() .endTemplate() .endSpec() .build(); client.apps().deployments().createOrReplace(deployment); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's go through this code. First, assuming that &lt;code&gt;exposedApp&lt;/code&gt; is the instance of your &lt;code&gt;ExposedApp&lt;/code&gt; CR that JOSDK provides you with when it triggers your &lt;code&gt;reconcile()&lt;/code&gt; method, you retrieve its name and extract the &lt;code&gt;imageRef&lt;/code&gt; value from its &lt;code&gt;spec&lt;/code&gt;. Remember that this field records the image reference of the application you want to expose. You will use that information to build a &lt;code&gt;Deployment&lt;/code&gt; using the Fabric8 client's &lt;code&gt;DeploymentBuilder&lt;/code&gt; class.&lt;/p&gt; &lt;p&gt;The fluent interface makes the code reads almost like English: you create the metadata from the labels, use these labels to create a selector, and create a new template for spawned containers. These containers are named after the CR (its &lt;code&gt;name&lt;/code&gt; is used as the container's name), and the image is specified quite logically by the &lt;code&gt;imageRef&lt;/code&gt; value extracted from the CR spec. In this example, the port information is hardcoded, but you could extend the CR to add that information as well.&lt;/p&gt; &lt;p&gt;Finally, after retrieving the &lt;code&gt;Deployment&lt;/code&gt;-specific interface, the method calls &lt;code&gt;createOrReplace()&lt;/code&gt;, which, as its name implies, either creates the &lt;code&gt;Deployment&lt;/code&gt; on the cluster or replaces it with the new values if the &lt;code&gt;Deployment&lt;/code&gt; already exists. Easy enough.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;createMetadata()&lt;/code&gt; method is in charge of setting the labels, but also needs to set the owner reference on your dependent resources:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;private ObjectMeta createMetadata(ExposedApp resource, Map&lt;String, String&gt; labels){ final var metadata=resource.getMetadata(); return new ObjectMetaBuilder() .withName(metadata.getName()) .addNewOwnerReference() .withUid(metadata.getUid()) .withApiVersion(resource.getApiVersion()) .withName(metadata.getName()) .withKind(resource.getKind()) .endOwnerReference() .withLabels(labels) .build(); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;Service&lt;/code&gt; is created in a similar fashion:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;client.services().createOrReplace(new ServiceBuilder() .withMetadata(createMetadata(exposedApp, labels)) .withNewSpec() .addNewPort() .withName("http") .withPort(8080) .withNewTargetPort().withIntVal(8080).endTargetPort() .endPort() .withSelector(labels) .withType("ClusterIP") .endSpec() .build()); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;Ingress&lt;/code&gt; is slightly more complex because it depends on which &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Ingress controller&lt;/a&gt; is deployed on your cluster. In this example, you're configuring the &lt;code&gt;Ingress&lt;/code&gt; specifically for the &lt;a href="https://kubernetes.github.io/ingress-nginx/"&gt;NGINX controller&lt;/a&gt;. If your cluster uses a different controller, the configuration would probably be different:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;final var metadata = createMetadata(exposedApp, labels); metadata.setAnnotations(Map.of( "nginx.ingress.kubernetes.io/rewrite-target", "/", "kubernetes.io/ingress.class", "nginx" )); client.network().v1().ingresses().createOrReplace(new IngressBuilder() .withMetadata(metadata) .withNewSpec() .addNewRule() .withNewHttp() .addNewPath() .withPath("/") .withPathType("Prefix") .withNewBackend() .withNewService() .withName(metadata.getName()) .withNewPort().withNumber(8080).endPort() .endService() .endBackend() .endPath() .endHttp() .endRule() .endSpec() .build()); &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And that's about it for this very simple reconciliation algorithm. The only thing left to do is return an &lt;code&gt;UpdateControl&lt;/code&gt; to let JOSDK know what to do with your CR after it's reconciled. In this case, you're not modifying the CR in any form, so you simply return &lt;code&gt;UpdateControl.noUpdate()&lt;/code&gt;. JOSDK now knows that it doesn't need to send an updated version of your CR to the cluster and can update its internal state accordingly.&lt;/p&gt; &lt;p&gt;At this point, your Operator should still be running using Quarkus dev mode. If you create an &lt;code&gt;ExposedApp&lt;/code&gt; resource and apply it to the cluster using &lt;code&gt;kubectl apply&lt;/code&gt;, your Operator should now create the associated resources, as you can see by running the following command:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;$ kubectl get all -l app.kubernetes.io/name=&lt;name of your CR&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If everything worked well, you should indeed see that a &lt;code&gt;Pod&lt;/code&gt;, a &lt;code&gt;Service&lt;/code&gt;, a &lt;code&gt;Deployment&lt;/code&gt;, and a &lt;code&gt;ReplicaSet&lt;/code&gt; have all been created for your application. The &lt;code&gt;Ingress&lt;/code&gt; is not part of the resources displayed by the &lt;code&gt;kubectl get all&lt;/code&gt; command, so you need a separate command to check on your &lt;code&gt;Ingress&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;$ kubectl get ingresses.networking.k8s.io -l app.kubernetes.io/name=&lt;name of your CR&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When writing this article, I created a &lt;code&gt;hello-quarkus&lt;/code&gt; project with an &lt;code&gt;ExposedApp&lt;/code&gt; that exposes a simple "Hello World" Quarkus application, and got the following result:&lt;/p&gt; &lt;pre&gt;&lt;code class="shell"&gt;$ kubectl get ingresses.networking.k8s.io -l app.kubernetes.io/name=hello-quarkus NAME CLASS HOSTS ADDRESS PORTS AGE hello-quarkus &lt;none&gt; * localhost 80 9m40s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This application exposes a &lt;code&gt;hello&lt;/code&gt; endpoint, and visiting &lt;code&gt;http://localhost/hello&lt;/code&gt; resulted in the expected greeting. For your convenience, we put the code of this operator in the &lt;a href="https://github.com/halkyonio/exposedapp-rhdblog"&gt;exposedapp-rhdblog&lt;/a&gt; GitHub repository. Future installments of this series will add more to this code, but the version specific to this installment will always be accessible via &lt;a href="https://github.com/halkyonio/exposedapp-rhdblog/tree/part-3"&gt;the part-3 tag&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This concludes part 3 of our series. You've finally implemented a very simple Operator and learned more about JOSDK in the process.&lt;/p&gt; &lt;p&gt;In part 1, you learned that one interesting aspect of Operators is that they enable users to deal with a Kubernetes cluster via the lens of an API customized to their needs and comfort level with Kubernetes clusters. In the use case we chose for this series, this simplified view is materialized by the &lt;code&gt;ExposedApp&lt;/code&gt; API. However, this article has demonstrated that, although the tools you've used have made it easier to expose an application via only its image reference, knowing &lt;em&gt;where&lt;/em&gt; to access the application is not trivial.&lt;/p&gt; &lt;p&gt;Similarly, checking whether things are working properly requires knowing about labels and how to retrieve associated resources from the cluster. The process is not difficult, but your Operator currently fulfills only one part of its contract. Thus, the next article in this series will look into adding that information to your CR so that your users really need to deal only with your Kubernetes extension and nothing else.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/04/writing-kubernetes-operators-java-josdk-part-3-implementing-controller" title="Writing Kubernetes Operators in Java with JOSDK, Part 3: Implementing a controller"&gt;Writing Kubernetes Operators in Java with JOSDK, Part 3: Implementing a controller&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Christophe Laprun</dc:creator><dc:date>2022-04-04T18:30:00Z</dc:date></entry><entry><title type="html">Kogito 1.19.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/04/kogito-1-19-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/04/kogito-1-19-0-released.html</id><updated>2022-04-04T13:39:08Z</updated><content type="html">We are glad to announce that the Kogito 1.19.0 release is now available! This goes hand in hand with , release. From a feature point of view, we included a series of new features and bug fixes, including: * Serverless Workflow expression handling improvements * Serverless Workflow event timeouts support is implemented for the and states. For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.18.0 artifacts are available at the . A detailed changelog for 1.19.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry></feed>
