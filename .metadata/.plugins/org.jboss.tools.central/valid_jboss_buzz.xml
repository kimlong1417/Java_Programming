<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>A SaaS architectural checklist for Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/18/saas-architectural-checklist-kubernetes" /><author><name>Mike Guerette, Rob Terzi, Michael Hrivnak</name></author><id>dabec593-28d2-4528-abef-debad18ed90d</id><updated>2022-05-18T07:00:00Z</updated><published>2022-05-18T07:00:00Z</published><summary type="html">&lt;p&gt;This is the first in a series of articles about building and deploying &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-saas"&gt;software as a service (SaaS)&lt;/a&gt; applications, which will focus on software and deployment architectures. The topics the series will cover can be used as the basis for a checklist for SaaS architecture. These topics include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Approaches to providing &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-multitenancy"&gt;multitenancy&lt;/a&gt; in SaaS applications&lt;/li&gt; &lt;li&gt;Security controls and considerations for SaaS&lt;/li&gt; &lt;li&gt;Approaches to storing and managing tenants' persistent data&lt;/li&gt; &lt;li&gt;Build and deployment pipelines and &lt;a href="https://developers.redhat.com/topics/automation/all"&gt;automation&lt;/a&gt; for SaaS development&lt;/li&gt; &lt;li&gt;Scalability, high availability, and disaster recovery for SaaS deployments&lt;/li&gt; &lt;li&gt;Options for deploying existing non-containerized applications in a SaaS environment&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As we publish articles in this series, we will update this article and provide the links to them. Bookmark this article for easy access to the complete set.&lt;/p&gt; &lt;p&gt;Throughout the series, we will highlight technologies that can be used to help you build and deploy SaaS applications. In particular, we will discuss best practices and the advantages that a container orchestration platform like &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; have for running SaaS applications.&lt;/p&gt; &lt;p&gt;This article provides background information on SaaS. Those unfamiliar with Kubernetes will also get a brief introduction to that platform to prepare you for the rest of this series.&lt;/p&gt; &lt;h2&gt;What is SaaS?&lt;/h2&gt; &lt;p&gt;From the end users' perspective, the defining characteristic of a SaaS application is that it's on-demand software typically delivered through the web, so the users only need a browser or mobile device to access it. The provider of the SaaS application manages all the software and infrastructure needed to deliver the application service to the consumers of the application.&lt;/p&gt; &lt;p&gt;SaaS applications are economically attractive because the development, infrastructure, and support costs are shared by multiple customers of the service. Consumers can start using the application without capital expenditures or waiting for the software to be installed. Subscription-based pricing lowers the risk for consumers, which results in shortened sales cycles for SaaS providers. The recurring revenue stream from subscriptions can be invested into the application to grow market share.&lt;/p&gt; &lt;p&gt;Agility is a significant advantage that SaaS providers have over traditional software vendors. SaaS consumers have nearly immediate access to new releases, allowing SaaS providers to iterate quickly. The ability to deliver new features to market quickly can be a competitive edge.&lt;/p&gt; &lt;p&gt;Because SaaS applications are shared, their consumers are referred to as &lt;em&gt;tenants.&lt;/em&gt; A tenant could either be a single user or a collection of users who belong to the same organization. Each tenant has a view that shows that they are using a private resource. Even though they might be sharing hardware or software instances, other tenants' use of the system is usually completely hidden from view. This is referred to as &lt;em&gt;multitenancy.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Red Hat OpenShift provides many capabilities to make SaaS deployments easier. It is a platform-as-a-service (PaaS) that builds on Kubernetes' capabilities to orchestrate multiple workloads and provide rapid scaling—capabilities that have made Kubernetes the de facto standard for cloud-native applications. Automated build and deployment pipelines, &lt;a href="https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator"&gt;Kubernetes Operators&lt;/a&gt;, and the ability to manage &lt;a href="https://www.redhat.com/en/topics/automation/what-is-infrastructure-as-code-iac"&gt;infrastructure as code&lt;/a&gt; are just some of the features that make OpenShift ideal for building and deploying SaaS solutions.&lt;/p&gt; &lt;h2&gt;What Kubernetes concepts are important for SaaS?&lt;/h2&gt; &lt;p&gt;For the discussion of SaaS architecture in this series, it is helpful to understand a few relevant terms and concepts of related to Kubernetes and Red Hat OpenShift. In this series, any discussion of Kubernetes will also apply to Red Hat OpenShift unless explicitly noted, since Red Hat OpenShift is an enterprise Kubernetes platform.&lt;/p&gt; &lt;p&gt;A Kubernetes &lt;em&gt;cluster&lt;/em&gt; is a group of hosts working together to run Linux containers in an orchestrated fashion. The Kubernetes concepts relevant to SaaS architecture include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Control plane: &lt;/strong&gt;The collection of processes that manage the Kubernetes cluster and allocate work to the hosts in that cluster. For a production environment, the control plane typically runs on three dedicated nodes to provide high availability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Worker nodes: &lt;/strong&gt;These are the hosts that run the application workloads in the cluster. The control plane assigns work to these nodes in the form of &lt;em&gt;pods.&lt;/em&gt; The number of worker node hosts in the cluster is determined by the anticipated or observed load of the applications that run on the cluster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pod:&lt;/strong&gt; A group of one or more &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; running on a single node. The containers running in the pod can be the application itself or application components such as a web server. A node can run many unrelated pods or multiple instances of the same pod as necessary to handle the load.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deployments and workload management:&lt;/strong&gt; Kubernetes provides a number of flexible mechanisms, including &lt;a href="https://www.redhat.com/en/topics/containers/what-is-kubernetes-deployment"&gt;deployments&lt;/a&gt; and stateful sets, for declaring which workloads should be running and how they should be managed by the cluster's control plane. A deployment can be used to declare that a specific number of pods running the application's web server are required. It is then up to Kubernetes to decide which nodes to run those pods on and replace them if either a pod or node fails.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Services:&lt;/strong&gt; Services within Kubernetes provide a mechanism to access an application component, such as a web service, running in one or more pods while avoiding any coupling to a specific pod or pods. Pods are ephemeral; they can be created and destroyed as necessary for failover and scaling. The abstraction of the network from services allows Kubernetes to route requests and provide load balancing between pods.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The components of a Kubernetes cluster are shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kubernetes_diagram-v3-770x717_0_0_v2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kubernetes_diagram-v3-770x717_0_0_v2.png?itok=zq0WmAQc" width="600" height="559" alt="Diagram showing the components of a Kubernetes cluster" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The components of a Kubernetes cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;Pods abstract network and storage away from the underlying containers. This allows pods to move to other worker nodes in the cluster as needed. The containers in a pod share a virtual IP address, hostname, and other resources that allow the containers within a pod to communicate with each other.&lt;/p&gt; &lt;p&gt;To allow pods to be easily moved and replicated, network services such as web servers are exposed outside of a pod by defining Kubernetes services. Kubernetes is responsible for routing any service requests to a running pod that can handle them, no matter where it is running in the cluster. In addition to allowing pods to move between worker nodes, using services also allows scaling by providing the ability to route and balance requests to multiple pods providing the same service.&lt;/p&gt; &lt;p&gt;For more information on Kubernetes concepts, refer to Red Hat's &lt;a href="https://www.redhat.com/en/topics/containers/what-is-kubernetes"&gt;What is Kubernetes?&lt;/a&gt; article.&lt;/p&gt; &lt;h2&gt;A look ahead&lt;/h2&gt; &lt;p&gt;The next articles in this SaaS architecture series will cover approaches to multitenancy, security, and the options available for creating a defense-in-depth security strategy. The full list of articles will remain at the top of this article.&lt;/p&gt; &lt;p&gt;Red Hat SaaS Foundations is a partner program designed for building enterprise-grade SaaS solutions on the Red Hat OpenShift or &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; platforms and deploying them across multiple cloud and non-cloud footprints. &lt;a href="http://mailto:saas@redhat.com"&gt;Email&lt;/a&gt; us to learn more about partnering with Red Hat to build your SaaS.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architectural-checklist-kubernetes" title="A SaaS architectural checklist for Kubernetes"&gt;A SaaS architectural checklist for Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mike Guerette, Rob Terzi, Michael Hrivnak</dc:creator><dc:date>2022-05-18T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.21.0 released!</title><link rel="alternate" href="https://blog.kie.org/2022/05/kogito-1-21-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2022/05/kogito-1-21-0-released.html</id><updated>2022-05-17T11:06:25Z</updated><content type="html">We are glad to announce that the Kogito 1.21.0 release is now available! This goes hand in hand with , release. From a feature point of view, we included a series of new features and bug fixes, including: * New source files add-on that allows you to download the source files (.bpmn, .bpmn2, .json, and .yaml). * New Jobs integration add-on based on Messaging / Kafka * OAuth2 support on Serveless Workflow OpenAPI BREAKING CHANGES * Persistence add-ons for Spring Boot runtime have been updated with a new Maven Artifact Id, for more details see . * Codegen maven step should be disable or there is a likely chance openapi in SWF will stop working (Quarkiverse integration is enabled with codegen maven step and is still in experimental phase, some specs are working, but others not) For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.19.0 artifacts are available at the . A detailed changelog for 1.21.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title>Manage JFR across instances with Cryostat and GraphQL</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/17/manage-jfr-across-instances-cryostat-and-graphql" /><author><name>Andrew Azores</name></author><id>2b8cc495-858d-4cd6-b5e4-f140f6b8684e</id><updated>2022-05-17T07:00:00Z</updated><published>2022-05-17T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://cryostat.io"&gt;Cryostat&lt;/a&gt; manages the monitoring of &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; applications using &lt;a href="https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170"&gt;Java Flight Recorder&lt;/a&gt; (JFR) in the cloud. Cryostat 2.1 includes support for &lt;a href="https://graphql.org"&gt;GraphQL&lt;/a&gt; to control flight recordings on multiple applications, &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; pods, with powerful filtering capacities. This article discusses the motivation for adding GraphQL support, shares some examples of queries along with expected results, and takes a look at the underlying web requests on the GraphQL endpoint.&lt;/p&gt; &lt;h2&gt;Why use GraphQL with Cryostat?&lt;/h2&gt; &lt;p&gt;Previous versions of Cryostat exposed flight recordings with a simple HTTP REST API that limited each request to a single conceptual action—starting one recording on one replica instance, for example. But Cryostat serves developers who create &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; architectures running large numbers of instances in multiple scaled replicas, and each replicated instance can have many flight recordings running for various purposes.&lt;/p&gt; &lt;p&gt;Thus, a developer who wanted to start an identical recording on 12 replicas of a simple container needed to make 12 API requests. Even worse, a developer starting an identical recording on 10 replicas across 10 microservices—a common use case—needed to make 10x10=100 API requests.&lt;/p&gt; &lt;p&gt;Clearly, Cryostat needs a flexible and powerful API for managing all these conditions. GraphQL is a simple but capable language for issuing queries on groups of carefully chosen instances. Reducing multiple actions to a single API request improves overall performance significantly, due to reduced network traffic overhead. As an additional benefit, developers can write much shorter and simpler queries to perform complex actions, rather than writing custom clients that parse API JSON responses and perform actions iteratively on response data.&lt;/p&gt; &lt;p&gt;Queries for target JVMs and the active or archived recordings that belong to them, as well as recordings present in the general Cryostat archives, can combine with mutations to start, stop, archive, and delete active or archived recordings to create powerful queries for building automation around Cryostat and JDK Flight Recorder.&lt;/p&gt; &lt;h2&gt;Example queries&lt;/h2&gt; &lt;p&gt;Cryostat accepts queries formatted in GraphQL at its &lt;code&gt;/api/beta/graphql&lt;/code&gt; endpoint. If you are completely unfamiliar with GraphQL, I suggest you take a brief look at &lt;a href="https://graphql.org/learn/"&gt;its documentation&lt;/a&gt; to gain a better understanding of which parts in the examples in this article are just GraphQL syntax and concepts, versus Cryostat-specific behavior.&lt;/p&gt; &lt;p&gt;Here is our first simple query:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;query { targetNodes { name nodeType labels target { alias serviceUri } } }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This asks Cryostat for the &lt;code&gt;targetNodes&lt;/code&gt; result, which is a query that returns all of the JVM targets that Cryostat is aware of. The response is an array of those objects. Since the query specifies various fields like &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;nodeType&lt;/code&gt;, the objects within the array contain only those fields.&lt;/p&gt; &lt;p&gt;Here is another, more advanced query:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;query { environmentNodes(filter: { name: "my-app-pod-abcd1234" }) { descendantTargets { doStartRecording(recording: { name: "myrecording", template: "Profiling", templateType: "TARGET", duration: 30 }) { name state } } } }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This asks Cryostat for the &lt;code&gt;environmentNodes&lt;/code&gt; result, which is a query that returns all of the nodes in the deployment graph that Cryostat is aware of but that are &lt;em&gt;not&lt;/em&gt; JVM target applications. In a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; context, results would include the &lt;code&gt;Deployments&lt;/code&gt; or &lt;code&gt;DeploymentConfigs&lt;/code&gt;, for example, and the &lt;code&gt;Pods&lt;/code&gt; that belong to them.&lt;/p&gt; &lt;p&gt;The previous query used a name-based filter, selecting only the nodes with the name &lt;code&gt;my-app-pod-abcd1234&lt;/code&gt;. Let's assume that this string matches one pod within our project namespace. Upon that pod, the query performs the nested query &lt;code&gt;descendantTargets&lt;/code&gt;, which yields an array of JVM target objects much like our previous example query.&lt;/p&gt; &lt;p&gt;Upon each of those JVM targets, we perform the &lt;code&gt;doStartRecording&lt;/code&gt; mutation. As its name suggests, this mutation causes Cryostat to start a new JDK Flight Recording on each of the JVM targets using the configuration provided. The response will contain simply the name and state of each of these recordings, which we can anticipate to be &lt;code&gt;myrecording&lt;/code&gt; and &lt;code&gt;RUNNING&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The previous example shows the power and utility of the GraphQL API, where a single request can make Cryostat perform the complex action of communicating with OpenShift to determine all of the Cryostat-compatible JVMs that belong to the specific pod and start a recording on each JVM.&lt;/p&gt; &lt;p&gt;Here is one final sample query:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;query { targetNodes(filter: { annotations: "PORT = 9093" }) { recordings { active(filter: { labels: "mylabel = redhatdevelopers" }) { doArchive { name } } } } }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If you are familiar with OpenShift, the &lt;code&gt;labels&lt;/code&gt; filter here might look familiar to you. This label selector uses the same syntax as OpenShift's label selectors, and applies to Cryostat's &lt;a href="https://developers.redhat.com/articles/2022/05/12/how-organize-jfr-data-recording-labels-cryostat-21"&gt;recording labels&lt;/a&gt; as well as to the labels and annotations in the deployment graph.&lt;/p&gt; &lt;p&gt;You can use expressions like &lt;code&gt;mylabel = redhatdevelopers&lt;/code&gt;, &lt;code&gt;env in (prod, stage)&lt;/code&gt;, or &lt;code&gt;!mylabel&lt;/code&gt;. You can also create a logical AND conjunction of multiple label selectors by passing them as an array in the filter:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-graphql"&gt;active(filter: { labels: ["mylabel = redhatdevelopers", "env in (prod, stage)"] })&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;To better understand what Cryostat sees in your deployment graph for your particular namespace, check the &lt;a href="https://github.com/cryostatio/cryostat/blob/cryostat-v2.1/HTTP_API.md#DiscoveryGetHandler"&gt;Discover API endpoint&lt;/a&gt; with a command like:&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl https://my-cryostat.openshift.example.com/api/v2.1/discovery | jq&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;There are several different kinds of filters, which can be combined and applied to various nested queries. So a large number of possible semantic requests can be pieced together. Refer to Cryostat's GraphQL &lt;a href="https://github.com/cryostatio/cryostat/blob/cryostat-v2.1/src/main/resources/queries.graphqls"&gt;queries.graphqls&lt;/a&gt; and &lt;a href="https://github.com/cryostatio/cryostat/blob/cryostat-v2.1/src/main/resources/types.graphqls"&gt;types.graphqls&lt;/a&gt; repositories to see all of the implemented queries and mutations, and learn which ones accept which kinds of filters. The names should be self-explanatory: Types ending in &lt;code&gt;FilterInput&lt;/code&gt; are filters, and type fields starting with &lt;code&gt;do&lt;/code&gt; are actually nested mutations that perform some action like starting a recording or copying a recording to the archives.&lt;/p&gt; &lt;h2&gt;API endpoint details&lt;/h2&gt; &lt;p&gt;The examples in this article show typical query payloads of a GraphQL endpoint, the behaviors that you can perform, and what the expected responses should be. In this section, we take a closer look at the specific details of how to make these requests.&lt;/p&gt; &lt;p&gt;The primary GraphQL endpoint for Cryostat 2.1 is &lt;code&gt;POST /api/beta/graphql&lt;/code&gt;. The &lt;code&gt;POST&lt;/code&gt; requests sent to this path include a body string formatted like &lt;code&gt;{ query: "THE_QUERY" }&lt;/code&gt;, replacing &lt;code&gt;THE_QUERY&lt;/code&gt; with the text of the query as given in the previous section's examples. Here's a concrete example of a request and its metadata:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;POST /api/beta/graphql HTTP/1.1 Accept: application/json, */*;q=0.5 Accept-Encoding: gzip, deflate Connection: keep-alive Content-Length: 171 Content-Type: application/json Host: localhost:8181 User-Agent: HTTPie/3.1.0 { "query": "query {\n targetNodes {\n name\n nodeType\n labels\n target {\n alias\n serviceUri\n }\n }\n}\n" }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You can also use a &lt;code&gt;GET /api/beta/graphql&lt;/code&gt; request with the same results:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;"/api/beta/graphql?query={targetNodes{name nodeType labels target{alias serviceUri}}}" HTTP/1.1 200 OK content-encoding: gzip content-length: 247 content-type: application/json&lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Taking GraphQL further&lt;/h2&gt; &lt;p&gt;In the previous section's examples, we explored some possible query filters. To play with the power of the GraphQL interface in the context of your environment, try the following exercise:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Craft a GraphQL query that snapshots all of your JVM applications at once and copies the resulting recordings into the Cryostat archives.&lt;/li&gt; &lt;li&gt;Execute your query by sending the request to Cryostat using cURL or your favorite HTTP client.&lt;/li&gt; &lt;li&gt;Write a script wrapping around this API request and add it to your crontab to run at 5:00 PM every day.&lt;/li&gt; &lt;li&gt;Combine this script with a &lt;a href="https://developers.redhat.com/articles/2022/05/11/how-build-automated-jfr-rules-cryostat-21s-new-ui"&gt;Cryostat automated rule&lt;/a&gt; that automatically starts a new continuous monitoring recording on your JVM applications whenever they appear.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Each of these pieces is simple in isolation, but together they form a powerful automation workflow.&lt;/p&gt; &lt;p&gt;One final note: The standard API in older Cryostat versions still exists in 2.1, and there are even further additions to this API for actions and capabilities that are not suitable for GraphQL—new endpoints for downloading JDK Flight Recorder files using JWT tokens rather than Authorization headers.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/17/manage-jfr-across-instances-cryostat-and-graphql" title="Manage JFR across instances with Cryostat and GraphQL"&gt;Manage JFR across instances with Cryostat and GraphQL&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Andrew Azores</dc:creator><dc:date>2022-05-17T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 2.9.1.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-2-9-1-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-9-1-final-released/</id><updated>2022-05-17T00:00:00Z</updated><content type="html">It is my pleasure to announce the availability of Quarkus 2.9.1.Final, the first maintenance release of our 2.9 release. It is a safe upgrade for anyone already using 2.9. If you are not using 2.8 already, please refer to the 2.9 migration guide. Full changelog You can get the full...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title type="html">DMN Types from Java Classes</title><link rel="alternate" href="https://blog.kie.org/2022/05/dmn-types-from-java-classes.html" /><author><name>Yeser Amer</name></author><id>https://blog.kie.org/2022/05/dmn-types-from-java-classes.html</id><updated>2022-05-16T18:48:05Z</updated><content type="html">Drawing a real case DMN asset may become a time-consuming activity. In some domains, the possible types involved in DMN logic can explode into dozens or even hundreds of possible involved objects. Although a well-designed UI can support users to define your domain object type in a simpler and faster way possible, other alternative strategies are a practical choice for the designer, in cases where a large domain is in place. This is one of the reasons, we designed and implemented a new feature in the DMN Editor: Importing Java Beans and translating them as DMN Type. Suppose you are a regular reader of the KIE Blog. In that case, you should already know our tooling editors offer developer-centric features, which place the developer in the center of the feature experience. In a few words, the scope of this goal relies on task automation, testing management, and reuse. The Import of Java Classes feature is one of these kinds of functionalities, and in detail: * Reducing time to define your DMN assets: A developer can write hundred of Java classes faster than defining them in the UI; * Domain usage: In some cases, your domain can be already available and well defined with Java Classes * Class management and Type safety: The enablement of the feature, the actual classes you can import into your project, and the type safety of your assets are manageable aspects the user can rely on. In this post, we will guide you in using this new feature! TUTORIAL Here are the required elements for this guide: * (1.46.0+); * (1.22.0+) *   or (0.19.0+); * (1.6.0+); * Your domain’s Java Beans; * The Activator, a Java class file required to activate the functionality; However, I strongly suggest using the latest versions of the above elements. PROJECT SETUP Let’s start laying the foundation of our project. Let’s consider a Kogito-based project present in our . Our pick-up is the , which already contains DMN assets. So, clone that module in your local. The next step is to add some Java Bean classes, which describe your own domain. In our case, we will use Book and Author classes. Our dmn-quarkus-example project An important point to highlight is your Java Classes must be Java Beans and, in detail, they must have public getters of their internal fields. This means if a field of your Java Class is private or without a public getter method, our functionality will be not able to find that field. An alternative is to set that field with the public identifier, but we strongly recommend referring to the Java Beans specification to use this feature. The second point to analyze is the required Activator Class (no matter the class name). This is a class that contains a annotation, and it’s essential for the correct behavior of the functionality. This annotation is available in the latest version of Kogito, that’s the reason we chose (and recommend) to use a kogito-based project. In case you need to directly import the annotation, just import in your maven project the kie-api dependency, with at least version 8.22.0.Beta. The Activator class The Activator class must be composed exactly in that way, an empty Java Class with @KieActivator annotation. Any additional code can affect negatively the feature behavior. Why are these requisites mandatory? Let’s go deeper to see how it works under the hood. THE JAVA COMPLETION EXTENSION In the last months, we designed and implemented an extension of VSCode plugin. As you can imagine, the main aim of that plugin is to provide language support for Java class files in VSCode. It provides, for instance, the Code completion feature when a user starts to type a Class in the editor. Our extension leverages that plugin and that Code Completion feature to retrieve Java Classes names and field names and eventually translated them to DMN types. Java Completion Extention Architecture In particular, the extension is composed by: * An API Module: There are the entry points of our extension. Currently, the API offers two methods: * getClasses(query: String): Which returns a list of class names that match with a given query term. (eg. Given “Boo” as a query parameter, a possible return list is ["com.Book", "com.Boom", "com.Boomerang", .. ] ) * getAccessors(className: String): Which returns the fields of a given class (eg. Given “Book” as a className parameter, a possible return list is [isAvaialble: boolean, author: com.Author, ...] ) * The Activator Java Class file: This Java class is the place where we simulate a Code Completion request to be served by the Language Support for Java(TM) by Red Hat. For example, in the case of a getClasses("Book") request, the activator will be dynamically filled according to this template: Template used to simulate a Code Completion request Considering our example, that template is filled with the parameter shown in the below screenshot: The template filled with our example parameters As you noticed, the Code Completion correctly suggests the Book class, and this represents the output of the API call too. Please note that this logic doesn’t actually modify the user-defined Activator class file, but it simply sends a request using using the , which is implemented by the . IMPORTING YOUR JAVA CLASSES At this point, we are ready to Import our Java classes as DMN Type, let’s start! Open your DMN model, in our case our reference is the TrafficViolation.dmn file already available in the example project. Go to the "Data Types" tab. Here, you should notice a new button, Import Java Classes. Be aware that this button is enabled only if the previously described requirements are satisfied. Import Java classes button in Data Types section After pressing the button, a pop will appear. Here, you can start to type the name of the class you need to import as DMN type. Let’s type "Book". Selecting Book class As a result of the query (and of the Code Completion behind the scenes), you should see the com.Book class as the query result. Select it and press the "Next" button. Managing the fields of the Book class In this step, you should expect to see all the fields of the com.Book class, with their own DMN type (eg. Book is a structure, name a string, numberOfCopies a number) Let’s pay attention to the first field, author. That field type is another user-defined Java class, com.Author. If you want to import that class too, just press the "Fetch "Author" class". Importing both Author and Book classes As a result, both com.Author and com.Book classes with their own fields are present in the list. Note that author field’s type is now Author. Time to land on the last step, the review one. Here, we should simply review if the selected classes are consistent with our expectations. We are definitely ready to import these DMN Type, so press the "Import" straightaway! Java classes imported as DMN Types And voila! com.Author and com.Book Java classes have been translated to Author and Book DMN types, ready to be used in your DMN logic. CONCLUSION In this article, we showed how to use our new "Import Java Classes" feature to import your domain Java classes as DMN Types, a useful alternative to importing your domain types. Thank you for reading! The post appeared first on .</content><dc:creator>Yeser Amer</dc:creator></entry><entry><title>How to use Operators with AWS Controllers for Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes" /><author><name>August Simonelli</name></author><id>481f182a-39a9-486b-9e0c-8425aec05234</id><updated>2022-05-16T07:00:00Z</updated><published>2022-05-16T07:00:00Z</published><summary type="html">&lt;p&gt;This is the first of two articles that show how to simplify the management of services offered for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; by Amazon Web Services (AWS), through the use of &lt;a href="https://gallery.ecr.aws/aws-controllers-k8s"&gt;Amazon's AWS Controllers for Kubernetes&lt;/a&gt; (ACK). You'll also learn how to use an &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"&gt;Operator&lt;/a&gt; to simplify installation further on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; clusters. Together, these tools provide standardized and familiar interfaces to AWS services from a Kubernetes environment.&lt;/p&gt; &lt;p&gt;This first article lays out the reasons for using controllers and Operators, and sets up your environment for their use. A subsequent article will show how to install AWS services and how to use them within OpenShift. The ideas behind these articles, and a demo showing their steps, appear in my video &lt;a href="https://www.youtube.com/watch?v=MEKTnbeXv2Y"&gt;Using AWS Controllers for Kubernetes (ACK) with Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;So many services, so little time&lt;/h2&gt; &lt;p&gt;If you deploy any kind of code on AWS, you know that the platform offers plenty of services for your applications to consume. Perhaps you use AWS services to support your application's infrastructure—to create registries with Amazon's Elastic Container Registry (ECR) or compute instances with EC2, for instance. Or maybe you're integrating AWS services directly into your development pipeline, deploying an RDS database on the back end, or building, training, and deploying &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; (ML) models with Amazon SageMaker.&lt;/p&gt; &lt;p&gt;Whatever you are doing, AWS probably has a service to make your work easier, your application better, and your use of time more efficient.&lt;/p&gt; &lt;p&gt;But with so many services, how do you integrate them easily into your code? Do you write complex CloudFormations scripts to call from your &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; workflows? Are you more old school and like to write wrappers in a familiar language? Or maybe it's all about APIs for you? One thing is for sure: There are a lot of options (and a lot of hoops to jump through).&lt;/p&gt; &lt;p&gt;And what if you're running Kubernetes in AWS—maybe on EKS, maybe on OpenShift, or maybe you're just rolling your own? How do you stay within the efficient, familiar, and friendly framework of coding for Kubernetes and still access AWS services without coming up with complex, confusing, and hard-to-maintain workarounds that break your workflow?&lt;/p&gt; &lt;h2&gt;It's easy: You ACK it&lt;/h2&gt; &lt;p&gt;With the growth of Kubernetes for mission-critical production workloads, AWS is a match made in heaven for Kubernetes developers. With so many resilient services, Kubernetes in AWS is a smorgasbord of functionality to improve, scale, and prepare your app for anything. Bring on Black Friday sales and Click Frenzy shopping days—Kubernetes in AWS has cloud-native efficiencies and AWS resiliency built-in.&lt;/p&gt; &lt;p&gt;And now, with AWS Controllers for Kubernetes (ACK), you can easily define and use AWS resources directly from Kubernetes. ACK allows Kubernetes users to define AWS resources using the Kubernetes API. This means you can declaratively define and create an AWS RDS database, S3 bucket, or many other resources, using the same workflow as the rest of your code. There's no need to break out and learn AWS-specific languages or processes. Instead, when an ACK controller is installed, you can use the Kubernetes API to instruct the controller to interact with the AWS service. As a bonus, Kubernetes continues to manage the service for you.&lt;/p&gt; &lt;p&gt;To enable these rich possibilities, each ACK instance is a unique Docker image available in the &lt;a href="https://gallery.ecr.aws/aws-controllers-k8s"&gt;ACK public gallery&lt;/a&gt;. An image is combined with a custom resource definition (CRD), allowing you to easily request custom resources (CR) to define the service and use it within your project. Additionally, integration with AWS's Identity and Access Management (IAM) ensures that all steps and interactions are secure and that role-based access control (RBAC) is managed transparently.&lt;/p&gt; &lt;p&gt;To understand how the controllers are built, including the generation of the artifacts, and to learn about the history of the ACK project, see the AWS blog post &lt;a href="https://aws.amazon.com/blogs/containers/aws-controllers-for-kubernetes-ack/"&gt;Introducing the AWS Controllers for Kubernetes (ACK)&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Operator, Operator, could you be so kind&lt;/h2&gt; &lt;p&gt;Although the default distribution method for ACK employs &lt;a href="https://helm.sh"&gt;Helm charts&lt;/a&gt;, a popular deployment tool on Kubernetes, we'll look at a simpler way to manage controllers through an Operator. Operators are a sleek aid to deployment and life cycle management for Kubernetes services. Publicly available Operators can be downloaded from &lt;a href="https://operatorhub.io"&gt;OperatorHub&lt;/a&gt;, a project started by Red Hat but used by many communities.&lt;/p&gt; &lt;p&gt;Operators, along with the &lt;a href="https://operatorframework.io"&gt;Operator Framework&lt;/a&gt;, make it super easy to install, manage, and maintain Kubernetes resources and applications across OpenShift clusters. The Operator Framework is an open source toolkit designed to manage Operators in an effective, automated, and scalable way.&lt;/p&gt; &lt;p&gt;So it's a no-brainer that we at Red Hat have worked hard to ensure that installing ACK service controllers with Operators is easy. We work closely with AWS engineers to ensure nothing is lost in the process. See &lt;a href="https://cloud.redhat.com/blog/attention-developers-you-can-now-easily-integrate-aws-services-with-your-applications-on-openshift"&gt;Attention developers: You can now easily integrate AWS services with your applications on OpenShift&lt;/a&gt;, a post on the Red Hat Cloud blog, for more details about that collaboration.&lt;/p&gt; &lt;h2&gt;Setup in AWS and Kubernetes&lt;/h2&gt; &lt;p&gt;Before installing a controller via OperatorHub, a cluster administrator needs only to carry out a few simple pre-installation steps in AWS to provide the controller credentials and authentication context for interacting with the AWS API.&lt;/p&gt; &lt;p&gt;Let's take a look at that process now. I'm using an OpenShift installation in AWS provided by &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/aws"&gt;Red Hat OpenShift Service on AWS&lt;/a&gt;, but you can use the Operators on any OpenShift cluster running on AWS with the &lt;a href="https://olm.operatorframework.io/"&gt;Operator Lifecycle Manager (OLM)&lt;/a&gt; installed.&lt;/p&gt; &lt;p&gt;Let's keep our installation tidy and create a namespace for our controllers. This is the namespace the Operators will expect to find when you install from OperatorHub, so it's best not to change the name after choosing it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project ack-system Now using project "ack-system" on server "https://api.rosatest.c63c.p1.openshiftapps.com:6443". You can add applications to this project with the 'new-app' command. For example, try: oc new-app rails-postgresql-example to build a new example application in Ruby. Or use kubectl to deploy a simple Kubernetes application: kubectl create deployment hello-node --image=k8s.gcr.io/serve_hostname&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we need a user in IAM that can own the controllers and be our service account. We are going to attach our security principals to this account. To maintain clear lines between services, you could create a user for each of your controller Operators: one user for your S3 interactions, another for your RDS interactions, and so on. But to keep things simple for this example, we'll use the same user for all controllers:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-ini"&gt;$ aws iam create-user --user-name ack-user { "User": { "Path": "/", "UserName": "ack-user", "UserId": "AIDARDQA3BHOTGU5KGN24", "Arn": "arn:aws:iam::1234567890:user/ack-user", "CreateDate": "2022-03-24T01:24:03+00:00" } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, we need an access key ID and secret access key for this user. Record the &lt;code&gt;AccessKeyId&lt;/code&gt; and &lt;code&gt;SecretAccessKey&lt;/code&gt; strings generated by the following command, because you are going to store them in Kubernetes. Storing the keys allows the controllers to interact programmatically with the AWS resources. We will use these keys in a moment, but make sure to write them down and protect them (don't worry about my security, because my keys have been deleted):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws iam create-access-key --user-name ack-user { "AccessKey": { "UserName": "ack-user", "AccessKeyId": "AKIARDQA3BHOVF4ZSHNW", "Status": "Active", "SecretAccessKey": "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", "CreateDate": "2022-03-24T01:24:24+00:00" } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to attach the correct AWS policy for each service to the principal (user) you just created. This procedure grants the user control over the specific, correct, AWS resources.&lt;/p&gt; &lt;p&gt;Grant access by assigning a policy's Amazon Resource Name (ARN) to the user directly. As mentioned, you could attach policies in a one-for-one relationship to unique users (e.g., an EC2 ARN to the &lt;code&gt;ack-ec2-user&lt;/code&gt; user), or all policies to a single user. For our example, you are going to attach all our policies to the same user (&lt;code&gt;ack-user&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;Recommended policy ARNs are provided in each service controller's GitHub repository in the &lt;code&gt;config/iam/&lt;/code&gt; directory. For instance, the EC2 controller's policy is in the following file on GitHub:&lt;/p&gt; &lt;pre&gt; https://github.com/aws-controllers-k8s/ec2-controller/blob/main/config/iam/recommended-policy-arn&lt;/pre&gt; &lt;p&gt;OK, let's connect some controllers' policies to our &lt;code&gt;ack-user&lt;/code&gt; service account:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws iam attach-user-policy \ --user-name ack-user \ --policy-arn 'arn:aws:iam::aws:policy/AmazonS3FullAccess' $ aws iam attach-user-policy \ --user-name ack-user \ --policy-arn 'arn:aws:iam::aws:policy/AmazonEC2FullAccess'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These commands grant our &lt;code&gt;ack-user&lt;/code&gt; IAM user access to AWS S3 and EC2. If you want to install additional ACK operators for other AWS services, you need to enable those policies, too.&lt;/p&gt; &lt;p&gt;Next, you need to present the &lt;code&gt;ack-user&lt;/code&gt; secure credentials in a way that can be safely passed to the controller, so it will be allowed to make changes to the AWS service for which it is responsible. These credentials are a Kubernetes Secret and a ConfigMap, defined with some specific information. The naming of these assets is intentional and cannot be varied, or the Operator will not be able to find the credentials.&lt;/p&gt; &lt;p&gt;Place the &lt;code&gt;AccessKeyId&lt;/code&gt; and &lt;code&gt;SecretAccessKey&lt;/code&gt; values you recorded early in a file called &lt;code&gt;secrets.txt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;AWS_ACCESS_KEY_ID=AKIARDQA3BHOVF4ZSHNW AWS_SECRET_ACCESS_KEY=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&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a secret with these keys in the &lt;code&gt;ack-system&lt;/code&gt; namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create secret generic \ --namespace ack-system \ --from-env-file=secrets.txt ack-user-secrets secret/ack-user-secrets created&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You also need to set some environment variables for the controllers. Do this by creating a ConfigMap in the &lt;code&gt;ack-system&lt;/code&gt; namespace. Add the following to a file called &lt;code&gt;config.txt&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;ACK_ENABLE_DEVELOPMENT_LOGGING=true ACK_LOG_LEVEL=debug ACK_WATCH_NAMESPACE= AWS_REGION=ap-southeast-2 ACK_RESOURCE_TAGS=acktagged AWS_ENDPOINT_URL=&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll need to adjust the values to suit your own environment. Most of the values should be obvious, but it's important to keep &lt;code&gt;ACK_WATCH_NAMESPACE&lt;/code&gt; blank so that the controller can properly watch all namespaces. Additionally, you should not rename these variables, as the operators are preconfigured to use them as they are here.&lt;/p&gt; &lt;p&gt;Create the ConfigMap:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc create configmap \ --namespace ack-system \ --from-env-file=config.txt ack-user-config configmap/ack-user-config created&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Where to now?&lt;/h2&gt; &lt;p&gt;This article has you set up to use as many AWS services as you want through ACK and OperatorHub. The second article in the series will install EC2 and S3 as examples, and perform an S3 operation from Kubernetes.&lt;/p&gt; &lt;h2&gt;Share your experiences&lt;/h2&gt; &lt;p&gt;If you'd like to help, learn more, or just connect in general, head on over to the &lt;a href="http://kubernetes.slack.com"&gt;Kubernetes Slack channel&lt;/a&gt; and join us in #provider-aws to say hello to the AWS and Red Hat engineers creating the code, various ACK users, and even the occasional blog post author.&lt;/p&gt; &lt;p&gt;We're looking for more good examples of complex deployments created in AWS via ACK. If you've got a deployment you think would be made easier with ACK, or one you've already made better, let us know on the Slack channel or in the comments section below. We might showcase your work in some upcoming articles or videos.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes" title="How to use Operators with AWS Controllers for Kubernetes"&gt;How to use Operators with AWS Controllers for Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>August Simonelli</dc:creator><dc:date>2022-05-16T07:00:00Z</dc:date></entry><entry><title type="html">Jakarta EE 10 is on its way with WildFly 27</title><link rel="alternate" href="http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-ee-10-is-on-its-way-with-wildfly-27/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java-ee/jakarta-ee/jakarta-ee-10-is-on-its-way-with-wildfly-27/</id><updated>2022-05-16T06:49:09Z</updated><content type="html">WildFly 27 (Alpha) is now available. On the the top features of this releases is the preview support for some of Jakarta EE 10 features plus several product enhancement. This article launches you on a tour of this new release by focusing on fundamentals. Jakarta EE 10 status in WildFly WildFly 27 Alpha is the ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to organize JFR data with recording labels in Cryostat 2.1</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/12/how-organize-jfr-data-recording-labels-cryostat-21" /><author><name>Janelle Law</name></author><id>0f799239-8a34-4447-9021-f1f0b8b2be25</id><updated>2022-05-12T07:00:00Z</updated><published>2022-05-12T07:00:00Z</published><summary type="html">&lt;p&gt;With the tech preview release of &lt;a href="https://cryostat.io"&gt;Cryostat&lt;/a&gt; 2.1, users can attach metadata or custom labels to &lt;a href="https://developers.redhat.com/blog/2020/08/25/get-started-with-jdk-flight-recorder-in-openjdk-8u#"&gt;JDK flight recordings&lt;/a&gt; that monitor &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized&lt;/a&gt; &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; applications, and can manage those recordings using Cryostat. Recording labels can identify recordings in queries and perform actions on multiple recordings containing the same label. This article shows how to add and edit metadata labels on your JDK flight recordings, including the recordings managed through &lt;a href="https://developers.redhat.com/articles/2021/11/09/automating-jdk-flight-recorder-containers"&gt;automated rules&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Add a recording label to a JDK flight recording&lt;/h2&gt; &lt;p&gt;Navigate to the &lt;strong&gt;Recordings&lt;/strong&gt; tab on the Cryostat console and select a target JVM from the drop-down menu. Click &lt;strong&gt;Create&lt;/strong&gt; to create a custom flight recording (Figure 1). When creating the flight recording, expand the &lt;strong&gt;Show metadata options&lt;/strong&gt; form section. Click &lt;strong&gt;Add Label&lt;/strong&gt; to add a key-value label pair to the recording. For more details about creating recordings, see the &lt;a href="https://cryostat.io/guides/#startstop-a-recording"&gt;Cryostat guides&lt;/a&gt;.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/metadata-label-1_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/metadata-label-1_0.png?itok=ETHOR8ws" width="1440" height="823" alt="A custom flight recording can be tracked through custom labels." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A custom flight recording can be tracked through custom labels. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The new recording will appear in the &lt;strong&gt;Recordings&lt;/strong&gt; tab with your custom label, along with default labels containing information about the selected recording template, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/metadata-label-2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/metadata-label-2.png?itok=Td6iPTIA" width="1440" height="728" alt="The Active Recordings table shows the labels on each recording." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The Active Recordings table shows the labels on each recording. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Edit recording labels for a JDK flight recording&lt;/h2&gt; &lt;p&gt;Recording labels can also be edited after recordings have been created or re-uploaded to archives. For instance, it looks like the custom label in our example contains a typo. You can fix it by editing the label.&lt;/p&gt; &lt;p&gt;Click the ellipsis menu beside the recording table entry, then click &lt;strong&gt;Edit Metadata&lt;/strong&gt;. The labels section will appear as a form where you can add, edit, or delete existing labels. Click &lt;strong&gt;Save&lt;/strong&gt; to save your edited labels, as shown in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/metadata-label-3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/metadata-label-3.png?itok=-FF9scuN" width="1440" height="728" alt="The value of a label has been edited." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The value of a label has been edited. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The recording labels should now be updated in the &lt;strong&gt;Active Recordings&lt;/strong&gt; table, as shown in Figure 4.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/metadata-label-4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/metadata-label-4.png?itok=Sw62SIU2" width="1440" height="728" alt="The Active Recordings table shows the updated labels." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Active Recordings table shows the updated labels. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;On the &lt;strong&gt;Active Recordings&lt;/strong&gt; table, click the checkbox next to the recording, then click &lt;strong&gt;Archive&lt;/strong&gt; to archive your recording.&lt;/p&gt; &lt;p&gt;The archived recording also copies the labels from the active recording, as shown in Figure 5. This means you can easily find the archived recordings associated with an active recording by searching for the metadata labels applied to the active recording. You can also add additional labels to any recording uploaded to the Cryostat archives. The labels are preserved for as long as the recording remains in the archives.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/metadata-label-5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/metadata-label-5.png?itok=1EBW7Bbr" width="1440" height="728" alt="Labels are also shown for each recording in the Archived Recordings tab." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Labels are also shown for each recording in the Archived Recordings tab. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Labeling recordings generated with automated rules&lt;/h2&gt; &lt;p&gt;Automated rules, introduced in the article &lt;a href="https://developers.redhat.com/articles/2021/11/09/automating-jdk-flight-recorder-containers"&gt;Automating JDK Flight Recorder in containers&lt;/a&gt;, simplify the tracking and management of multiple JDK flight recordings. An automated rule automatically applies a metadata label to indicate the name of the automated rule that created that recording. For example, if your automated rule was named &lt;code&gt;my_automated_rule&lt;/code&gt;, all recordings generated with that rule will contain the metadata label &lt;code&gt;Rule: my_automated_rule&lt;/code&gt;. When these recordings are archived, the labels are preserved to help you easily locate all of your recordings generated by &lt;code&gt;my_automated_rule&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has explained how to add and edit recording metadata labels for a single JDK flight recording, or for multiple recordings if they are managed by an automated rule. Automated rules also apply metadata labels to each recording they create.&lt;/p&gt; &lt;p&gt;In a future article in this series, you will learn to search for and operate on recordings using these labels. For more information about Cryostat, visit the project's &lt;a href="https://cryostat.io/get-started/"&gt;getting started page&lt;/a&gt;. For questions, comments and feedback, feel free to connect with us on &lt;a href="https://github.com/cryostatio"&gt;GitHub&lt;/a&gt; or join our &lt;a href="https://groups.google.com/g/cryostat-development"&gt;mailing list&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/12/how-organize-jfr-data-recording-labels-cryostat-21" title="How to organize JFR data with recording labels in Cryostat 2.1"&gt;How to organize JFR data with recording labels in Cryostat 2.1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Janelle Law</dc:creator><dc:date>2022-05-12T07:00:00Z</dc:date></entry><entry><title type="html">WildFly Preview 27 Alpha1 is released</title><link rel="alternate" href="https://wildfly.org//news/2022/05/12/WildFlyPreview27-Alpha1-Released/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2022/05/12/WildFlyPreview27-Alpha1-Released/</id><updated>2022-05-12T00:00:00Z</updated><content type="html">Today we have released a 27.0.0.Alpha1 version of WildFly Preview. This release serves as a milestone on our way toward support for Jakarta EE 10 in WildFly Preview, and eventually in standard WildFly. As discussed in my January , the main focus of the WildFly developers as we work on WildFly 27 is implementing Jakarta EE 10 support. That work has now reached a point in WildFly Preview where it’s useful to evaluate WildFly Preview as a compatible implementation of the new . In order to help with bringing the EE 10 Core Profile specification to completion, we’ve released WildFly Preview 27.0.0.Alpha1. Note that we are not adding 27.0.0.Alpha1 binaries for standard WildFly to . Standard WildFly is still built with the Jakarta EE 8 APIs, but we plan to switch to EE 10 in our main branch soon, and we will not be doing a standard WildFly 27 Final release that supports EE 8. So, there is not much purpose looking at 27.0.0.Alpha1 for standard WildFly. (Its binaries can be found in the .) We’re also not releasing quickstarts or cloud images for this release. WHAT’S NEW? A couple of noteworthy items in this release are support for CDI 4.0 (via Weld 5), including CDI Lite, along with the much-asked-for transition from Hibernate 5.3 to Hibernate 6. The full list of issues resolved is available . JAVA SE SUPPORT You can run 27.0.0.Alpha1 on Java SE 11 or Java SE 17. The WildFly project no longer supports Java SE 8 in our feature releases, although our planned 26.1.1 and 26.1.2 bug fix releases will support SE 8. STANDARDS SUPPORT The 27.0.0.Alpha1 release is not a compatible implementation of Jakarta EE 8 or 9.1, nor is it a compatible implementation of MicroProfile Platform 4.1 or 5. Strict specification compliance wasn’t a focus of this alpha release, other than a desire to be compatible with the EE 10 Core Profile once that specification is released. UPCOMING CHANGES As discussed in my January , WildFly 26.1 was the last WildFly feature release that will support Java SE 8, Jakarta EE 8 and MicroProfile 4.1, while WildFly Preview 26.1 was the last release that will support Jakarta EE 9.1. The WildFly 27 release will require Java SE 11 or higher and will support Jakarta EE 10 and MicroProfile APIs based on the jakarta.* package namespace. We plan to do a WildFly 26.1.1 bug fix release in May. Something different from previous releases is we also intend to do a WildFly 26.1.2 bug fix release in the July-August time frame. The aim of that release will be to deliver any critical fixes we’ve discovered, particularly security related items. We recognize that moving on from SE 8 and EE 8 may be a substantial task for many of our users, so we want to help ease that transition by providing an extra bug fix release. ENJOY! Thank you for your continued support of WildFly. We’d love to hear your feedback at the .</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title type="html">How to add users using file-based strategy in PAM/DM 7.12</title><link rel="alternate" href="https://blog.kie.org/2022/05/users-file-strategy-rhpam.html" /><author><name>Archana Krishnan</name></author><id>https://blog.kie.org/2022/05/users-file-strategy-rhpam.html</id><updated>2022-05-11T16:20:25Z</updated><content type="html">Issue Identified: Custom Users/Roles not created in RHPAM 7.12.1/EAP 7.4.1. Sample of invalid user.xml: &lt;?xml version="1.0" ?&gt; &lt;identity xmlns="urn:elytron:1.0"&gt; &lt;attributes&gt; &lt;name="roles" value="kie-server"&gt;&lt;/attribute&gt; &lt;attribute name="roles" value="rest-all"&gt;&lt;/attribute&gt; &lt;attribute name="roles" value="admin"&gt;&lt;/attribute&gt; &lt;attribute name="roles" value="kiemgmt"&gt;&lt;/attribute&gt; &lt;attribute name="roles" value="Administrators"&gt;&lt;/attribute&gt; &lt;attribute name="roles" value="user"&gt;&lt;/attribute&gt; &lt;/attributes&gt;&lt;/identity&gt;$ Error in logs: 23:35:20,692 ERROR [org.jboss.as.controller.management-operation] (CLI command executor) WFLYCTL0013: Operation (“set-password”) failed – address: () – failure description: “WFLYCTL0216: Management resource ‘[ (\”subsystem\” =&gt; \”elytron\”), (\”filesystem-realm\” =&gt; \”ApplicationRealm\”) ]’ not found” The batch failed with the following error (you are remaining in the batch editing mode to have a chance to correct the error): WFLYCTL0062: Composite operation failed and was rolled back. Steps that failed: Step: step-11 Operation: /subsystem=elytron/filesystem-realm=ApplicationRealm:set-password(identity=pamAdmin, clear={password=’testAdmin’}) Failure: WFLYCTL0216: Management resource ‘‘ not found Warning in logs: 23:36:18,734 WARN [org.jboss.modules.define] (ServerService Thread Pool -- 86) Failed to define class org.jboss.resteasy.microprofile.config.ServletConfigSourceImpl in Module "org.jboss.resteasy.resteasy-jaxrs" version 3.15.1.Final-redhat-00001 from local module loader @21edd891 (finder: local module finder @de579ff (roots: /opt/eap/modules,/opt/eap/modules/system/layers/openshift,/opt/eap/modules/system/layers/base/.overlays/layer-base-jboss-eap-7.4.1.CP,/opt/eap/modules/system/layers/base,/opt/eap/modules/system/add-ons/keycloak)): java.lang.NoClassDefFoundError: Failed to link org/jboss/resteasy/microprofile/config/ServletConfigSourceImpl (Module "org.jboss.resteasy.resteasy-jaxrs" version 3.15.1.Final-redhat-00001 from local module loader @21edd891 (finder: local module finder @de579ff (roots: /opt/eap/modules,/opt/eap/modules/system/layers/openshift,/opt/eap/modules/system/layers/base/.overlays/layer-base-jboss-eap-7.4.1.CP,/opt/eap/modules/system/layers/base,/opt/eap/modules/system/add-ons/keycloak))): org/eclipse/microprofile/config/spi/ConfigSource at java.base/java.lang.ClassLoader.defineClass1(Native Method) Other errors if an invalid user/roles properties file is provided: sh-4.4$ /opt/eap/bin/elytron-tool.sh filesystem-realm --users-file /home/jboss/custom/application-users.properties --roles-file /home/jboss/custom/application-roles.properties --output-location /opt/eap/standalone/configuration/kie-fs-realm-users --filesystem-realm-name kie-fs-realmusers --debug WARNING: No roles were found for user WARNING: Roles were found for user , but user was not defined. WARNING: No roles were found for user Exception encountered executing the command: java.lang.IndexOutOfBoundsException at java.base/java.lang.Character.offsetByCodePoints(Character.java:8699) WARNING: No password was found for user WARNING: No roles were found for user WARNING: No roles were found for user Exception encountered executing the command: java.lang.IndexOutOfBoundsException SOLUTION The following steps will help resolve the above issues: * Patch RHPAM 7.12.1 with EAP 7.4.4 STEP 1/5: FROM registry.redhat.io/rhpam-7/rhpam-kieserver-rhel8:7.12.1-3 STEP 2/5: COPY jboss-eap-7.4.4-patch.zip /tmp/jboss-eap-7.4.4-patch.zip --&gt; Using cache f9926b6ad308871c77bf3f1e650104f1c64f249b487613e4181d8e1e9ca9cd07 --&gt; f9926b6ad30 STEP 3/5: USER root --&gt; Using cache 15639841591027c9db7a4056ea69b51252d72dac6a2704528533d5b0ce03496f --&gt; 15639841591 STEP 4/5: RUN $JBOSS_HOME/bin/jboss-cli.sh --command="patch apply /tmp/jboss-eap-7.4.4-patch.zip --override-modules" ; rm /tmp/jboss-eap-7.4.4-patch.zip { "outcome" : "success", "result" : {} } STEP 5/5: USER 185 COMMIT image-registry.openshift-image-registry.svc:5000/op2/rhpam-kieserver-rhel8-custom:7.12.1-test --&gt; 85398f6feb7 Successfully tagged image-registry.openshift-image-registry.svc:5000/op2/rhpam-kieserver-rhel8-custom:7.12.1-test 85398f6feb78e1485f53a2ee154d20d33b2b7457a13325cfc9a928c7a7592ce3 * Validate EAP version [jboss@4c610ade4e51 eap]$ ls JBossEULA.txt LICENSE.txt appclient bin docs domain jboss-modules.jar jolokia.jar migration modules standalone version.txt welcome-content [jboss@4c610ade4e51 eap]$ more version.txt Red Hat JBoss Enterprise Application Platform - Version 7.4.4.GA * Update the custom application-users.properties and application-roles.properties file to include Realm name: Sample application-users.properties: Sample application-roles.properties: * Command to update custom users/roles file through elytron-tool.sh echo "START - enable-users" /opt/eap/bin/elytron-tool.sh filesystem-realm --users-file /home/jboss/custom/application-users.properties --roles-file /home/jboss/custom/application-roles.properties --output-location /opt/kie/data/kie-fs-realm-users find /opt/kie/data/kie-fs-realm-users -name *.xml -exec sed -i 's/&lt;attribute name="roles"/&lt;attribute name="role"/g' {} \; echo "END - enable-users" * Expected user.xml generated in output-location (/opt/kie/data/kie-fs-realm-users): &lt;?xml version="1.0" ?&gt; &lt;identity xmlns="urn:elytron:1.0"&gt; &lt;credentials&gt; &lt;password algorithm="digest-md5" format="base64"&gt;Ag9pbnRlZ3JhdGlvblVzZXIQQXBwbGljYXRpb25SZWFsbSjAetOv+11Kg3GFrzK+r98&lt;/password&gt; &lt;/credentials&gt; &lt;attributes&gt; &lt;attribute name="role" value="kie-server"&gt;&lt;/attribute&gt; &lt;attribute name="role" value="rest-all"&gt;&lt;/attribute&gt; &lt;attribute name="role" value="admin"&gt;&lt;/attribute&gt; &lt;attribute name="role" value="kiemgmt"&gt;&lt;/attribute&gt; &lt;attribute name="role" value="Administrators"&gt;&lt;/attribute&gt; &lt;attribute name="role" value="user"&gt;&lt;/attribute&gt; &lt;/attributes&gt;&lt;/identity&gt;sh-4.4$ ROOT CAUSE RHPAM 7.12.1 paired with EAP 7.4.1 does not create a valid XML file for kie-fs-realm users/roles. Reference RedHat support case – The post appeared first on .</content><dc:creator>Archana Krishnan</dc:creator></entry></feed>
