<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>CodeReady Workspaces scales up, is now Red Hat OpenShift Dev Spaces</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/04/01/codeready-workspaces-scales-now-red-hat-openshift-dev-spaces" /><author><name>Mario Loriedo</name></author><id>f964d602-70d4-4bd8-b17d-364ea8b01374</id><updated>2022-04-01T18:00:00Z</updated><published>2022-04-01T18:00:00Z</published><summary type="html">&lt;p&gt;Red Hat will soon release a higher-performing, richer version of the &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;Red Hat CodeReady Workspaces&lt;/a&gt; development platform, along with a name change to Red Hat OpenShift Dev Spaces. The platform has been significantly upgraded by moving to a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; Operator. This article explains the major changes in this upgrade and their effects on administrators and users.&lt;/p&gt; &lt;p&gt;The team worked on this version for over a year and introduced the new platform as a tech preview after CodeReady Workspaces 2.10. The official name of this release is Red Hat OpenShift Dev Spaces 3.0.&lt;/p&gt; &lt;h2&gt;Changes to the workspace engine&lt;/h2&gt; &lt;p&gt;The core of CodeReady Workspaces used to be a Java REST web service named che-server, which provisioned pods and other objects to run development environments on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. With OpenShift Dev Spaces 3.0, we are introducing a new OpenShift Operator to replace the che-server: the DevWorkspace Operator (Figure 1). This is a big architectural change whose benefits we'll summarize in the following sections.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/operator.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/operator.png?itok=X3Q-d9o6" width="1440" height="625" alt="The interface between the user interface and the workspace is now a Kubernetes Operator instead of a Java REST service." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The engine between the user interface and the workspace is now a Kubernetes Operator instead of a Java REST service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Scalability and high availability&lt;/h3&gt; &lt;p&gt;The che-server could not scale horizontally: it wasn't possible to run two instances concurrently. The new engine is a Kubernetes controller, so it runs behind the kube-apiserver that is designed to scale horizontally. The data is persisted in an &lt;a href="https://etcd.io"&gt;etcd&lt;/a&gt; key-value store that is designed to be highly available.&lt;/p&gt; &lt;h3&gt;A universal API&lt;/h3&gt; &lt;p&gt;Workspaces are Kubernetes objects. Useful traits of this architecture include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Kubernetes clients such as kubectl or the OpenShift console can manage them.&lt;/li&gt; &lt;li&gt;They are secured via &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/"&gt;Kubernetes RBAC&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;They can be validated and protected by admission webhooks.&lt;/li&gt; &lt;li&gt;The devfile specification is automatically generated from the API.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;A simpler design&lt;/h3&gt; &lt;p&gt;The new workspace engine has a single responsibility, which is to manage workspace resources. The engine is decoupled from the developer's IDE and the server-side components of the OpenShift Dev Spaces service. Communication between components happens asynchronously using ConfigMaps and Secrets rather than a REST API.&lt;/p&gt; &lt;h3&gt;An opportunity for refactoring&lt;/h3&gt; &lt;p&gt;The new engine has been written from scratch, and we took the opportunity to address some of the main issues reported by our users:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Simpler configuration for the administrator&lt;/li&gt; &lt;li&gt;No hard dependency on Red Hat's single sign-on technology&lt;/li&gt; &lt;li&gt;Simpler network model and TLS certificate management with one unique gateway&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;What's changing?&lt;/h2&gt; &lt;p&gt;The switch to the DevWorkspace Operator introduces changes for existing users and administrators of CodeReady Workspaces. Here is a summary.&lt;/p&gt; &lt;h3&gt;Administration&lt;/h3&gt; &lt;p&gt;There is a new server-side component: the gateway (powered by the open source reverse proxy &lt;a href="https://traefik.io"&gt;Traefik&lt;/a&gt;) that handles users' connection to the IDEs. Several simplifications have been made to administration, along with new options:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;User workspaces can be managed with the &lt;code&gt;oc&lt;/code&gt; command-line interface (CLI) or via the OpenShift console.&lt;/li&gt; &lt;li&gt;CRW doesn't require Red Hat's SSO anymore and uses OpenShift &lt;a href="https://oauth.net"&gt;OAuth&lt;/a&gt; for authentication.&lt;/li&gt; &lt;li&gt;Deployment is simpler because there are fewer configuration options. For instance, the namespace strategy and the exposure strategy are set to "per-user" and "single-host" respectively, as we have observed that users want those settings.&lt;/li&gt; &lt;li&gt;The operator installation mode is "all namespaces."&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;User experience&lt;/h3&gt; &lt;p&gt;Our enhancements to the user experience include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;We now support version 2 of the devfile spec, which has some significant improvements in regard to parents and events. Version 1 devfiles also continue to be supported.&lt;/li&gt; &lt;li&gt;Devfiles are IDE agnostic and don't include plug-in definitions anymore. IDE-specific configurations are managed in separate files as extensions.json in &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;Visual Studio Code&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;We have added tech preview support for VisualStudio Code as an IDE (in addition to Eclipse Theia and JetBrains IDEs).&lt;/li&gt; &lt;li&gt;Workspaces load faster, with fewer containers per workspace.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;How to get OpenShift Dev Spaces&lt;/h2&gt; &lt;p&gt;We plan to release Red Hat OpenShift Dev Spaces at the end of April. It will support OpenShift 4.10. Installation can be done through the usual channels, via the OperatorHub or the command line.&lt;/p&gt; &lt;p&gt;Existing Red Hat CodeReady Workspaces customers won't be upgraded automatically to OpenShift Dev Spaces but will need to follow a manual procedure to upgrade their instance.&lt;/p&gt; &lt;h2&gt;Further reading&lt;/h2&gt; &lt;p&gt;To learn more, check out the following resource:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://devfile.io/docs/devfile/2.1.0/user-guide/migrating-to-devfile-v2/"&gt;Migrating to devfile v2&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Kudos to the whole Che team at Red Hat that worked hard on this challenging pivot.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/04/01/codeready-workspaces-scales-now-red-hat-openshift-dev-spaces" title="CodeReady Workspaces scales up, is now Red Hat OpenShift Dev Spaces"&gt;CodeReady Workspaces scales up, is now Red Hat OpenShift Dev Spaces&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mario Loriedo</dc:creator><dc:date>2022-04-01T18:00:00Z</dc:date></entry><entry><title>Bind workloads to services easily with the Service Binding Operator and Red Hat OpenShift</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/11/binding-workloads-services-made-easier-service-binding-operator-red-hat" /><author><name>Kartikey Mamgain</name></author><id>599c8903-5790-4727-ba2e-96251c2f02e2</id><updated>2022-03-31T07:00:00Z</updated><published>2022-03-31T07:00:00Z</published><summary type="html">&lt;p&gt;Applications running under &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; must expose secrets in order to connect to external services such as REST APIs, databases, and event buses. Each service provider suggests a different way to access their secrets, and for a long time, each application has consumed those secrets in a custom way. The &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/getting-started/installing-service-binding.html"&gt;Service Binding Operator&lt;/a&gt; makes the life of the application developer a lot easier by providing a consistent and declarative service binding method.&lt;/p&gt; &lt;p&gt;This article will show you how to use the Service Binding Operator to bind an application to a database, enhanced by help from the &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; console. The example in this article installs a MySQL database and a simple application: the &lt;a href="https://github.com/spring-projects/spring-petclinic"&gt;Spring PetClinic sample application&lt;/a&gt;, enriched by &lt;a href="https://github.com/spring-cloud/spring-cloud-bindings"&gt;Spring Cloud Bindings&lt;/a&gt; library.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To bind to a MySQL application, you need to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Install the Service Binding Operator in all namespaces&lt;/li&gt; &lt;li&gt;Create a project&lt;/li&gt; &lt;li&gt;Install the Percona Distribution for MySQL Operator&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;To complete these prerequisites, you must be in the &lt;strong&gt;Administrator&lt;/strong&gt; perspective of the Red Hat OpenShift web console, which requires administrator access to the cluster. If you do not have the required access, get in touch with the cluster administrator to perform the steps in this section.&lt;/p&gt; &lt;h3&gt;Install the Service Binding Operator in all namespaces&lt;/h3&gt; &lt;ol&gt;&lt;li&gt;In the &lt;strong&gt;Administrator&lt;/strong&gt; perspective of the web console, navigate to &lt;strong&gt;Operators→Operator Hub&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Use the &lt;strong&gt;Filter by Keyword&lt;/strong&gt; box to search for &lt;strong&gt;Service Binding Operator&lt;/strong&gt; in the catalog. Click the &lt;strong&gt;Service Binding Operator&lt;/strong&gt; box.&lt;/li&gt; &lt;li&gt;Read the brief description of the Operator on the &lt;strong&gt;Service Binding Operator&lt;/strong&gt; page, then click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Install Operator&lt;/strong&gt; page (Figure 1): &lt;ul&gt;&lt;li&gt;Select &lt;strong&gt;All namespaces on the cluster (default)&lt;/strong&gt; for the &lt;strong&gt;Installation Mode&lt;/strong&gt;. This mode installs the Operator in the default &lt;code&gt;openshift-operators&lt;/code&gt; namespace, which enables the Operator to watch and be available to all namespaces in the cluster.&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;Automatic&lt;/strong&gt; for the &lt;strong&gt;Approval Strategy&lt;/strong&gt;. This ensures that the future upgrades to the Operator are handled automatically by the &lt;a href="https://docs.openshift.com/container-platform/4.7/operators/understanding/olm/olm-understanding-olm.html"&gt;Operator Lifecycle Manager&lt;/a&gt; (OLM).&lt;/li&gt; &lt;li&gt;Select an &lt;strong&gt;Update Channel&lt;/strong&gt;. The default &lt;strong&gt;stable&lt;/strong&gt; channel installs the latest stable and supported release of the Operator.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Install.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Installed Operator - ready for use&lt;/strong&gt; pane, click &lt;strong&gt;View Operator&lt;/strong&gt;. The Operator is listed on the &lt;strong&gt;Installed Operators&lt;/strong&gt; page.&lt;/li&gt; &lt;li&gt;Verify that the &lt;strong&gt;Status&lt;/strong&gt; is set to &lt;strong&gt;Succeeded&lt;/strong&gt; to confirm the successful installation of the Service Binding Operator.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure1_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure1_0.png?itok=UolbQx-i" width="600" height="328" alt="Screenshot of the Install Operator page, which gives you options for how the Operator is run and updated" loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The Install Operator page gives you options for how the Operator is run and updated.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Create a project&lt;/h3&gt; &lt;p&gt;Every application runs in a project. To create a project for this application, follow these steps:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;In the &lt;strong&gt;Administrator&lt;/strong&gt; perspective, navigate to &lt;strong&gt;Home→Projects&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Create Project&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Name&lt;/strong&gt; field, enter &lt;code&gt;my-petclinic&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Install the Percona Distribution for MySQL Operator&lt;/h3&gt; &lt;p&gt;This Operator is one of the easiest ways to get a running database, so we'll use the Operator for this article. Install as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;In the &lt;strong&gt;Administrator&lt;/strong&gt; perspective of the web console, navigate to &lt;strong&gt;Operators→Operator Hub&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Use the &lt;strong&gt;Filter by Keyword&lt;/strong&gt; box to search for &lt;strong&gt;Percona Distribution for MySQL&lt;/strong&gt;. &lt;strong&gt;Operator&lt;/strong&gt; in the catalog.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Percona Distribution for MySQL Operator&lt;/strong&gt; box and then click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Install Operator&lt;/strong&gt; page (Figure 2): &lt;ul&gt;&lt;li&gt;Select &lt;strong&gt;A specific namespace on the cluster&lt;/strong&gt; for the &lt;strong&gt;Installation Mode&lt;/strong&gt;. (The Percona Distribution for MySQL Operator does not support installation in all namespaces.)&lt;/li&gt; &lt;li&gt;Select &lt;strong&gt;my-petclinic&lt;/strong&gt; from the &lt;strong&gt;Installed Namespace&lt;/strong&gt; dropdown, then click &lt;strong&gt;Install&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;On the &lt;strong&gt;Installed Operator - Ready for Use&lt;/strong&gt; pane, click &lt;strong&gt;View Operator&lt;/strong&gt;. The Operator is listed on the &lt;strong&gt;Installed Operators&lt;/strong&gt; page.&lt;/li&gt; &lt;li&gt;Verify the &lt;strong&gt;Status&lt;/strong&gt; is set to &lt;strong&gt;Succeeded&lt;/strong&gt; to confirm the successful installation of the &lt;strong&gt;Percona Distribution for MySQL&lt;/strong&gt; Operator.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure2_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure2_0.png?itok=klsq-1Yl" width="600" height="339" alt="Screenshot of the Install Operator page showing the options specific to the particular Operator you selected." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Install Operator page shows the options specific to the particular Operator you selected.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Installing the Percona Distribution for MySQL Operator does not create the database instance itself. You will create one later in this article.&lt;/p&gt; &lt;h2&gt;Use the Service Binding Operator&lt;/h2&gt; &lt;p&gt;So far you've been setting up the prerequisites in the &lt;strong&gt;Administrator&lt;/strong&gt; perspective of the OpenShift console, but for the rest of this article, you should be working in the &lt;strong&gt;Developer&lt;/strong&gt; perspective of the &lt;strong&gt;my-petclinic&lt;/strong&gt; project.&lt;/p&gt; &lt;h3&gt;Create an instance of a Percona XtraDB Cluster&lt;/h3&gt; &lt;p&gt;Now you'll create the instance of the cluster you're going to run in Red Hat OpenShift:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Navigate to the &lt;strong&gt;Add+&lt;/strong&gt; view.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Import YAML&lt;/strong&gt; under the &lt;strong&gt;From Local Machine&lt;/strong&gt; section (Figure 3). &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure3_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure3_0.png?itok=mPEv4Laq" width="600" height="332" alt="Screenshot showing how to find the From Local Machine box and click Import YAML." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Find the From Local Machine box and click Import YAML.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;mysqlcluster-deployment.yaml&lt;/code&gt;, the minimal custom resource file provided in &lt;a href="https://github.com/redhat-developer/service-binding-operator/blob/master/samples/apps/spring-petclinic/mysqlcluster-deployment.yaml"&gt;Service Binding Operator's Github repository&lt;/a&gt;, and upload it.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Create&lt;/strong&gt;. It will take a few minutes for the stateful set to be created.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Search&lt;/strong&gt; on the left-hand navigation bar.&lt;/li&gt; &lt;li&gt;Use the &lt;strong&gt;Resources&lt;/strong&gt; dropdown and search for &lt;strong&gt;StatefulSet&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The &lt;strong&gt;Search&lt;/strong&gt; page (Figure 4) now displays the following stateful sets:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;minimal-cluster-haproxy&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;minimal-cluster-pxc&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure4_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure4_0.png?itok=GjZXUrHN" width="600" height="263" alt="Screenshot showing that the displayed stateful sets verify that the database instance is created." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The displayed stateful sets verify that the database instance is created.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;h3&gt;Create the Spring PetClinic application&lt;/h3&gt; &lt;ol&gt;&lt;li&gt;Navigate to the &lt;strong&gt;Add+&lt;/strong&gt; view.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Import YAML&lt;/strong&gt; under the &lt;strong&gt;From Local Machine&lt;/strong&gt; section.&lt;/li&gt; &lt;li&gt;Use &lt;code&gt;petclinic-mysql-deployment&lt;/code&gt;, the YAML file provided in the &lt;a href="https://github.com/redhat-developer/service-binding-operator/blob/master/samples/apps/spring-petclinic/mysqlcluster-deployment.yaml"&gt;Serivce Binding Operator's Github repository&lt;/a&gt;, and upload it.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Create.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Search&lt;/strong&gt; on the left-hand navigation bar.&lt;/li&gt; &lt;li&gt;Use the &lt;strong&gt;Resources&lt;/strong&gt; dropdown and search for &lt;strong&gt;Deployment.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The &lt;strong&gt;spring-petclinic&lt;/strong&gt; deployment should now be available (Figure 5).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure5_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure5_0.png?itok=lvt8iSur" width="600" height="248" alt="Screenshot of the Search page showing the Spring Petclinic deployment." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: The Search page shows the Spring Petclinic deployment.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Bind the Spring PetClinic application to the MySQL database service&lt;/h3&gt; &lt;p&gt;The Service Binding Operator helps you bind the components within a cluster. Red Hat OpenShift makes use of the Operator even easier through a drag-and-drop facility in the Topology view of the web console. Use the interface as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Ensure that you are in the appropriate project, which is &lt;strong&gt;my-petclinic&lt;/strong&gt; in this example.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Topology&lt;/strong&gt; view, hover over the &lt;strong&gt;spring-petclinic&lt;/strong&gt; sample application to see a dangling arrow on the node.&lt;/li&gt; &lt;li&gt;Click and drag the arrow to the minimal-cluster Percona XtraDB cluster instance to make a binding connection.&lt;/li&gt; &lt;li&gt;Set the name of the connector when prompted. You can use the default name, &lt;strong&gt;spring-petclinic-d-minimal-cluster-pxdb&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;So what is going on behind the scenes as you implement the binding? The Service Binding Operator creates a binding secret to collect all the binding data, which is then projected into the application. The minimal-cluster-secret created by the instance of the Percona XtraDB cluster is included through annotations, along with other binding data exposed by the Percona XtraDB cluster.&lt;/p&gt; &lt;p&gt;The Spring PetClinic application invokes methods from the Spring Cloud Bindings library, which looks for the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; environment variable to locate the &lt;code&gt;/bindings&lt;/code&gt; directory where the binding data is projected. For more information on how an application uses these values, see the &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/using-projected-bindings/using-projected-bindings.html"&gt;Using Projected Bindings&lt;/a&gt; section of the Service Binding Operator's user guide.&lt;/p&gt; &lt;p&gt;For the Service Binding Operator to provide the binding data, the backing services must expose the binding data in a way that the Service Binding Operator can detect. Luckily, the Percona XtraDB instance is a bindable type of service. For more information please see the &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/exposing-binding-data/intro-expose-binding.html"&gt;Exposing Binding Data&lt;/a&gt; section of the user guide.&lt;/p&gt; &lt;p&gt;When the binding is successful, it is shown as an arrow connecting the application to the service (Figure 6).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure6_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure6_0.png?itok=Z1RBIUKI" width="600" height="338" alt="Screenshot of the Topology view, with an arrow to indicate that Spring PetClinic can connect to the Percona MySQL database." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: The Topology view shows an arrow to indicate that Spring PetClinic can connect to the Percona MySQL database.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Check the Service Binding status&lt;/h3&gt; &lt;p&gt;Now you need to check to see whether the Service Binding Operator has actually created a working connection:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Click &lt;strong&gt;Search&lt;/strong&gt; on the left-hand navigation bar.&lt;/li&gt; &lt;li&gt;Use the &lt;strong&gt;Resources&lt;/strong&gt; dropdown and select &lt;strong&gt;ServiceBinding&lt;/strong&gt;. The &lt;strong&gt;Search&lt;/strong&gt; page displays the &lt;strong&gt;spring-petclinic-d-minimal-cluster-pxdb&lt;/strong&gt; service binding.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;spring-petclinic-d-minimal-cluster-pxdb&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The console (Figure 7) shows the details of the Service Binding with the following Boolean statuses:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;CollectionReady&lt;/strong&gt;: The service can collect and persist intermediate manifests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;InjectionReady&lt;/strong&gt;: The application manifests can use the intermediate manifests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ready&lt;/strong&gt;: The application is bound successfully to the backing service.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure7.png?itok=_tT9BUfa" width="600" height="342" alt="Screenshot showing the statuses of three conditions in the application and backing service." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: The statuses of three conditions in the application and backing service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Create a route for the Spring PetClinic service&lt;/h3&gt; &lt;p&gt;The backing service has exposed a connection to the Spring PetClinic application, but the application also has to expose a connection to clients. Once again, the OpenShift console makes it easy to create a route:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Search&lt;/strong&gt; on the left-hand navigation bar.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Use the &lt;strong&gt;Resources&lt;/strong&gt; dropdown and select &lt;strong&gt;Route&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Create Route&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Set a name for the route and select &lt;strong&gt;spring-petclinic&lt;/strong&gt; service from the &lt;strong&gt;Service&lt;/strong&gt; dropdown (Figure 8).&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Select the&lt;strong&gt; Target port&lt;/strong&gt; for the service.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure8.png?itok=T3oI4XEX" width="600" height="300" alt="Screenshot of the Create Route page, which allows you to specify the parameters you need for the route to your application." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: The Create Route page allows you to specify the parameters you need for the route to your application.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;In the &lt;strong&gt;Developer→Topology&lt;/strong&gt; view, click the &lt;strong&gt;Open URL&lt;/strong&gt; link to get access to the running application (Figure 9).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure9.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure9.png?itok=QoyMcxt4" width="600" height="338" alt="Screenshot of your application in the Topology view, which includes an Open URL link for viewing its web page." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: Your application in the Topology view includes an Open URL link for viewing its web page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can now remotely view the Spring PetClinic sample application to confirm that the application is now connected to the database service, and that the data has been successfully projected to the application from the MySQL database service (Figure 10).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure10.png?itok=Ftvd2OSn" width="600" height="191" alt="Screenshot showing that the Spring PetClinic application contains data loaded from the backing service." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: The Spring PetClinic application contains data loaded from the backing service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Insights and conclusion&lt;/h2&gt; &lt;p&gt;Using the Service Binding Operator to bind an application to a backing service provides the following benefits:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Enabling you to connect your applications to backing services with a consistent and predictable experience.&lt;/li&gt; &lt;li&gt;Eliminating the error-prone manual configuration of binding information.&lt;/li&gt; &lt;li&gt;Providing service operators a low-touch administrative experience to provision and manage access to services.&lt;/li&gt; &lt;li&gt;Enriching the development lifecycle with a consistent and declarative service binding method that eliminates environments' discrepancies.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The Red Hat OpenShift Container Platform web console provides a user interface that simplifies the deployment of applications and custom resources and the binding of applications to backing services using the Service Binding Operator.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/11/binding-workloads-services-made-easier-service-binding-operator-red-hat" title="Bind workloads to services easily with the Service Binding Operator and Red Hat OpenShift"&gt;Bind workloads to services easily with the Service Binding Operator and Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kartikey Mamgain</dc:creator><dc:date>2022-03-31T07:00:00Z</dc:date></entry><entry><title type="html">Portfolio Architecture Examples - Edge Collection</title><link rel="alternate" href="http://www.schabell.org/2022/03/portfolio-architecture-examples-edge-collection.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/03/portfolio-architecture-examples-edge-collection.html</id><updated>2022-03-31T05:00:00Z</updated><content type="html">Figure 1: The portfolio architecture process For a few years now we've been working on a project we have named . These are based on selecting a specific use case we are seeing used in the real world by customers and then finding implementations of that case using three or more products from the Red Hat portfolio. This basic premise is used as the foundation, but many aspects of open source are included in both the process and the final product we have defined. There is a community, where we share the initial project kickoff with a group of architects and use their initial feedback from the start. We also present the architecture product we've created right at the end before we publish to ensure usability by architects in the field. The final publish product includes some internal only content around the customer projects researched, but most of the content is  through various open source channels.  This article is sharing an overview of the product we've developed, what's available to you , and concludes by sharing a collection of architectures we've published. INTRODUCTION The basis of a portfolio architecture is a use case, two to three actual implementations that can be researched, and includes the use of a minimum of three products. This is the ideal foundation for a project to start, but we encountered a problem with use cases containing emerging technologies or emerging domains in the market. To account for these we've chosen to note the fact that these are opinionated architectures based on internal reference architectures.  The product has been defined as complete for publishing when it contains the following content: * Short use case definition * Diagrams - logical, schematic (physical), and detail diagrams * Public slide deck containing the use case story and architecture diagrams * Internal slide deck containing both the pubic deck content and the confidential customer research * Video (short) explanation of the architecture * Either a technical brief document or one or more articles covering the solution architecture Note that the items in italics are all available to anyone  in the Red Hat Portfolio Architecture Center or in the Portfolio Architecture Examples repository. FIGURE 2: LOGICAL DIAGRAM DESIGN TEMPLATE TOOLING AND WORKSHOPS The progress towards our products required a good idea of how we wanted to diagram our architectures. We chose to keep them very generic and simple in style to facilitate all levels of conversation around a particular use case without getting bogged down in notational discussions.  A simple three level design for our architectures was captured by using logical, schematic, and detail diagrams. All of these have been integrated in  with pre-defined templates and icons for easily getting started. Furthermore, we've developed a tooling workshop to quickly ramp up on the design methods and tooling we've made available. It's called , has been featured in several. EDGE COLLECTION The collection featured today focuses on edge computing architectures. There are currently five architectures in this collection and we'll provide a short overview of each, leaving the in depth exploration as an exercise for the reader. Figure 3: Edge architecture collection In each of these architecture overviews you'll find a table of contents outlining the technologies used, several example schematic diagrams with descriptions, and a link in the last section to open the diagrams directly into the online tooling in your browser. This architecture covers the use case of providing a consistent infrastructure experience from cloud to edge and enabling modern containerized applications at edge. (Note: this project is a new architecture and currently in progress, so sharing one of the schematic architecture diagrams and you can monitor this project for updates as it progresses to completion.) This use case is bringing cloud like capabilities to the edge locations. This architecture covers the use case around data center to edge where the energy (utility) infrastructure companies operate across vast geographical area that connects the upstream drilling operations with downstream fuel processing and delivery to customers. These companies need to monitor the condition of pipeline and other infrastructure for operational safety and optimization. The use case is bringing computing closer to the edge by monitoring for potential issues with gas pipelines (edge). The manufacturing industry has consistently used technology to fuel innovation, production optimization and operations. Now, with the combination of edge computing and AI/ML, manufacturers can benefit from bringing processing power closer to data. This helps actions be taken faster on things like errors and predictive maintenance. The use case is for boosting manufacturing efficiency and product quality with artificial intelligence and machine learning out to the edge. This architecture covers edge medical diagnosis in the healthcare industry. It Accelerates medical diagnosis using condition detection in medical imagery with AI/ML at medical facilities. The use case is accelerating medical diagnosis using condition detection in medical imagery with AI/ML at medical facilities. This Portfolio Architecture targets energy providers in North America that need to be compliant with NERC regulations and in order to achieve this they decide to modernise the interfaces between their business applications and their SCADA systems also for a better consumption of information that can be used in combination with AI/ML and decision management tools to better address customer needs. The use case is providing interfaces with SCADA systems that are compliant with NERC regulations, creating different layers of API gateways to protect business service depending on the network zones. If you are interested in more architecture solutions like these, feel free to export the . More architecture collections include: * Application development * Automation * Data engineering * * Finance * * Infrastructure * Retail *</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Which is better: A monolithic Kafka cluster vs many?</title><link rel="alternate" href="http://www.ofbizian.com/2022/03/which-is-better-monolithic-kafka.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/03/which-is-better-monolithic-kafka.html</id><updated>2022-03-30T10:17:00Z</updated><content type="html">Apache Kafka is designed for performance and large volumes of data. Kafka's append-only log format, sequential I/O access, and zero copying all support high throughput with low latency. Its partition-based data distribution lets it scale horizontally to hundreds of thousands of partitions. Because of these capabilities, it can be tempting to use a single monolithic Kafka cluster for all of your eventing needs. Using one cluster reduces your operational overhead and development complexities to a minimum. But is "a single Kafka cluster to rule them all" the ideal architecture, or is it better to split Kafka clusters? To answer that question, we have to consider the segregation strategies for maximizing performance and optimizing cost while increasing Kafka adoption. We also have to understand the impact of using , on a public cloud, or managing it yourself on-premise (Are you looking to experiment with Kafka? Get started in minutes with a no-cost ). This article explores these questions and more, offering a structured way to decide whether or not to segregate Kafka clusters in your organization. Figure 1 summarizes the questions explored in this article.   Figure 1. A mind map for Apache Kafka cluster segregation strategies shows the concerns that can drive a multiple-cluster setup. BENEFITS OF A MONOLITHIC KAFKA CLUSTER To start, let's explore some of the benefits of using a single, monolithic Kafka cluster. Note that by this I don't mean literally a single Kafka cluster for all environments, but a single production Kafka cluster for the entire organization. The different environments would still typically be fully isolated with their respective Kafka clusters. A single production Kafka cluster is simpler to use and operate and is a no-brainer as a starting point. GLOBAL EVENT HUB Many companies are sold on the idea of having a single "Kafka backbone" and the value they can get from it. The possibility of combining data from different topics from across the company arbitrarily in response to future and yet unknown business needs is a huge motivation. As a result, some organizations end up using Kafka as a centralized enterprise service bus (ESB) where they put all their messages under a single cluster. The chain of streaming applications is deeply interconnected. This approach can work for companies with a small number of applications and development teams, and with no hard departmental data boundaries that are enforced in large corporations by business and regulatory forces. (Note that this singleton Kafka environment expects no organizational boundaries.) The monolithic setup reduces thinking about event boundaries, speeds up development, and works well until an operational or a process limitation kicks in. NO TECHNICAL CONSTRAINTS Certain technical features are available only within a single Kafka cluster. For example, a common pattern used by stream processing applications is to perform read-process-write operations in a sequence without any tolerances for errors that could lead to duplicates or loss of messages. To address that strict requirement, Kafka offers transactions that ensure that each message is consumed from the source topic and published to a target topic in exactly-once processing semantics. That guarantee is possible only when the source and target topics are within the same Kafka cluster. A consumer group, such as a , can process data from a single Kafka cluster only. Therefore, multi-topic subscriptions or load balancing across the consumers in a consumer group are possible only within a single Kafka cluster. In a multi-Kafka setup, enabling such stream processing requires data replication across clusters. Each Kafka cluster has a unique URL, a few authentication mechanisms, Kafka-wide authorization configurations, and other cluster-level settings. With a single cluster, all applications can make the same assumptions, use the same configurations, and send all events to the same location. These are all good technical reasons for sharing a single Kafka cluster whenever possible. LOWER COST OF OWNERSHIP I assume that you use Kafka because you have a huge volume of data, or you want to do low latency asynchronous interactions, or take advantage of both of these with added high availability—not because you have modest data needs and Kafka is a fashionable technology. Offering high-volume, low-latency Kafka processing in a production environment has a significant cost. Even a lightly used Kafka cluster deployed for production purposes requires three to six brokers and three to five ZooKeeper nodes. The components should be spread across multiple availability zones for redundancy. Note: ZooKeeper , but its role will still have to be performed by the cluster. You have to budget for base compute, networking, storage, and operating costs for every Kafka cluster. This cost applies whether you self-manage a Kafka cluster on-premises with something like or consume Kafka as a service. There are attempts at "serverless" Kafka offerings that try to be more creative and hide the cost per cluster in other cost lines, but somebody still has to pay for resources. Generally, running and operating multiple Kafka clusters costs more than a single larger cluster. There are exceptions to this rule, where you achieve local cost optimizations by running a cluster at the point where the data and processing happens or by avoiding replication of large volumes of non-critical data, and so on. BENEFITS OF MULTIPLE KAFKA CLUSTERS Although Kafka can scale beyond the needs of a single team, it is not designed for . Sharing a single Kafka cluster across multiple teams and different use cases requires precise application and cluster configuration, a rigorous governance process, standard naming conventions, and best practices for preventing abuse of the shared resources. Using multiple Kafka clusters is an alternative approach to address these concerns. Let's explore a few of the reasons that you might choose to implement multiple Kafka clusters. OPERATIONAL DECOUPLING Kafka's sweet spot is real-time messaging and distributed data processing. Providing that at scale requires operational excellence. Here are a few manageability concerns that apply to operating Kafka. WORKLOAD CRITICALITY Not all Kafka clusters are equal. A batch processing Kafka cluster that can be populated from source again and again with derived data doesn't have to replicate data into multiple sites for higher availability. An ETL data pipeline can afford more downtime than a real-time messaging infrastructure for frontline applications. Segregating workloads by service availability and data criticality helps you pick the most suitable deployment architecture, optimize infrastructure costs, and direct the right level of operating attention to every workload. MAINTAINABILITY The larger a cluster gets, the longer it can take to upgrade and expand the cluster due to rolling restarts, data replication, and rebalancing. In addition to the length of the change window, the time when the change is performed might also be important. A customer-facing application might have an upgrade window that differs from a customer service application. Using separate Kafka clusters allows faster upgrades and more control over the time and the sequence of rolling out a change. REGULATORY COMPLIANCE Regulations and certifications typically leave no room for compromise. You might have to host a Kafka cluster on a specific cloud provider or region. You might have to allow access only to support personnel from a specific country. All personally identifiable information (PII) data might have to be on a particular cluster with short retention, separate administrative access, and network segmentation. You might want to hold the data encryption keys for specific clusters. The larger your company is, the longer the requirements list gets. TENANT ISOLATION The secret for happy application coexistence on a shared infrastructure relies on having good primitives for access, resource, and logical isolation. Unlike , Kafka has no concept like namespaces for enforcing quotas and access control or avoiding topic naming collisions. Let's explore some of the resulting challenges for isolating tenants. RESOURCE ISOLATION Although Kafka has mechanisms to control resource use, it doesn't prevent a bad tenant from monopolizing the cluster resources. Storage size can be controlled per topic through retention size, but cannot be limited for a group of topics corresponding to an application or tenant. Network utilization can be enforced through quotas, but it is applied at the client connection level. There is no means to prevent an application from creating an unlimited number of topics or partitions until the whole cluster gets to a halt. All of that means you have to enforce these resource control mechanisms while operating at different granularity levels, and enforce additional conventions for the healthy coexistence of multiple teams on a single cluster. An alternative is to assign separate Kafka clusters to each functional area and use cluster-level resource isolation. SECURITY BOUNDARY Kafka's access control with the default authorization mechanism (ACLs) is more flexible than the quota mechanism and can apply to multiple resources at once through pattern matching. But you have to ensure good naming convention hygiene. The structure for topic name prefixes becomes part of your security policy. ACLs control which users can perform which actions on which resources, but a user with admin access to a Kafka instance has access to all the topics in that Kafka instance. With multiple clusters, each team can have admin rights only to their Kafka instance. The alternative is to ask someone with admin rights to edit the ACLs and update topics rights and such. Nobody likes having to open a ticket to another team to get a project rolling. LOGICAL DECOUPLING A single cluster shared across multiple teams and applications with different needs can quickly get cluttered and difficult to navigate. You might have teams that need very few topics and others that generate hundreds of them. Some teams might even generate topics on the fly from existing data sources by . You might need hundreds of granular ACLs for some applications that are less trusted, and coarse-grained ACLs for others. You might have a large number of producers and consumers. In the absence of namespaces, properties, and labels that can be used for logical segregation of resources, the only option left is to use naming conventions creatively. USE CASE OPTIMIZATION So far we have looked at the manageability and multi-tenancy needs that apply to most shared platforms in common. Next, we will look at a few examples of Kafka cluster segregation for specific use cases. The goal of this section is to list the long tail of reasons for segregating Kafka clusters that varies for every organization and demonstrate that there is no "wrong" reason for creating another Kafka cluster. DATA LOCALITY Data has gravity, meaning that a useful dataset tends to attract related services and applications. The larger a dataset is, the harder it is to move around. Data can originate from a constrained or offline environment, preventing it from streaming into the cloud. Large volumes of data might reside in a specific region, making it economically unfeasible to replicate the data to other locations. Therefore, you might create separate Kafka clusters at regions, cloud providers, or even to benefit from data's gravitational characteristics. FINE-TUNING Fine-tuning is the process of precisely adjusting the parameters of a system to fit certain objectives. In the Kafka world, the primary interactions that an application has with a cluster center on the concept of topics. And while every topic has separate and fine-tuning configurations, there are also cluster-wide settings that apply to all applications. For instance, cluster-wide configurations such as redundancy factor (RF) and in-sync replicas (ISR) apply to all topics if not explicitly overridden per topic. In addition, some constraints apply to the whole cluster and all users, such as the allowed authentication and authorization mechanisms, IP whitelists, the maximum message size, whether dynamic topic creation is allowed, and so on. Therefore, you might create separate clusters for large messages, less-secure authentication mechanisms, and other oddities to localize and isolate the effect of such configurations from the rest of the tenants. DOMAIN OWNERSHIP Previous sections described examples of cluster segregation to address data and application concerns, but what about business domains? Aligning Kafka clusters by business domain can enforce ownership and give users more responsibilities. Domain-specific clusters can offer more freedom to the domain owners and reduce reliance on a central team. This division can also reduce cross-cluster data replication needs because most joins are likely to happen within the boundaries of a business domain. PURPOSE-BUILT Kafka clusters can be created and configured for a particular use case. Some clusters might be born while and others created while implementing event-driven . Some clusters might be created to handle unpredictable loads, whereas others might be optimized for stable and predictable processing. For example, for stream processing with topic compaction enabled, separate clusters for service communication with short message retention, and a logging cluster for log aggregation. for producers and consumers. The so-called fronting clusters are responsible for getting messages from all applications and buffering, while consumer clusters contain only a subset of the data needed for stream processing. These decisions for classifying clusters are based on high-level criteria, but you might also have low-level criteria for separate clusters. For example, to benefit from page caching at the operating-system level, you might create a separate cluster for consumers that re-read topics from the beginning each time. The separate cluster would prevent any disruption of the page caches for real-time consumers that read data from the current head of each topic. You might also create a separate cluster for the odd use case of a single topic that uses the whole cluster. The reasons can be endless. SUMMARY The argument "one thing to rule them all" has been used for pretty much any technology: mainframes, databases, application servers, ESBs, Kubernetes, cloud providers, and so on. But generally, the principle falls apart. At some point, decentralizing and scaling with multiple instances offer more benefits than continuing with one centralized instance. Then a new threshold is reached, and the technology cycle starts to centralize again, which sparks the next phase of innovation. Kafka is following this historical pattern. In this article, we looked at common motivations for growing a monolithic Kafka cluster along with reasons for splitting it out. But not all points apply to all organizations in every circumstance. Every organization has different business goals and execution strategies, team structure, application architecture, and data processing needs. Every organization is at a different stage of its , a cloud-based architecture, edge computing, data mesh—you name it. You might run on-premises Kafka clusters for good reason and give more weight to the operational concerns you have to deal with. Software-as-a-Service (SaaS) offerings such as can provision a Kafka cluster with a single click and remove the concerns around maintainability, workload criticality, and compliance. With such services, you might pay more attention to governance, logical isolation, and controlling data locality. If you have a reasonably sized organization, you will have hybrid and multi-cloud Kafka deployments and a new set of concerns around optimizing and reusing Kafka skills, patterns, and best practices across the organization. These concerns are topics for another article. I hope this guide provides a way to structure your decision-making process for segregating Kafka clusters. Follow me at to join my journey of learning Apache Kafka. This post was originally published on Red Hat Developers. To read the original post, check .</content><dc:creator>Unknown</dc:creator></entry><entry><title>Generate and save an HTML report in Jenkins on OpenShift 4</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/30/generate-and-save-html-report-jenkins-openshift-4" /><author><name>Muhammad Edwin</name></author><id>ed097a95-68ee-4774-919c-153dddb7b95d</id><updated>2022-03-30T07:00:00Z</updated><published>2022-03-30T07:00:00Z</published><summary type="html">&lt;p&gt; &lt;/p&gt; &lt;p&gt;Jenkins is one of the most popular &lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt; tools for automating builds and deployments. It is very flexible and can be deployed on almost every operating system, as well as on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. This article shows you how to deploy Jenkins on OpenShift 4.9, create a simple pipeline to deploy a &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; application to OpenShift, do some testing, save the test results as HTML, and publish it as an artifact so that people can see the results.&lt;/p&gt; &lt;p&gt;For this scenario, we'll generate an HTML report using &lt;a href="https://jeremylong.github.io/DependencyCheck/dependency-check-maven/plugin-info.html"&gt;Maven OWASP Dependency Check plugins&lt;/a&gt;. The report will contain a list of libraries that contain vulnerabilities. This pipeline runs on Jenkins 2.2 on top of OpenShift 4.9.&lt;/p&gt; &lt;h2&gt;Use Jenkins to generate a report on OpenShift 4&lt;/h2&gt; &lt;p&gt;There are multiple ways to set up Jenkins on OpenShift 4. This article uses a template provided by the OpenShift Developer Catalog.&lt;/p&gt; &lt;p&gt;First, check whether the Jenkins template is available in OpenShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get template -n openshift | grep jenkins&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If Jenkins templates are available, you'll get output such as:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;jenkins-ephemeral Jenkins service, without persistent storage.... 8 (all set) 7 jenkins-ephemeral-monitored Jenkins service, without persistent storage. ... 9 (all set) 8 jenkins-persistent Jenkins service, with persistent storage.... 10 (all set) 8 jenkins-persistent-monitored Jenkins service, with persistent storage. ... 11 (all set) 9&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now let's try to spawn a Jenkins ephemeral report. In this case, &lt;em&gt;ephemeral&lt;/em&gt; means that the service is not storing its data. The ephemeral approach is good for a nonproduction environment. The following command creates Jenkins instances in the CI/CD namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app --template=jenkins-ephemeral -n cicd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the created route to see the exact URL for the newly created Jenkins instances. Then open that URL and log in with your OpenShift credentials.&lt;/p&gt; &lt;h2&gt;Building and running a pipeline&lt;/h2&gt; &lt;p&gt;Start creating a build pipeline by selecting a Pipeline icon in your Jenkins instance, as shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure1.png?itok=cWjJRK96" width="600" height="140" alt="Screenshow showing how to create a Pipeline in the Jenkins console." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Create a Pipeline in the Jenkins console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, paste the following code into the Pipeline script, as shown in Figure 2:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;node('maven') { stage ('pull code') { sh "git clone https://github.com/edwin/hello-world-java-docker.git source" } stage ('build') { dir("source") { sh "mvn -Dmaven.repo.local=/tmp/m2 org.owasp:dependency-check-maven:check" } } stage ('generate report') { dir("source") { publishHTML (target: [ allowMissing: true, alwaysLinkToLastBuild: true, keepAll: true, reportDir: 'target', reportFiles: 'dependency-check-report.html', reportName: "Application-Dependency-Check-Report" ]) } } }&lt;/code&gt;&lt;/pre&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure2.png?itok=EjRvejxq" width="600" height="252" alt="Screenshow showing the script being inserted into a Pipeline script." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The script should be inserted into a Pipeline Script.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now focus on the "generate report" stage, where you save your HTML report as a build artifact. Press the &lt;strong&gt;Build Now&lt;/strong&gt; button, highlighted in Figure 3, to trigger the pipeline.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure3.png?itok=IyXQEsrQ" width="600" height="246" alt="Screenshow showin ghow the Pipeline's web page lets you build the Pipeline." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The Pipeline's web page lets you build the Pipeline.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;And now the HTML report appears in the menu bar on the left, as shown in Figure 4.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure4.png?itok=2EPd1vFc" width="600" height="395" alt="Screenshot showing that a report appears in the menu of the Pipeline web page." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: A report appears in the menu of the Pipeline web page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Clicking on the report button displays the contents of the generated HTML report, as illustrated in Figure 5.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure5.png?itok=mY8XUimA" width="600" height="388" alt="Screenshot of a formatted report displayed on the web page." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: A formatted report is displayed on the web page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you find that your report shows some errors or is a bit disorganized, there's a workaround to fix it. Go to &lt;strong&gt;Script Console&lt;/strong&gt; inside the &lt;strong&gt;Manage Jenkins&lt;/strong&gt; menu, as shown in Figure 6.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/figure6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/figure6.png?itok=weLqW9h1" width="600" height="231" alt="Screenshot showing that the Script Console is available in Tools and Actions." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: The Script Console is available in Tools and Actions.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Type the following script inside the &lt;strong&gt;Script Console&lt;/strong&gt; text field and then press the &lt;strong&gt;Run&lt;/strong&gt; button.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", "")&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In addition to automating your CI/CD process, Jenkins can automate the recording of events that occur during its own run. This article has illustrated several parts of the open source environment that make it easy to generate and save reports from Jenkins.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/30/generate-and-save-html-report-jenkins-openshift-4" title="Generate and save an HTML report in Jenkins on OpenShift 4"&gt;Generate and save an HTML report in Jenkins on OpenShift 4&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Muhammad Edwin</dc:creator><dc:date>2022-03-30T07:00:00Z</dc:date></entry><entry><title>C++ standardization (core language) progress in 2021</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/29/c-standardization-core-language-progress-2021" /><author><name>Jason Merrill</name></author><id>158d176a-ef10-475d-b296-15db643e1140</id><updated>2022-03-29T07:00:00Z</updated><published>2022-03-29T07:00:00Z</published><summary type="html">&lt;p&gt;This article covers the highlights of the &lt;a href="https://developers.redhat.com/topics/c"&gt;C++&lt;/a&gt; standardization proposals before the International Organization for Standardization (ISO) committee's Core and Evolution Working Groups last year. Read on to find out what's coming in C++23.&lt;/p&gt; &lt;h2&gt;Virtual ISO working group meetings&lt;/h2&gt; &lt;p&gt;For most of 2020, meetings about the C++ standard were held virtually and treated by the attendees as tentative, mostly filling time until we could meet in person again. As in-person meetings continue to be pushed out further into the future, and we've gotten more comfortable with the online format, we've gotten a lot more done in virtual meetings.&lt;/p&gt; &lt;p&gt;Now, instead of doing most of our business during the in-person meetings three times a year, the different working groups frequently meet throughout the year, some weekly, some monthly, or whatever seems appropriate. During the weeks when we previously had our in-person meetings, we hold only the Monday plenary meeting, virtually, to ratify the work the various groups have done in the intervening months.&lt;/p&gt; &lt;h2&gt;Notable core language changes for C++23&lt;/h2&gt; &lt;p&gt;The "Declarations and where to find them" and "Down with ()" papers that I mentioned in &lt;a data-saferedirecturl="https://www.google.com/url?q=https://developers.redhat.com/blog/2021/05/07/report-from-the-virtual-iso-c-meetings-in-2020-core-language&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw2gXTQQun-Yz6gkQR8-XYZG" href="https://developers.redhat.com/blog/2021/05/07/report-from-the-virtual-iso-c-meetings-in-2020-core-language"&gt;last year's report&lt;/a&gt; were accepted, as expected.&lt;/p&gt; &lt;h3&gt;if consteval&lt;/h3&gt; &lt;p&gt;The new &lt;code&gt;if consteval&lt;/code&gt; syntax (&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1938R3&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3_zOBnBo8UcZSqQgeGRRat" href="http://wg21.link/P1938R3"&gt;P1938R3&lt;/a&gt;) creates an immediate function context within a &lt;code&gt;constexpr&lt;/code&gt; function. In C++20. many people expected to be able to create such a context with &lt;code&gt;if (std::is_constant_evaluated())&lt;/code&gt;, but that syntax doesn't have that effect, and does have significant pitfalls. So new syntax seemed warranted. An example of the new syntax is:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;consteval int f(int i) { return i; } constexpr int g(int i) { if consteval { return f(i) + 1; // ok: immediate function context } else { return 42; } }&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Deducing "this"&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P0847R7&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3ewEtn0DkqGO_OGQIDafuA" href="http://wg21.link/P0847R7"&gt;P0847R7&lt;/a&gt; allows the object parameter (normally implicitly &lt;code&gt;this&lt;/code&gt;) of a non-static member function to be declared explicitly, and have its type deduced:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;struct X { int i; template &lt;typename Self&gt; auto&amp;&amp; foo(this Self&amp;&amp; self) { return self.i; } };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The primary use is expected to be to deduce the value category of the object argument, replacing the need for multiple overloads with different ref-qualifiers. One possibly surprising effect is that the object parameter type can also deduce to a derived type, causing references to a particular member to resolve instead to a member of the same name in the derived class:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;struct D : X { char i; } d; char&amp; r = d.foo(); // ok, returns reference to D::i, not B::i &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Adding #elifdef and #elifndef preprocessing directives&lt;/h3&gt; &lt;p&gt;These directives were recently added to C23, so &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2334R1&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw22XJ-AfGfBy4VFYaBng-Ml" href="http://wg21.link/P2334R1"&gt;P2334R1&lt;/a&gt; also adds them to C++23 to avoid preprocessor incompatibilities. Programmers have often been surprised that these directives don't exist, and in C++23 they will.&lt;/p&gt; &lt;h3&gt;Multidimensional subscript operator&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2128R6&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw05F4_8nThnnAQeknk4Hvt9" href="http://wg21.link/P2128R6"&gt;P2128R6&lt;/a&gt; changes the &lt;code&gt;operator[]&lt;/code&gt; member function from always having a single parameter to having any number of parameters. As a result, a multidimensional array class can use &lt;code&gt;ar[1,2,3]&lt;/code&gt; to index directly rather than through the proxy classes needed to support &lt;code&gt;ar[1][2][3]&lt;/code&gt;. Previously, the &lt;code&gt;ar[1,2,3]&lt;/code&gt; syntax was deprecated.&lt;/p&gt; &lt;h3&gt;C++ Identifier Syntax using Unicode Annex 31&lt;/h3&gt; &lt;p&gt;Past C++ standards have attempted to define their own subsets of Unicode to allow in identifiers, but each revision had flaws. C++23 (&lt;a href="http://wg21.link/"&gt;P1949R7&lt;/a&gt;) leaves that task instead to the Unicode Consortium, which now provides &lt;a data-saferedirecturl="https://www.google.com/url?q=http://www.unicode.org/reports/tr31/&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1u5OV0QMDLlf0PJ7bLq9_i" href="http://www.unicode.org/reports/tr31/"&gt;Annex 31&lt;/a&gt; for this purpose. This change conveniently coincides with the recent concern about &lt;a data-saferedirecturl="https://www.google.com/url?q=https://access.redhat.com/security/vulnerabilities/RHSB-2021-007&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1RrkyUw__nni5VJuzYCvIt" href="https://access.redhat.com/security/vulnerabilities/RHSB-2021-007"&gt;Trojan source attacks using bidirectional text&lt;/a&gt;, because Annex 31 significantly limits the use of bidirectional characters in identifiers.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Find out &lt;a href="https://developers.redhat.com/articles/2022/01/12/prevent-trojan-source-attacks-gcc-12"&gt;how to prevent Trojan Source attacks with GCC 12&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;CWG2397, auto specifier for pointers and references to arrays&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2386R0&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3pZmbKWjysVeEbYVsajeVX" href="http://wg21.link/P2386R0"&gt;P2386R0&lt;/a&gt; allows the declaration of pointers and references to arrays of &lt;code&gt;auto&lt;/code&gt;, such as:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt; int a[3]; auto (*p)[3] = &amp;a; // now OK&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Non-literal variables, labels, and gotos in constexpr functions&lt;/h3&gt; &lt;p&gt;Restrictions on &lt;code&gt;constexpr&lt;/code&gt; functions have been gradually weakening since C++11, in particular by changing requirements that a particular construct never appear in the function; instead the construct can be used but makes the evaluation non-constant. &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2242R3&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw0dRp-YrPXlhKN8u2a3KGT9" href="http://wg21.link/P2242R3"&gt;P2242R3&lt;/a&gt; makes that change for the constructs mentioned in the title, and for static or thread-local variables.&lt;/p&gt; &lt;h2&gt;What's in the pipeline for C++23?&lt;/h2&gt; &lt;p&gt;In addition to the changes just described, which the committee has accepted, additional changes are likely to make it into C++23. At least they have moved on to electronic voting by the full Evolution Working Group.&lt;/p&gt; &lt;h3&gt;Attributes on lambda-expressions&lt;/h3&gt; &lt;p&gt;There has not yet been a way to apply an attribute like &lt;code&gt;[[nodiscard]]&lt;/code&gt; to a lambda's operator(). The proposed &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2173R1&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1rz_Vj_S5_HlZZ3a-mJFFQ" href="http://wg21.link/P2173R1"&gt;P2173R1&lt;/a&gt; uses the syntax &lt;code&gt;[][[nodiscard]]() { ... }&lt;/code&gt;, which matches the position of attributes that appertain to a declarator-id in a declaration that has one.&lt;/p&gt; &lt;h3&gt;The equality operator you are looking for&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2468&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3wxSFmc3TLae5E9noUlfLp" href="http://wg21.link/P2468"&gt;P2468&lt;/a&gt; attempts to fix a long-standing problem. The implicit reversal of &lt;code&gt;operator==&lt;/code&gt; made some valid C++17 code ill-formed in C++20, most frequently where a class defines comparison operators that are accidentally asymmetric:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;struct S { bool operator==(const S&amp;) { return true; } // mistakenly non-const bool operator!=(const S&amp;) { return false; } // mistakenly non-const }; bool b = S{} != S{}; // well-formed in C++17, ambiguous in C++20&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the GCC compiler, I made this example work by making a reversed operator worse than an unreversed operator if they have the same parameter types. P2468 considered this fix, but instead proposes that if a class has matching &lt;code&gt;operator==&lt;/code&gt; and &lt;code&gt;operator!=&lt;/code&gt; declarations, no reversed candidates are considered.&lt;/p&gt; &lt;h3&gt;More constexpr relaxations&lt;/h3&gt; &lt;p&gt;In C++20, a function declared &lt;code&gt;constexpr&lt;/code&gt; that can never produce a constant result is ill-formed, and no diagnostic is required. This has proven a burden for implementers who would like their function to be usable in a constant expression when other functions that it calls become usable, but don't want to have to use macro trickery to coordinate the addition of the &lt;code&gt;constexpr&lt;/code&gt; keyword at the same time. So &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2448&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1QRy4t6DUh_7sUWTyhYaWM" href="http://wg21.link/P2448"&gt;P2448&lt;/a&gt; proposes removing that rule, along with the related rule that a defaulted member function can be declared &lt;code&gt;constexpr&lt;/code&gt; only if it could produce a constant result.&lt;/p&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2350&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1zaBq7Q0D9NQeSnNLHbCXH" href="http://wg21.link/P2350"&gt;P2350&lt;/a&gt; proposes allowing a class to be marked as constexpr to avoid the need to mark each member function separately.&lt;/p&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2280&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1xsO6OOLdDGKsyjv0UDzkT" href="http://wg21.link/P2280"&gt;P2280&lt;/a&gt; proposes that binding an object with a non-constant address to a reference parameter of a &lt;code&gt;constexpr&lt;/code&gt; function be allowed in a constant-expression, so long as it is never accessed. This is intended particularly to support uses like the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;template &lt;typename T, size_t N&gt; constexpr size_t array_size(T (&amp;)[N]) { return N; // referent of parameter is never used }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Inspired by these changes, for GCC 12, I've added an &lt;code&gt;-fimplicit-constexpr&lt;/code&gt; option that takes what seems to me the natural next step of treating all inline functions as implicitly &lt;code&gt;constexpr&lt;/code&gt;; I'm interested in feedback about people's experience with it before I propose it formally.&lt;/p&gt; &lt;h3&gt;Extended floating-point types and standard names&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1467&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw16uFucfFjKMZeJR6MSSfRi" href="http://wg21.link/P1467"&gt;P1467&lt;/a&gt; adds conditionally-supported &lt;code&gt;std::float{16,32,64,128}_t&lt;/code&gt; and &lt;code&gt;std::bfloat16_t&lt;/code&gt; for the corresponding IEEE types, as well as corresponding literal suffixes and adjustments to overload resolution to rank conversions between the greatly expanded set of floating-point types.&lt;/p&gt; &lt;h3&gt;Static operator()&lt;/h3&gt; &lt;p&gt;It seems unnecessary to prohibit the &lt;code&gt;operator()&lt;/code&gt; member function from being static, and &lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1169&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3skaAVqWz78W-9_bZKj-Fx" href="http://wg21.link/P1169"&gt;P1169&lt;/a&gt; allows it.  If a static definition had been allowed when lambdas were introduced, the call operator for a captureless lambda probably would have been implicitly static, but this paper does not propose making that change.&lt;/p&gt; &lt;h3&gt;Support for #warning&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2437&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw1aPG6otqkfyA2wLBpiKKxB" href="http://wg21.link/P2437"&gt;P2437&lt;/a&gt; adds the #warning preprocessor directive. Most compilers have already supported it for a long time.&lt;/p&gt; &lt;h3&gt;Labels at the end of compound statements&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2324&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw2nkm21KFkIZ_7nbHJLNh4i" href="http://wg21.link/P2324"&gt;P2324&lt;/a&gt; makes a convenience tweak that has already been adopted by C.&lt;/p&gt; &lt;h3&gt;Portable assumptions&lt;/h3&gt; &lt;p&gt;&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1774&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw2OMIBVhk69E0PYOUsyIkEe" href="http://wg21.link/P1774"&gt;P1774&lt;/a&gt; adds the &lt;code&gt;[[assume(expr)]]&lt;/code&gt; syntax to tell the compiler that it should assume during optimization that the expression argument evaluates to true. The C++20 contracts proposal ran aground on disagreement over whether contracts should or should not be assumed, so this proposal is now independent of any potential future contracts feature. This might be the least likely of the features discussed here to make it into C++23, as there is significant resistance to any assumption feature from one vendor, but there was strong support from the rest of the evolution group when the paper was discussed.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Work continues in study groups on various other proposals, such as pattern matching (&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P1371&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw3JQRHNSR80pmtB8JI2OUfW" href="http://wg21.link/P1371"&gt;P1371&lt;/a&gt;) and a return of Contracts (&lt;a data-saferedirecturl="https://www.google.com/url?q=http://wg21.link/P2182&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw30kCxjhgzF9PbsT46Nz8sq" href="http://wg21.link/P2182"&gt;P2182&lt;/a&gt;), but they are not trying to hit the deadline for C++23.&lt;/p&gt; &lt;p&gt;At the moment, the &lt;a data-saferedirecturl="https://www.google.com/url?q=https://isocpp.org/std/meetings-and-participation/upcoming-meetings&amp;amp;source=gmail&amp;amp;ust=1641079515135000&amp;amp;usg=AOvVaw0Nrhg62bi8B5urQofUPXSu" href="https://isocpp.org/std/meetings-and-participation/upcoming-meetings"&gt;next in-person meeting&lt;/a&gt; is planned for November 2022. We'll see whether it actually happens, and if so, how it is affected by our experience with virtual meetings.&lt;/p&gt; &lt;p&gt;Many of the proposed changes in C++23 simplify or relax complex restrictions. The changes reflect requests from the field and should make C++ easier for complex projects.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/29/c-standardization-core-language-progress-2021" title="C++ standardization (core language) progress in 2021"&gt;C++ standardization (core language) progress in 2021&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jason Merrill</dc:creator><dc:date>2022-03-29T07:00:00Z</dc:date></entry><entry><title type="html">Portfolio Architecture Examples - Telco Collection</title><link rel="alternate" href="http://www.schabell.org/2022/03/portfolio-architecture-exmaples-telco-collection.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/03/portfolio-architecture-exmaples-telco-collection.html</id><updated>2022-03-29T05:00:00Z</updated><content type="html">Figure 1: The portfolio architecture process For a few years now we've been working on a project we have named . These are based on selecting a specific use case we are seeing used in the real world by customers and then finding implementations of that case using three or more products from the Red Hat portfolio. This basic premise is used as the foundation, but many aspects of open source are included in both the process and the final product we have defined. There is a community, where we share the initial project kickoff with a group of architects and use their initial feedback from the start. We also present the architecture product we've created right at the end before we publish to ensure usability by architects in the field. The final publish product includes some internal only content around the customer projects researched, but most of the content is  through various open source channels.  This article is sharing an overview of the product we've developed, what's available to you , and concludes by sharing a collection of architectures we've published. INTRODUCTION The basis of a portfolio architecture is a use case, two to three actual implementations that can be researched, and includes the use of a minimum of three products. This is the ideal foundation for a project to start, but we encountered a problem with use cases containing emerging technologies or emerging domains in the market. To account for these we've chosen to note the fact that these are opinionated architectures based on internal reference architectures.  The product has been defined as complete for publishing when it contains the following content: * Short use case definition * Diagrams - logical, schematic (physical), and detail diagrams * Public slide deck containing the use case story and architecture diagrams * Internal slide deck containing both the pubic deck content and the confidential customer research * Video (short) explanation of the architecture * Either a technical brief document or one or more articles covering the solution architecture Note that the items in italics are all available to anyone  in the Red Hat Portfolio Architecture Center or in the Portfolio Architecture Examples repository. FIGURE 2: LOGICAL DIAGRAM DESIGN TEMPLATE TOOLING AND WORKSHOPS The progress towards our products required a good idea of how we wanted to diagram our architectures. We chose to keep them very generic and simple in style to facilitate all levels of conversation around a particular use case without getting bogged down in notational discussions.  A simple three level design for our architectures was captured by using logical, schematic, and detail diagrams. All of these have been integrated in  with pre-defined templates and icons for easily getting started. Furthermore, we've developed a tooling workshop to quickly ramp up on the design methods and tooling we've made available. It's called , has been featured in several. TELCO COLLECTION The collection featured today is centred around architectures in the telco domain. There are currently three architectures in this collection and we'll provide a short overview of each, leaving the in depth exploration as an exercise for the reader. Figure 3: Telco architecture collection In each of these architecture overviews you'll find a table of contents outlining the technologies used, several example schematic diagrams with descriptions, and a link in the last section to open the diagrams directly into the online tooling in your browser. 5G is the latest evolution of wireless mobile technology that aims to enable the delivery of highly immersive experiences for people and ultra reliable, low latency communication between devices. At the heart of each 5G network lies the 5G Core (5GC). This portfolio architecture conceives 5G Core as a set of disaggregated, cloud native applications that communicate internally and externally over well defined standard interfaces. Each 5GC component is implemented as a container-based application and is referred to as cloud-native network function (CNF). The use case is ultra-reliable, immersive experiences for people and objects when and where it matters most. 5G is the latest evolution of wireless mobile technology. In this portfolio architecture we’ll discuss an 5G solution built with open source technologies at core, that can work across any hyperscaler. The use case is building an adaptable, on-demand infrastructure services for 5G Core that can deliver across diverse use cases with minimal CAPEX and OPEX. 5G is the latest evolution of wireless mobile technology. It can deliver a number of services from the network edge: * Enhanced mobile broadband (eMBB) * 5G enhances data speeds and experiences with new radio capabilities like mmWave frequency spectrum for higher bandwidth allocation and theoretical throughput up to 20Gbps. * Ultra-reliable, low-latency communication (uRLLC) * 5G supports vertical industry requirements, including sub-millisecond latency with less than 1 lost packet in 105 packets. * Massive machine type communications (mMTC) * 5G supports cost-efficient and robust connection for up to 1 million mMTC, NB-IOT, and LTE-M devices per square kilometer without network overloading. The use case is base on the facts that digital transformation of mobile networks is accelerating and cloudification is increasing. Following the core network, radio access network (RAN) solutions are now taking advantage of the benefits of cloud computing. If you are interested in more architecture solutions like these, feel free to export the . More architecture collections include: * Application development * Automation * Data engineering * Edge * Finance * * Infrastructure * Retail *</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">WildFly 26.1 Beta S2I images have been released on quay.io</title><link rel="alternate" href="https://wildfly.org//news/2022/03/29/WildFly-s2i-26-1-Beta1-Released/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2022/03/29/WildFly-s2i-26-1-Beta1-Released/</id><updated>2022-03-29T00:00:00Z</updated><content type="html">WILDFLY 26.1 BETA S2I DOCKER IMAGES The WildFly S2I (Source-to-Image) builder and runtime Docker images for WildFly 26.1 Beta have been released on . For complete documentation on how to use these images using S2I, OpenShift and Docker, refer to the WildFly S2I . ANTICIPATING A FUTURE SET OF WILDFLY IMAGES The quay.io/wildfly/wildfly-centos7 and quay.io/wildfly/wildfly-runtime-centos7 have been deprecated since WildFly 26. When building or running the server, a deprecation notice is displayed in the console. This WildFly 26.1 Beta and the following 26.1 Final images will be the latest released in quay.io. We have started our migration to the new WildFly S2I images that will be released for WildFly 26.1. The new architecture is based on the and a new pair of docker images. This has provided the details of this new approach. Stay tuned! JF Denise</content><dc:creator>Jean-François Denise</dc:creator></entry><entry><title>Simplify secure connections to PostgreSQL databases with Node.js</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/03/28/simplify-secure-connections-postgresql-databases-nodejs" /><author><name>Michael Dawson</name></author><id>f045aeec-19ed-4a10-a365-a0fcbfed01d6</id><updated>2022-03-28T07:00:00Z</updated><published>2022-03-28T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; is an advanced open source relational database that is commonly used by applications to store structured data. Before accessing a database, the application must connect and provide security credentials. As a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; developer, how can you safely share and provide those credentials in &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt; code without a lot of work? This article introduces service bindings and the &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; package, along with a convenient graphical interface in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;When using a database, the four basic operations are create, read, update, and delete (CRUD, for short). Our team maintains an &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud"&gt;example CRUD application on GitHub&lt;/a&gt; that shows how to connect to a PostgreSQL database and execute the four basic operations. We use that example to illustrate the security model in this article.&lt;/p&gt; &lt;h2&gt;Security risks when connecting to the PostgreSQL database&lt;/h2&gt; &lt;p&gt;The information you need to connect to a PostgreSQL database is:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;User&lt;/li&gt; &lt;li&gt;Password&lt;/li&gt; &lt;li&gt;Host&lt;/li&gt; &lt;li&gt;Database&lt;/li&gt; &lt;li&gt;Port&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You definitely need to be careful about who has access to the user and password, and ideally, you don't want any of these values to be public. This section looks at some simple methods that fail to protect this sensitive information adequately.&lt;/p&gt; &lt;h3&gt;Setting environment variables explicitly&lt;/h3&gt; &lt;p&gt;Using environment variables is the easiest way to configure a connection and is often used in examples like the following JavaScript code:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const serviceHost = process.env.MY_DATABASE_SERVICE_HOST; const user = process.env.DB_USERNAME; const password = process.env.DB_PASSWORD; const databaseName = process.env.POSTGRESQL_DATABASE const connectionString = `postgresql://${user}:${password}@${serviceHost}:5432/${databaseName}`; connectionOptions = { connectionString }; const pool = new Pool(connectionOptions);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Unfortunately, using environment variables is not necessarily secure. If you set the environment variables from the command line, anybody with access to the environment can see them. Tools and frameworks also often make it easy to access environment variables for debugging purposes. For example, in OpenShift, you can view the environment variables from the console, as shown in Figure 1. So you need to find a way to provide connection credentials while keeping them hidden from interlopers.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/pod_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/pod_0.png?itok=gwHLAigv" width="824" height="642" alt="Pod details in the OpenShift console reveal the environmental variables set in the pod." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Pod details in the OpenShift console reveal the environmental variables set in the pod.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Loading environment variables from dotenv&lt;/h3&gt; &lt;p&gt;Instead of setting the credentials in the environment directly, a safer way is to use a package such as &lt;a href="https://www.npmjs.com/package/dotenv"&gt;dotenv&lt;/a&gt; to get the credentials from a file and provide them to the Node.js application environment. The benefit of using &lt;code&gt;dotenv&lt;/code&gt; is that the credentials don't show up in the environment outside of the Node.js process.&lt;/p&gt; &lt;p&gt;Although this approach is better, the credentials still might be exposed if you dump the Node.js environment for debugging through a &lt;a href="https://developer.ibm.com/articles/easily-identify-problems-in-your-nodejs-apps-with-diagnostic-report/"&gt;Node.js diagnostic report&lt;/a&gt;. You are also left with the question of how to get the &lt;code&gt;dotenv&lt;/code&gt; file securely to the application. If you are deploying to &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;, you can map a file into deployed &lt;a href="https://developers.redhat.com/topics/containers/"&gt;containers&lt;/a&gt;, but that will take some planning and coordination for deployments.&lt;/p&gt; &lt;p&gt;By this point, you are probably thinking that this seems like a lot of work and are wondering whether you need to configure the connection information for each type of service and set of credentials that are needed by an application. The good news is that for Kubernetes environments, this problem has already been solved. We cover the solution, service binding, in the next section.&lt;/p&gt; &lt;h2&gt;Passing the credentials securely: Service binding in Kubernetes&lt;/h2&gt; &lt;p&gt;Service binding is a standard approach to map a set of files into containers to provide credentials in a safe and scalable way. You can read more about the Service Binding specification for Kubernetes on &lt;a href="https://github.com/k8s-service-bindings/spec"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The specification does not define what files are mapped in for a given service type. In OpenShift, binding to a PostgreSQL database instance (created using either the Crunchy or the Cloud Native PostgreSQL Operators, as described in an &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;overview of the Service Binding Operator&lt;/a&gt;) results in mapping the following files into the application container:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ SERVICE_BINDING_ROOT/&lt;postgressql-instance-name&gt; ├── user ├── host ├── database ├── password ├── port ├── ca.crt └── tls.key └── tls.crt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; is passed to the application through the environment.&lt;/p&gt; &lt;p&gt;The last three files contain the keys and certificates needed to connect over the widely used Transport Layer Security (TLS) standard and are present only if the database is configured to use TLS.&lt;/p&gt; &lt;h2&gt;Consuming service bindings easily with kube-service-bindings&lt;/h2&gt; &lt;p&gt;Now that you have the credentials available to the application running in the container, the remaining work is to read the credentials from those files and provide them to the PostgreSQL client used within your Node.js application. But wait—that still sounds like a lot of work, and it's also tied to the client you are using.&lt;/p&gt; &lt;p&gt;To make this easier, we've put together an npm package called &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt;, which makes it easy for Node.js applications to consume these secrets without requiring developers to be familiar with service bindings.&lt;/p&gt; &lt;p&gt;The package provides the &lt;code&gt;getBinding()&lt;/code&gt; method, which does roughly the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Look for the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; variable in order to determine whether bindings are available.&lt;/li&gt; &lt;li&gt;Read the connection information from the files.&lt;/li&gt; &lt;li&gt;Map the names of the files to the option names needed by the Node.js clients that will connect to the service.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Figure 2 shows the steps.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/get.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/get.png?itok=DwFwUD5F" width="1191" height="707" alt="The getBinding() method involves three main steps." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The getBinding() method involves three main steps.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Let's assume you connect to PostgreSQL using the popular &lt;a href="https://www.npmjs.com/package/pg"&gt;pg&lt;/a&gt; client, a library that provides all the basic commands to interact with the database. In this case you call the &lt;code&gt;getBinding()&lt;/code&gt; method with &lt;code&gt;POSTGRESQL&lt;/code&gt; and &lt;code&gt;pg&lt;/code&gt; to tell &lt;code&gt;kube-service-bindings&lt;/code&gt; which client the application is using, and then pass the object returned by &lt;code&gt;getBinding()&lt;/code&gt;when you create a Pool object. Minus error checking, the code is as simple as this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const serviceBindings = require('kube-service-bindings'); const { Pool } = require('pg'); let connectionOptions; try { connectionOptions = serviceBindings.getBinding('POSTGRESQL', 'pg'); } catch (err) { } const pool = new Pool(connectionOptions);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first parameter to &lt;code&gt;getBindings()&lt;/code&gt; is &lt;code&gt;POSTGRESQL&lt;/code&gt;, to specify that you are connecting to a PostgreSQL database. The second parameter, &lt;code&gt;pg&lt;/code&gt;, tells &lt;code&gt;kube-service-bindings&lt;/code&gt; that you are using the &lt;code&gt;pg&lt;/code&gt; client so that the call will return the information as an object that can be passed when creating a &lt;code&gt;pg&lt;/code&gt; Pool object.&lt;/p&gt; &lt;p&gt;The CRUD example, and more specifically the &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud/blob/94cce5e4056f58511bd1b66576e64594e5ac4d4f/lib/db/index.js#L7"&gt;lib/db/index.js&lt;/a&gt; file, has been updated so that it can get the credentials from the environment, or automatically using &lt;code&gt;kube-service-bindings&lt;/code&gt; when credentials are available through service bindings.&lt;/p&gt; &lt;p&gt;With &lt;code&gt;kube-service-bindings&lt;/code&gt;, it's easy for Node.js developers to use credentials available through service bindings. The second part is to set up the service bindings themselves. The procedure is to install the Service Binding Operator as &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;described in the overview article&lt;/a&gt; mentioned earlier, install an Operator to help you create databases, create the database for your application, and finally apply some YAML to tell the Service Binding Operator to bind the database to your application.&lt;/p&gt; &lt;h2&gt;Setting up service bindings in OpenShift&lt;/h2&gt; &lt;p&gt;With the release of &lt;a href="https://docs.openshift.com/container-platform/4.8/release_notes/ocp-4-8-release-notes.html"&gt;OpenShift 4.8&lt;/a&gt;, you can use the OpenShift user interface (UI) to do the service binding. Thus, administrators and operators of a cluster can easily set up the PostgreSQL database instance for an organization. Developers can then connect their applications without needing to know the credentials. You can use the UI for convenience during initial development, and then YAML for more automated or production deployments.&lt;/p&gt; &lt;p&gt;The UI steps are quite simple:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Create a database using one of the PostgresSQL Operators.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Deploy your application to the same namespace using &lt;code&gt;kube-service-bindings&lt;/code&gt;. Figure 3 shows the topology view of the namespace.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/name_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/name_0.png?itok=Kdcvsl97" width="600" height="244" alt="The namespace contains the PostgreSQL database and Node.js application." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: The namespace contains the PostgreSQL database and Node.js application.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;Drag a link from the application to the database until you see the "Create a binding connector" box pop up (Figure 4).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/bind.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/bind.png?itok=nH27ra3j" width="600" height="249" alt="Create a binding from the Node.js application to the PostgreSQL database." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Create a binding from the Node.js application to the PostgreSQL database.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;Finally, release the mouse button. The binding is created (Figure 5) and the credentials are automatically mapped into your application pods. If you've configured your application to retry the connection until service bindings are available, it should then get the credentials and connect to the database.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/done.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/done.png?itok=iDfeRLWt" width="600" height="266" alt="The binding is established." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: The binding is established.&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Further resources&lt;/h2&gt; &lt;p&gt;This article introduced you to the credentials needed to connect to a PostgreSQL database and how they can be safely provided to your Node.js applications. To learn more, try the following:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Install and experiment with the &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud"&gt;CRUD example&lt;/a&gt; to explore the code and &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt;. (If you are really adventurous, you can create your own files and set &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; to point to them.)&lt;/li&gt; &lt;li&gt;Work through how to set up service bindings for a PostgreSQL database using the instructions in the &lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;Service Binding Operator overview&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Connect the &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http-crud"&gt;CRUD example&lt;/a&gt; to the PostgreSQL database you created using the UI.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;We hope you found this article informative. To stay up to date with what else Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/03/28/simplify-secure-connections-postgresql-databases-nodejs" title="Simplify secure connections to PostgreSQL databases with Node.js"&gt;Simplify secure connections to PostgreSQL databases with Node.js&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2022-03-28T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus Tools for Visual Studio Code - 1.10.0 release</title><link rel="alternate" href="https://quarkus.io/blog/vscode-quarkus-1.10.0/" /><author><name>Roland Grunberg</name></author><id>https://quarkus.io/blog/vscode-quarkus-1.10.0/</id><updated>2022-03-28T00:00:00Z</updated><content type="html">Quarkus Tools for Visual Studio Code 1.10.0 has been released on the VS Code Marketplace &amp;amp; Open VSX. For a list of all changes, please refer to the changelog. It’s been about 8 months since our last release of Quarkus Tools for VS Code. 8 months!! That’s a really long...</content><dc:creator>Roland Grunberg</dc:creator></entry></feed>
